{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "007b5205",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c52dd00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import json, os\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import AdamW, Adam\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "from transformers import set_seed\n",
    "from transformers import AdamWeightDecay\n",
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, RobertaTokenizer\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, \\\n",
    "roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f55447",
   "metadata": {},
   "source": [
    "Specify a constant seeder for processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bbb1556",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeders = [123456, 789012, 345678, 901234, 567890, 123, 456, 789, 012, 345]\n",
    "\n",
    "seed = seeders[0]\n",
    "\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494ef531",
   "metadata": {},
   "source": [
    "Pre-trained tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aca2c333",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_variation = \"microsoft/CodeGPT-small-py\" \n",
    "# \"gpt2\" # \"microsoft/CodeGPT-small-py\" # \"microsoft/CodeGPT-small-py-adaptedGPT2\" # microsoft/CodeGPT-small-java-adaptedGPT2 # microsoft/CodeGPT-small-java\n",
    "if model_variation == \"gpt2\":\n",
    "    PAD_TOKEN = \"<|pad|>\"\n",
    "    EOS_TOKEN = \"<|endoftext|>\"\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_variation, do_lower_case=True, pad_token=PAD_TOKEN,\n",
    "    eos_token=EOS_TOKEN)\n",
    "else:\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_variation, do_lower_case=True)\n",
    "    # tokenizer = RobertaTokenizer(vocab_file=\"../../tokenizer_training/cpp_tokenizer/cpp_tokenizer-vocab.json\",\n",
    "    #                          merges_file=\"../../tokenizer_training/cpp_tokenizer/cpp_tokenizer-merges.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88221c5a",
   "metadata": {},
   "source": [
    "Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6af2cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = os.path.join('..', '..', '..')\n",
    "dataset = pd.read_csv(os.path.join(root_path, 'data', 'train.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2522ef27",
   "metadata": {},
   "source": [
    "Shuffle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06f1a1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    index Access Gained Attack Origin Authentication Required Availability  \\\n",
      "0   82302           NaN        Remote            Not required      Partial   \n",
      "1   57423           NaN         Local            Not required     Complete   \n",
      "2   48002           NaN         Local            Not required     Complete   \n",
      "3   92783           NaN        Remote            Not required      Partial   \n",
      "4  123879           NaN        Remote            Not required      Partial   \n",
      "\n",
      "           CVE ID                                        CVE Page   CWE ID  \\\n",
      "0  CVE-2018-11598  https://www.cvedetails.com/cve/CVE-2018-11598/  CWE-125   \n",
      "1   CVE-2015-8539   https://www.cvedetails.com/cve/CVE-2015-8539/  CWE-264   \n",
      "2   CVE-2016-9685   https://www.cvedetails.com/cve/CVE-2016-9685/  CWE-400   \n",
      "3  CVE-2018-20784  https://www.cvedetails.com/cve/CVE-2018-20784/  CWE-400   \n",
      "4   CVE-2013-0918   https://www.cvedetails.com/cve/CVE-2013-0918/  CWE-264   \n",
      "\n",
      "  Complexity Confidentiality  ... parentID  \\\n",
      "0     Medium         Partial  ...      NaN   \n",
      "1        Low        Complete  ...      NaN   \n",
      "2        Low             NaN  ...      NaN   \n",
      "3        Low         Partial  ...      NaN   \n",
      "4     Medium         Partial  ...      NaN   \n",
      "\n",
      "                                               patch   project  \\\n",
      "0  @@ -122,6 +122,16 @@ void jspReplaceWith(JsVar...  Espruino   \n",
      "1  @@ -120,7 +120,10 @@ int user_update(struct ke...     linux   \n",
      "2  @@ -202,8 +202,10 @@ xfs_attr_shortform_list(x...     linux   \n",
      "3  @@ -352,10 +352,9 @@ static inline void list_d...     linux   \n",
      "4  @@ -3039,9 +3039,9 @@ WebNavigationPolicy Rend...    Chrome   \n",
      "\n",
      "                              project_after  \\\n",
      "0  bf4416ab9129ee3afd56739ea4e3cd0da5484b6b   \n",
      "1  096fe9eaea40a17e125569f9e657e34cdb6d73bd   \n",
      "2  2e83b79b2d6c78bf1b4aa227938a214dcbddc83f   \n",
      "3  c40f7d74c741a907cfaeb73a7697081881c497d0   \n",
      "4  0a57375ad73780e61e1770a9d88b0529b0dbd33b   \n",
      "\n",
      "                             project_before target  \\\n",
      "0  7a481444575e487698497f4eed672734a0795967      0   \n",
      "1  6ffeba9607343f15303a399bc402a538800d89d9      0   \n",
      "2  36f90b0a2ddd60823fe193a85e60ff1906c2a9b3      0   \n",
      "3  6d101ba6be2a26a3e1f513b5e293f0fd2b79ec5c      0   \n",
      "4  e3cb4529d79a4993535da612dafedc8c40f075bb      0   \n",
      "\n",
      "                                   vul_func_with_fix  \\\n",
      "0  bool jspIsInterrupted() {\\n  return (execInfo....   \n",
      "1  void user_destroy(struct key *key)\\n{\\n\\tstruc...   \n",
      "2  xfs_attr_leaf_list(xfs_attr_list_context_t *co...   \n",
      "3  static inline void update_tg_load_avg(struct c...   \n",
      "4  static void NotifyTimezoneChange(WebKit::WebFr...   \n",
      "\n",
      "                                      processed_func flaw_line flaw_line_index  \n",
      "0  bool jspIsInterrupted() {\\n  return (execInfo....       NaN             NaN  \n",
      "1  void user_destroy(struct key *key)\\n{\\n\\tstruc...       NaN             NaN  \n",
      "2  xfs_attr_leaf_list(xfs_attr_list_context_t *co...       NaN             NaN  \n",
      "3  static inline void update_tg_load_avg(struct c...       NaN             NaN  \n",
      "4  static void NotifyTimezoneChange(WebKit::WebFr...       NaN             NaN  \n",
      "\n",
      "[5 rows x 39 columns]\n",
      "150908\n"
     ]
    }
   ],
   "source": [
    "data = dataset.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "print(data.head())\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977166a0-54c7-4be3-8f5e-1c1ea6f12053",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data[\"project\"] != \"Chrome\"]\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21ecf64f-8f25-40d8-a057-998b6e56cb40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed_func</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bool jspIsInterrupted() {\\n  return (execInfo....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>void user_destroy(struct key *key)\\n{\\n\\tstruc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xfs_attr_leaf_list(xfs_attr_list_context_t *co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>static inline void update_tg_load_avg(struct c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>static void NotifyTimezoneChange(WebKit::WebFr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      processed_func  target\n",
       "0  bool jspIsInterrupted() {\\n  return (execInfo....       0\n",
       "1  void user_destroy(struct key *key)\\n{\\n\\tstruc...       0\n",
       "2  xfs_attr_leaf_list(xfs_attr_list_context_t *co...       0\n",
       "3  static inline void update_tg_load_avg(struct c...       0\n",
       "4  static void NotifyTimezoneChange(WebKit::WebFr...       0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[[\"processed_func\", \"target\"]]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e09f413",
   "metadata": {},
   "source": [
    "Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dabaa45-00e9-43df-b91b-f60e826f0107",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset=[\"processed_func\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "479d07b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of words: 15441\n"
     ]
    }
   ],
   "source": [
    "word_counts = data[\"processed_func\"].apply(lambda x: len(x.split()))\n",
    "max_length = word_counts.max()\n",
    "print(\"Maximum number of words:\", max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3055b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "0    142172\n",
      "1      8736\n",
      "Name: count, dtype: int64\n",
      "Percentage:  6.1446698365360275 %\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "vc = data[\"target\"].value_counts()\n",
    "\n",
    "print(vc)\n",
    "\n",
    "print(\"Percentage: \", (vc[1] / vc[0])*100, '%')\n",
    "\n",
    "n_categories = len(vc)\n",
    "print(n_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ab7af89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bool jspIsInterrupted() {\\n  return (execInfo....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>void user_destroy(struct key *key)\\n{\\n\\tstruc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xfs_attr_leaf_list(xfs_attr_list_context_t *co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>static inline void update_tg_load_avg(struct c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>static void NotifyTimezoneChange(WebKit::WebFr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Labels\n",
       "0  bool jspIsInterrupted() {\\n  return (execInfo....       0\n",
       "1  void user_destroy(struct key *key)\\n{\\n\\tstruc...       0\n",
       "2  xfs_attr_leaf_list(xfs_attr_list_context_t *co...       0\n",
       "3  static inline void update_tg_load_avg(struct c...       0\n",
       "4  static void NotifyTimezoneChange(WebKit::WebFr...       0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.DataFrame(({'Text': data['processed_func'], 'Labels': data['target']}))\n",
    "#train_data = train_data[0:100]\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b6f1bb",
   "metadata": {},
   "source": [
    "Split to train-val-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68f3b8ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>void PdfCompositorClient::Connect(service_mana...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>int iwlagn_add_bssid_station(struct iwl_priv *...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>static int dnxhd_init_vlc(DNXHDContext *ctx, u...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>void CameraService::onFirstRef()\\n{\\n    LOG1(...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>int EmbedStream::getChar() {\\n  if (limited &amp;&amp;...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Labels\n",
       "0  void PdfCompositorClient::Connect(service_mana...       0\n",
       "1  int iwlagn_add_bssid_station(struct iwl_priv *...       0\n",
       "2  static int dnxhd_init_vlc(DNXHDContext *ctx, u...       0\n",
       "3  void CameraService::onFirstRef()\\n{\\n    LOG1(...       0\n",
       "4  int EmbedStream::getChar() {\\n  if (limited &&...       0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data = pd.read_csv(os.path.join(root_path, 'data', 'val.csv'))\n",
    "\n",
    "val_data = val_data[val_data[\"project\"] != \"Chrome\"]\n",
    "\n",
    "val_data = pd.DataFrame(({'Text': val_data['processed_func'], 'Labels': val_data['target']}))\n",
    "val_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b54033e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(os.path.join(root_path, 'data', 'test.csv'))\n",
    "\n",
    "test_data = test_data[test_data[\"project\"] != \"Chrome\"]\n",
    "\n",
    "test_data = pd.DataFrame(({'Text': test_data['processed_func'], 'Labels': test_data['target']}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52ea1ce",
   "metadata": {},
   "source": [
    "Pre-processing step: Under-sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "550b3672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution  Labels\n",
      "0    142172\n",
      "1      8736\n",
      "Name: count, dtype: int64\n",
      "Majority class  0\n",
      "Minority class  1\n",
      "Targeted number of majority class 17472\n",
      "Class distribution after augmentation Labels\n",
      "0    17472\n",
      "1     8736\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "sampling = False\n",
    "if n_categories == 2 and sampling == True:\n",
    "    # Apply under-sampling with the specified strategy\n",
    "    class_counts = pd.Series(train_data[\"Labels\"]).value_counts()\n",
    "    print(\"Class distribution \", class_counts)\n",
    "\n",
    "    majority_class = class_counts.idxmax()\n",
    "    print(\"Majority class \", majority_class)\n",
    "\n",
    "    minority_class = class_counts.idxmin()\n",
    "    print(\"Minority class \", minority_class)\n",
    "\n",
    "    target_count = 2 * class_counts[class_counts.idxmin()] # class_counts[class_counts.idxmin()] # int(class_counts.iloc[0] / 2)  \n",
    "    print(\"Targeted number of majority class\", target_count)\n",
    "\n",
    "    # under\n",
    "    sampling_strategy = {majority_class: target_count}        \n",
    "    rus = RandomUnderSampler(random_state=seed, sampling_strategy=sampling_strategy)\n",
    "\n",
    "    x_train_resampled, y_train_resampled = rus.fit_resample(np.array(train_data[\"Text\"]).reshape(-1, 1), train_data[\"Labels\"]) \n",
    "    print(\"Class distribution after augmentation\", pd.Series(y_train_resampled).value_counts())\n",
    "\n",
    "\n",
    "    # Shuffle the resampled data while preserving the correspondence between features and labels\n",
    "    x_train_resampled, y_train_resampled = shuffle(x_train_resampled, y_train_resampled, random_state=seed)\n",
    "\n",
    "    # rename\n",
    "    X_train = x_train_resampled\n",
    "    Y_train = y_train_resampled\n",
    "\n",
    "    X_train = pd.Series(X_train.reshape(-1))\n",
    "\n",
    "else:\n",
    "    X_train = train_data[\"Text\"]\n",
    "    Y_train = train_data[\"Labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc04f73",
   "metadata": {},
   "source": [
    "Pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "561c45d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b63c75a5d2694d9f8a009e19937bbd10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/510M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iliaskaloup\\AppData\\Local\\anaconda3\\envs\\torchenv\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\iliaskaloup\\.cache\\huggingface\\hub\\models--microsoft--CodeGPT-small-py. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at microsoft/CodeGPT-small-py and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = GPT2ForSequenceClassification.from_pretrained(model_variation,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id, num_labels=n_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ad207b",
   "metadata": {},
   "source": [
    "Resize model embedding to match new tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3369b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50002, 768)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f8c8a0",
   "metadata": {},
   "source": [
    "Compute maximum length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb29633e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMaxLen(X):\n",
    "\n",
    "    # Code for identifying max length of the data samples after tokenization using transformer tokenizer\n",
    "    \n",
    "    max_length = 0\n",
    "    max_row = 0\n",
    "    \n",
    "    # Iterate over each sample in your dataset\n",
    "    for i, input_ids in enumerate(X['input_ids']):\n",
    "        # Convert input_ids to a PyTorch tensor\n",
    "        input_ids_tensor = torch.tensor(input_ids)\n",
    "        # Calculate the length of the tokenized sequence for the current sample\n",
    "        length = torch.sum(input_ids_tensor != tokenizer.pad_token_id).item()\n",
    "        # Update max_length and max_row if the current length is greater\n",
    "        if length > max_length:\n",
    "            max_length = length\n",
    "            max_row = i\n",
    "\n",
    "    print(\"Max length of tokenized data:\", max_length)\n",
    "    print(\"Row with max length:\", max_row)\n",
    "    \n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ee53d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iliaskaloup\\AppData\\Local\\Temp\\ipykernel_16800\\514096298.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids_tensor = torch.tensor(input_ids)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of tokenized data: 512\n",
      "Row with max length: 4\n"
     ]
    }
   ],
   "source": [
    "X = tokenizer(\n",
    "        text=X_train.tolist(),\n",
    "        add_special_tokens=True,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors='pt',\n",
    "        return_token_type_ids=False,\n",
    "        return_attention_mask=True,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "max_len = getMaxLen(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9649855c",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7f8e2d9-f57f-4aa6-ad38-eff591b83b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer(\n",
    "    text=X_train.tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=max_len,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors='pt',\n",
    "    return_token_type_ids=False,\n",
    "    return_attention_mask=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b11feae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = tokenizer(\n",
    "    text=val_data['Text'].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=max_len,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors='pt',\n",
    "    return_token_type_ids=False,\n",
    "    return_attention_mask=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b957eb5-bf6f-4f35-b8df-ab77c87c98f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tokenizer(\n",
    "    text=test_data['Text'].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=max_len,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors='pt',\n",
    "    return_token_type_ids=False,\n",
    "    return_attention_mask=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80aff597",
   "metadata": {},
   "source": [
    "Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba55e5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "lr = 2e-5 #5e-05\n",
    "batch_size = 8 #16\n",
    "patience = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa0b964",
   "metadata": {},
   "source": [
    "Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec18b579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([26208]), torch.Size([18864]), torch.Size([18864]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train = torch.LongTensor(Y_train.tolist())\n",
    "Y_val = torch.LongTensor(val_data[\"Labels\"].tolist())\n",
    "Y_test = torch.LongTensor(test_data[\"Labels\"].tolist())\n",
    "Y_train.size(), Y_val.size(), Y_test.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27897fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train[\"input_ids\"], X_train[\"attention_mask\"], Y_train)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "val_dataset = TensorDataset(X_val[\"input_ids\"], X_val[\"attention_mask\"], Y_val)\n",
    "val_sampler = SequentialSampler(val_dataset)\n",
    "val_dataloader = DataLoader(val_dataset, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "test_dataset = TensorDataset(X_test[\"input_ids\"], X_test[\"attention_mask\"], Y_test)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fdf28391",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = lr, # default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # default is 1e-8.\n",
    "                  )\n",
    "\n",
    "max_steps = len(train_dataloader)*n_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "            num_warmup_steps=max_steps // 5,\n",
    "            num_training_steps=max_steps)\n",
    "\n",
    "loss_fun = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "580acc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_steps = len(train_dataloader) * n_epochs\n",
    "\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, # Default value in run_glue.py \n",
    "#                                             num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2f8ac5a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0fc1a47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2ForSequenceClassification(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50002, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (score): Linear(in_features=768, out_features=2, bias=False)\n",
      ")\n",
      "No. of trainable parameters:  124245504\n"
     ]
    }
   ],
   "source": [
    "print(model.to(device))\n",
    "print(\"No. of trainable parameters: \", sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7059e551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(filename, epoch, model, optimizer, scheduler, train_loss_per_epoch, val_loss_per_epoch, train_f1_per_epoch, val_f1_per_epoch):\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'model': model,\n",
    "        'optimizer': optimizer,\n",
    "        'scheduler': scheduler,\n",
    "        'train_loss_per_epoch': train_loss_per_epoch,\n",
    "        'val_loss_per_epoch': val_loss_per_epoch,\n",
    "        'train_f1_per_epoch': train_f1_per_epoch,\n",
    "        'val_f1_per_epoch': val_f1_per_epoch\n",
    "        }\n",
    "    torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a4f51b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # we do not retrain our pre-trained BERT and train only the last linear dense layer\n",
    "# for param in model.bert_model.parameters():\n",
    "#     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fde3894",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d19bf0-f050-42fd-a61c-96211057f4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize values for implementing Callbacks\n",
    "## Early Stopping\n",
    "best_val_f1 = -1\n",
    "best_epoch = -1\n",
    "no_improvement_counter = 0\n",
    "## Save best - optimal checkpointing\n",
    "checkpoint_dir = './checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "save_path = os.path.join(checkpoint_dir, 'best_weights.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f609ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch:  1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c64b4b6272e2447e9b9a7458ba44c709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/3276 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52277674afb84df1807c9c930736f8ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/2358 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Train Loss: 0.4026 - Valid Loss: 0.0899\n",
      "Epoch 1/10 - Train F1: 0.7513 - Valid F1: 0.8129\n",
      "Model saved at epoch:  1\n",
      "Epoch:  2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f209bd9a16da4c5cbc6e48d13171e36a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/3276 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Training...\")\n",
    "milli_sec1 = int(round(time.time() * 1000))\n",
    "\n",
    "train_loss_per_epoch = []\n",
    "val_loss_per_epoch = []\n",
    "train_f1_per_epoch = []\n",
    "val_f1_per_epoch = []\n",
    "\n",
    "for epoch_num in range(n_epochs):\n",
    "    print('Epoch: ', epoch_num + 1)\n",
    "    \n",
    "    #Training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    total_preds = []\n",
    "    total_labels = []\n",
    "    for step_num, batch_data in enumerate(tqdm(train_dataloader, desc='Training')):\n",
    "        \n",
    "        input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
    "                \n",
    "        # clear previously calculated gradients\n",
    "        model.zero_grad() # optimizer.zero_grad()\n",
    "        \n",
    "        # get model predictions for the current batch\n",
    "        output = model(input_ids = input_ids, attention_mask=att_mask) # , labels=labels\n",
    "        \n",
    "        # compute the loss between actual and predicted values\n",
    "        loss = loss_fun(output.logits, labels) #loss = output.loss #output[0]       \n",
    "        # add on to the total loss\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        clip_grad_norm_(parameters=model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Print training loss after each batch\n",
    "        #print(\"Epoch {}/{} - Batch {}/{} - Training Loss: {:.4f}\".format(epoch_num+1, n_epochs, step_num+1, len(train_dataloader), loss.item()))\n",
    "        \n",
    "        # model predictions are stored on GPU. So, push it to CPU\n",
    "        preds = np.argmax(output.logits.cpu().detach().numpy(),axis=-1)\n",
    "        # append the model predictions\n",
    "        total_preds+=list(preds)\n",
    "        total_labels+=labels.cpu().numpy().tolist()\n",
    "        \n",
    "    train_loss_per_epoch.append(train_loss / len(train_dataloader))    \n",
    "    train_accuracy=accuracy_score(total_labels, total_preds)\n",
    "    if n_categories > 2:\n",
    "        train_precision=precision_score(total_labels, total_preds, average='macro')\n",
    "        train_recall=recall_score(total_labels, total_preds, average='macro')\n",
    "        train_f1=f1_score(total_labels, total_preds, average='macro')\n",
    "    else:\n",
    "        train_precision=precision_score(total_labels, total_preds)\n",
    "        train_recall=recall_score(total_labels, total_preds)\n",
    "        train_f1=f1_score(total_labels, total_preds)\n",
    "        train_roc_auc=roc_auc_score(total_labels, total_preds)\n",
    "    train_f2 = (5*train_precision*train_recall) / (4*train_precision+train_recall)\n",
    "\n",
    "    #Validation\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    valid_pred = []\n",
    "    actual_labels = []\n",
    "    with torch.no_grad():\n",
    "        for step_num_e, batch_data in enumerate(tqdm(val_dataloader, desc='Validation')):\n",
    "            input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
    "            \n",
    "            output = model(input_ids = input_ids, attention_mask=att_mask) # , labels=labels\n",
    "            \n",
    "            preds = np.argmax(output.logits.cpu().detach().numpy(), axis=-1)\n",
    "            valid_pred+=list(preds)\n",
    "            actual_labels+=labels.cpu().numpy().tolist()\n",
    "\n",
    "            loss = loss_fun(output.logits, labels) #loss = output.loss #output[0]\n",
    "            valid_loss += loss.item()\n",
    "        \n",
    "    val_loss_per_epoch.append(valid_loss / len(val_dataloader))    \n",
    "    val_accuracy=accuracy_score(actual_labels, valid_pred)\n",
    "    if n_categories > 2:\n",
    "        val_precision=precision_score(actual_labels, valid_pred, average='macro')\n",
    "        val_recall=recall_score(actual_labels, valid_pred, average='macro')\n",
    "        val_f1=f1_score(actual_labels, valid_pred, average='macro')\n",
    "    else:\n",
    "        val_precision=precision_score(actual_labels, valid_pred)\n",
    "        val_recall=recall_score(actual_labels, valid_pred)\n",
    "        val_f1=f1_score(actual_labels, valid_pred)\n",
    "        val_roc_auc=roc_auc_score(actual_labels, valid_pred)\n",
    "    val_f2 = (5*val_precision*val_recall) / (4*val_precision+val_recall)\n",
    "    \n",
    "    print(\"Epoch {}/{} - Train Loss: {:.4f} - Valid Loss: {:.4f}\".format(epoch_num+1, n_epochs, train_loss_per_epoch[-1], val_loss_per_epoch[-1]))\n",
    "    print(\"Epoch {}/{} - Train F1: {:.4f} - Valid F1: {:.4f}\".format(epoch_num+1, n_epochs, train_f1, val_f1))\n",
    "    \n",
    "    train_f1_per_epoch.append(train_f1)\n",
    "    val_f1_per_epoch.append(val_f1)\n",
    "\n",
    "    total_epochs = epoch_num + 1\n",
    "    # Implement Callbacks: Early Stopping and save best\n",
    "    # Check if the validation F1 score has improved\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        best_epoch = epoch_num + 1\n",
    "        no_improvement_counter = 0 # Reset the counter\n",
    "        \n",
    "        # Save the best model checkpoint\n",
    "        save_checkpoint(save_path, epoch_num+1, model.state_dict(), optimizer.state_dict(), scheduler.state_dict(), train_loss_per_epoch, val_loss_per_epoch, train_f1_per_epoch, val_f1_per_epoch)\n",
    "        print(\"Model saved at epoch: \", epoch_num+1)\n",
    "    else:\n",
    "        no_improvement_counter += 1\n",
    "        \n",
    "        if no_improvement_counter >= patience:\n",
    "            print(\"No improvement for\", patience, \"consecutive epochs.\")\n",
    "            print(\"Early stopping after epoch No.\", total_epochs)\n",
    "            print(\"Best model after epoch No\", best_epoch)\n",
    "            print(\"Best achieved val_f1 = \", best_val_f1)\n",
    "            break\n",
    "\n",
    "milli_sec2 = int(round(time.time() * 1000))s\n",
    "print(\"Training is completed after\", milli_sec2-milli_sec1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dcae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, total_epochs + 1)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(epochs, train_loss_per_epoch,label ='training loss')\n",
    "ax.plot(epochs, val_loss_per_epoch, label = 'validation loss' )\n",
    "ax.set_title('Training and Validation loss')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a54858",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, total_epochs + 1)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(epochs, train_f1_per_epoch,label ='training F1-score')\n",
    "ax.plot(epochs, val_f1_per_epoch, label = 'validation F1-score')\n",
    "ax.set_title('Training and Validation F1-scores')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('F1-score')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53379dac",
   "metadata": {},
   "source": [
    "Load best model from checkpoint during training with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1534cd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(save_path, map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90781fe",
   "metadata": {},
   "source": [
    "Make predictions on the testing set and compute evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c019f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_pred = []\n",
    "actual_labels = []\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for step_num, batch_data in enumerate(tqdm(test_dataloader, desc='Testing')):\n",
    "        input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
    "        \n",
    "        output = model(input_ids = input_ids, attention_mask=att_mask) #, labels= labels\n",
    "\n",
    "        loss = loss_fun(output.logits, labels) #loss = output.loss #output[0]\n",
    "        test_loss += loss.item()\n",
    "   \n",
    "        preds = np.argmax(output.logits.cpu().detach().numpy(), axis=-1)\n",
    "        test_pred+=list(preds)\n",
    "        actual_labels+=labels.cpu().numpy().tolist()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c1d9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_report = classification_report(actual_labels, test_pred)\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "total_test_loss = test_loss/len(test_dataloader) \n",
    "accuracy=accuracy_score(actual_labels, test_pred)\n",
    "if n_categories > 2:\n",
    "    precision=precision_score(actual_labels, test_pred, average='macro')\n",
    "    recall=recall_score(actual_labels, test_pred, average='macro')\n",
    "    f1=f1_score(actual_labels, test_pred, average='macro')\n",
    "else:\n",
    "    precision=precision_score(actual_labels, test_pred)\n",
    "    recall=recall_score(actual_labels, test_pred)\n",
    "    f1=f1_score(actual_labels, test_pred)\n",
    "    roc_auc=roc_auc_score(actual_labels, test_pred)\n",
    "f2 = (5*precision*recall) / (4*precision+recall)\n",
    "\n",
    "print(\"Accuracy:%.2f%%\"%(accuracy*100))\n",
    "print(\"Precision:%.2f%%\"%(precision*100))\n",
    "print(\"Recall:%.2f%%\"%(recall*100))\n",
    "print(\"F1 score:%.2f%%\"%(f1*100))\n",
    "print(\"F2 score:%.2f%%\"%(f2*100))\n",
    "if roc_auc:\n",
    "    print(\"Roc_Auc score:%.2f%%\"%(roc_auc*100))\n",
    "\n",
    "conf_matrix = confusion_matrix(actual_labels, test_pred)\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "#acc = ((tp+tn)/(tp+tn+fp+fn))\n",
    "\n",
    "print(\"TP=\",tp)\n",
    "print(\"TN=\",tn)\n",
    "print(\"FP=\",fp)\n",
    "print(\"FN=\",fn)\n",
    "#print(conf_matrix)\n",
    "sn.heatmap(conf_matrix, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd72cad1",
   "metadata": {},
   "source": [
    "Export classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8ca2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"forSequence\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c96a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the path\n",
    "path = os.path.join(root_path, 'results', model_variation.split(\"/\")[-1], method, str(seed))\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# Define the CSV file path\n",
    "csv_file_path = os.path.join(path, f\"{seed}.csv\")\n",
    "\n",
    "# Write data to CSV\n",
    "data = {\n",
    "    \"accuracy\": accuracy,\n",
    "    \"precision\": precision,\n",
    "    \"recall\": recall,\n",
    "    \"f1\": f1,\n",
    "    \"f2\": f2,\n",
    "    \"roc_auc\": roc_auc\n",
    "}\n",
    "\n",
    "# Write to CSV\n",
    "with open(csv_file_path, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=data.keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerow(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c4131a",
   "metadata": {},
   "source": [
    "Compute the average values of the classication metrics considering the results for all different seeders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac1c102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary to store cumulative sum of metrics\n",
    "cumulative_metrics = defaultdict(float)\n",
    "count = 0  # Counter to keep track of number of CSV files\n",
    "\n",
    "# Iterate over all CSV files in the results folder\n",
    "results_folder = os.path.join(root_path, \"results\", model_variation.split(\"/\")[-1], method, str(seed))\n",
    "for filename in os.listdir(results_folder):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        csv_file_path = os.path.join(results_folder, filename)\n",
    "        with open(csv_file_path, \"r\", newline=\"\") as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                for metric, value in row.items():\n",
    "                    cumulative_metrics[metric] += float(value)\n",
    "        count += 1\n",
    "        \n",
    "# Compute average values\n",
    "average_metrics = {metric: total / count for metric, total in cumulative_metrics.items()}\n",
    "\n",
    "# Print average values \n",
    "print(average_metrics)\n",
    "\n",
    "# Define the path for the average CSV file\n",
    "avg_csv_file_path = os.path.join(root_path, \"results\", model_variation.split(\"/\")[-1], method, \"avg.csv\")\n",
    "\n",
    "# Write average metrics to CSV\n",
    "with open(avg_csv_file_path, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=average_metrics.keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerow(average_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86a5545-54b2-40c9-a1ff-e2ec46fb39b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
