{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72b7c616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install transformers library.\n",
    "#!pip install -q git+https://github.com/huggingface/transformers.git\n",
    "# Install helper functions.\n",
    "#!pip install -q git+https://github.com/gmihaila/ml_things.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "420b2683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import io\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import csv\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from ml_things import plot_dict, plot_confusion_matrix, fix_text\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30b0402f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x17dec231b90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize seeder and randomness\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "#tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afa32ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of transformers model - will use already pretrained model.\n",
    "# Path of transformer model - will load your own model from local disk.\n",
    "#model_name_or_path = 'microsoft/CodeGPT-small-py' # 'microsoft/CodeGPT-small-py' # 'gpt2'\n",
    "model_name_or_path = './model_logs_fromScratch' # './model_logs' './model_logs_fromScratch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4c2b42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "def dropEmpty(tokens0):\n",
    "    tokens = []\n",
    "    for i in range(0, len(tokens0)):\n",
    "        temp = tokens0[i]\n",
    "        if temp != []:\n",
    "            tokens.append(temp)\n",
    "    return tokens\n",
    "\n",
    "# Read pre-processed dataset\n",
    "with open('data_reduced_bert.csv', newline='', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f)\n",
    "    data = list(reader)\n",
    "data = dropEmpty(data)\n",
    "random.shuffle(data)\n",
    "#data = data[0:100] # subsample for checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6708a42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_successive_duplicates(list_of_lists, element_to_remove):\n",
    "#     result = []\n",
    "    \n",
    "#     for sublist in list_of_lists:\n",
    "#         cleaned_sublist = []\n",
    "#         previous_element = None\n",
    "        \n",
    "#         for current_element in sublist:\n",
    "#             if current_element == element_to_remove and current_element == previous_element:\n",
    "#                 continue  # Skip successive duplicates of the specified element\n",
    "#             cleaned_sublist.append(current_element)\n",
    "#             previous_element = current_element\n",
    "        \n",
    "#         result.append(cleaned_sublist)\n",
    "    \n",
    "#     return result\n",
    "\n",
    "\n",
    "# element_to_remove = \"ID\"\n",
    "# data = remove_successive_duplicates(data, element_to_remove)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24146b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getLengths(data):\n",
    "    lens = []\n",
    "    for i in range(len(data)):\n",
    "        lens.append(len(data[i])-2)\n",
    "    lens = pd.DataFrame(lens)\n",
    "    lensFreq = lens[0].value_counts()\n",
    "    lensFreq=pd.DataFrame(lensFreq)\n",
    "    return lens, lensFreq\n",
    "\n",
    "lens, lensFreq = getLengths(data)\n",
    "max_len = max(lens[0])\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8abd5231",
   "metadata": {},
   "outputs": [],
   "source": [
    "labs = []\n",
    "for d in data:\n",
    "    labs.append(d[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45afda3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the dataset's repository\n",
    "val_ratio = 0.15\n",
    "\n",
    "## split dataset to train-val-test sets\n",
    "### split data into train and test (85% train, 15% test)\n",
    "train_val, test = train_test_split(data, test_size=val_ratio, random_state=seed, stratify = labs)\n",
    "\n",
    "labs2 = []\n",
    "for d in train_val:\n",
    "    labs2.append(d[1])\n",
    "\n",
    "### split train and validation\n",
    "train, val = train_test_split(train_val, test_size=val_ratio, random_state=seed, stratify = labs2)\n",
    "\n",
    "\n",
    "## create the base directory if it doesn't exist\n",
    "import shutil\n",
    "if not os.path.exists('pythonvp'):\n",
    "    os.mkdir('pythonvp')\n",
    "else:\n",
    "    shutil.rmtree('pythonvp')\n",
    "    os.mkdir('pythonvp')\n",
    "\n",
    "## create subdirectories for clean and vuln\n",
    "for subset in ['train', 'val', 'test']:\n",
    "    if not os.path.exists(os.path.join('pythonvp', subset)):\n",
    "        os.mkdir(os.path.join('pythonvp', subset))\n",
    "\n",
    "    ### create subdirectories for train, val, and test\n",
    "    for category in ['clean', 'vuln']:\n",
    "        if not os.path.exists(os.path.join('pythonvp', subset, category)):\n",
    "            os.mkdir(os.path.join('pythonvp', subset, category))\n",
    "\n",
    "## write files to appropriate directories\n",
    "def makeRepo(data, subset):\n",
    "    for item in data:\n",
    "        filename = item[0]\n",
    "        category = 'clean' if item[1] == '0' else 'vuln'\n",
    "        body = ' '.join([str(token) for token in item[2:]])\n",
    "\n",
    "        with open(os.path.join('pythonvp', subset, category, filename[:-3] + '.py'), 'w') as f:\n",
    "            f.write(body)\n",
    "\n",
    "            \n",
    "## call makeRepo function\n",
    "makeRepo(train, 'train')\n",
    "makeRepo(val, 'val')\n",
    "makeRepo(test, 'test')\n",
    "\n",
    "## Merge train and val folders\n",
    "subset = \"train_val\"\n",
    "if not os.path.exists(os.path.join('pythonvp', subset)):\n",
    "        os.mkdir(os.path.join('pythonvp', subset))\n",
    "for category in ['clean', 'vuln']:\n",
    "        if not os.path.exists(os.path.join('pythonvp', subset, category)):\n",
    "            os.mkdir(os.path.join('pythonvp', subset, category))\n",
    "\n",
    "pososto = 1\n",
    "train_val = train_val[0:int(len(train_val)*pososto)]\n",
    "makeRepo(train_val, 'train_val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6354bc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define models and parameters\n",
    "from transformers import (set_seed,\n",
    "                          TrainingArguments,\n",
    "                          Trainer,\n",
    "                          GPT2Config,\n",
    "                          GPT2Tokenizer,\n",
    "                          AdamW, \n",
    "                          get_linear_schedule_with_warmup,\n",
    "                          GPT2ForSequenceClassification)\n",
    "\n",
    "# Set seed for reproducibility.\n",
    "set_seed(seed)\n",
    "\n",
    "# Number of training epochs (authors on fine-tuning Bert recommend between 2 and 4).\n",
    "epochs = 10\n",
    "patience = 2\n",
    "\n",
    "# Number of batches - depending on the max sequence length and GPU memory.\n",
    "# For 512 sequence length batch of 10 works without cuda memory issues.\n",
    "# For small sequence length can try batch of 32 or higher.\n",
    "batch_size = 6\n",
    "\n",
    "# Pad or truncate text sequences to a specific length\n",
    "# if `None` it will use maximum sequence of word piece tokens allowed by model.\n",
    "max_length = 512\n",
    "\n",
    "# Look for gpu to use. Will use `cpu` by default if no gpu found.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Dictionary of labels and their id - this will be used to convert.\n",
    "# String labels to number ids.\n",
    "labels_ids = {'clean': 0, 'vuln': 1}\n",
    "\n",
    "# How many labels are we using in training.\n",
    "# This is used to decide size of classification head.\n",
    "n_labels = len(labels_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc1f808e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VulnCodeDataset(Dataset):\n",
    "  r\"\"\"PyTorch Dataset class for loading data.\n",
    "\n",
    "  This is where the data parsing happens.\n",
    "\n",
    "  This class is built with reusability in mind: it can be used as is as.\n",
    "\n",
    "  Arguments:\n",
    "\n",
    "    path (:obj:`str`):\n",
    "        Path to the data partition.\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, path, use_tokenizer):\n",
    "\n",
    "    # Check if path exists.\n",
    "    if not os.path.isdir(path):\n",
    "      # Raise error if path is invalid.\n",
    "      raise ValueError('Invalid `path` variable! Needs to be a directory')\n",
    "    self.texts = []\n",
    "    self.labels = []\n",
    "    # Since the labels are defined by folders with data we loop \n",
    "    # through each label.\n",
    "    for label in ['vuln', 'clean']:\n",
    "      code_path = os.path.join(path, label)\n",
    "\n",
    "      # Get all files from path.\n",
    "      files_names = os.listdir(code_path)#[:10] # Sample for debugging.\n",
    "      # Go through each file and read its content.\n",
    "      for file_name in tqdm(files_names, desc=f'{label} files'):\n",
    "        file_path = os.path.join(code_path, file_name)\n",
    "\n",
    "        # Read content.\n",
    "        \n",
    "        #content = io.open(file_path, mode='r', encoding='utf-8').read()\n",
    "        content = io.open(file_path, mode='r', encoding=\"ISO-8859-1\").read()\n",
    "        # Fix any unicode issues.\n",
    "        content = fix_text(content)\n",
    "        # Save content.\n",
    "        self.texts.append(content)\n",
    "        # Save encode labels.\n",
    "        self.labels.append(label)\n",
    "\n",
    "    # Number of exmaples.\n",
    "    self.n_examples = len(self.labels)\n",
    "    \n",
    "\n",
    "    return\n",
    "\n",
    "  def __len__(self):\n",
    "    r\"\"\"When used `len` return the number of examples.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    return self.n_examples\n",
    "\n",
    "  def __getitem__(self, item):\n",
    "    r\"\"\"Given an index return an example from the position.\n",
    "    \n",
    "    Arguments:\n",
    "\n",
    "      item (:obj:`int`):\n",
    "          Index position to pick an example to return.\n",
    "\n",
    "    Returns:\n",
    "      :obj:`Dict[str, str]`: Dictionary of inputs that contain text and \n",
    "      asociated labels.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    return {'text':self.texts[item],\n",
    "            'label':self.labels[item]}\n",
    "\n",
    "\n",
    "\n",
    "class Gpt2ClassificationCollator(object):\n",
    "    r\"\"\"\n",
    "    Data Collator used for GPT2 in a classificaiton rask. \n",
    "    \n",
    "    It uses a given tokenizer and label encoder to convert any text and labels to numbers that \n",
    "    can go straight into a GPT2 model.\n",
    "\n",
    "    This class is built with reusability in mind: it can be used as is as long\n",
    "    as the `dataloader` outputs a batch in dictionary format that can be passed \n",
    "    straight into the model - `model(**batch)`.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "      use_tokenizer (:obj:`transformers.tokenization_?`):\n",
    "          Transformer type tokenizer used to process raw text into numbers.\n",
    "\n",
    "      labels_ids (:obj:`dict`):\n",
    "          Dictionary to encode any labels names into numbers. Keys map to \n",
    "          labels names and Values map to number associated to those labels.\n",
    "\n",
    "      max_sequence_len (:obj:`int`, `optional`)\n",
    "          Value to indicate the maximum desired sequence to truncate or pad text\n",
    "          sequences. If no value is passed it will used maximum sequence size\n",
    "          supported by the tokenizer and model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, use_tokenizer, labels_encoder, max_sequence_len=None):\n",
    "\n",
    "        # Tokenizer to be used inside the class.\n",
    "        self.use_tokenizer = use_tokenizer\n",
    "        # Check max sequence length.\n",
    "        self.max_sequence_len = use_tokenizer.model_max_length if max_sequence_len is None else max_sequence_len\n",
    "        # Label encoder used inside the class.\n",
    "        self.labels_encoder = labels_encoder\n",
    "\n",
    "        return\n",
    "\n",
    "    def __call__(self, sequences):\n",
    "        r\"\"\"\n",
    "        This function allowes the class objesct to be used as a function call.\n",
    "        Sine the PyTorch DataLoader needs a collator function, I can use this \n",
    "        class as a function.\n",
    "\n",
    "        Arguments:\n",
    "\n",
    "          item (:obj:`list`):\n",
    "              List of texts and labels.\n",
    "\n",
    "        Returns:\n",
    "          :obj:`Dict[str, object]`: Dictionary of inputs that feed into the model.\n",
    "          It holddes the statement `model(**Returned Dictionary)`.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get all texts from sequences list.\n",
    "        texts = [sequence['text'] for sequence in sequences]\n",
    "        # Get all labels from sequences list.\n",
    "        labels = [sequence['label'] for sequence in sequences]\n",
    "        # Encode all labels using label encoder.\n",
    "        labels = [self.labels_encoder[label] for label in labels]\n",
    "        # Call tokenizer on all texts to convert into tensors of numbers with \n",
    "        # appropriate padding.\n",
    "        inputs = self.use_tokenizer(text=texts, return_tensors=\"pt\", padding=True, truncation=True,  max_length=self.max_sequence_len)\n",
    "        # Update the inputs with the associated encoded labels as tensor.\n",
    "        inputs.update({'labels':torch.tensor(labels)})\n",
    "\n",
    "        return inputs\n",
    "\n",
    "\n",
    "def train(dataloader, optimizer_, scheduler_, device_):\n",
    "  r\"\"\"\n",
    "  Train pytorch model on a single pass through the data loader.\n",
    "\n",
    "  It will use the global variable `model` which is the transformer model \n",
    "  loaded on `_device` that we want to train on.\n",
    "\n",
    "  This function is built with reusability in mind: it can be used as is as long\n",
    "    as the `dataloader` outputs a batch in dictionary format that can be passed \n",
    "    straight into the model - `model(**batch)`.\n",
    "\n",
    "  Arguments:\n",
    "\n",
    "      dataloader (:obj:`torch.utils.data.dataloader.DataLoader`):\n",
    "          Parsed data into batches of tensors.\n",
    "\n",
    "      optimizer_ (:obj:`transformers.optimization.AdamW`):\n",
    "          Optimizer used for training.\n",
    "\n",
    "      scheduler_ (:obj:`torch.optim.lr_scheduler.LambdaLR`):\n",
    "          PyTorch scheduler.\n",
    "\n",
    "      device_ (:obj:`torch.device`):\n",
    "          Device used to load tensors before feeding to model.\n",
    "\n",
    "  Returns:\n",
    "\n",
    "      :obj:`List[List[int], List[int], float]`: List of [True Labels, Predicted\n",
    "        Labels, Train Average Loss].\n",
    "  \"\"\"\n",
    "\n",
    "  # Use global variable for model.\n",
    "  global model\n",
    "\n",
    "  # Tracking variables.\n",
    "  predictions_labels = []\n",
    "  true_labels = []\n",
    "  # Total loss for this epoch.\n",
    "  total_loss = 0\n",
    "\n",
    "  # Put the model into training mode.\n",
    "  model.train()\n",
    "\n",
    "  # For each batch of training data...\n",
    "  for batch in tqdm(dataloader, total=len(dataloader)):\n",
    "\n",
    "    # Add original labels - use later for evaluation.\n",
    "    true_labels += batch['labels'].numpy().flatten().tolist()\n",
    "    \n",
    "    # move batch to device\n",
    "    batch = {k:v.type(torch.long).to(device_) for k,v in batch.items()}\n",
    "    \n",
    "    # Always clear any previously calculated gradients before performing a\n",
    "    # backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Perform a forward pass (evaluate the model on this training batch).\n",
    "    # This will return the loss (rather than the model output) because we\n",
    "    # have provided the `labels`.\n",
    "    # The documentation for this a bert model function is here: \n",
    "    # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "    outputs = model(**batch)\n",
    "\n",
    "    # The call to `model` always returns a tuple, so we need to pull the \n",
    "    # loss value out of the tuple along with the logits. We will use logits\n",
    "    # later to calculate training accuracy.\n",
    "    loss, logits = outputs[:2]\n",
    "\n",
    "    # Accumulate the training loss over all of the batches so that we can\n",
    "    # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "    # single value; the `.item()` function just returns the Python value \n",
    "    # from the tensor.\n",
    "    total_loss += loss.item()\n",
    "\n",
    "    # Perform a backward pass to calculate the gradients.\n",
    "    loss.backward()\n",
    "\n",
    "    # Clip the norm of the gradients to 1.0.\n",
    "    # This is to help prevent the \"exploding gradients\" problem.\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    # Update parameters and take a step using the computed gradient.\n",
    "    # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "    # modified based on their gradients, the learning rate, etc.\n",
    "    optimizer.step()\n",
    "\n",
    "    # Update the learning rate.\n",
    "    scheduler.step()\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "\n",
    "    # Convert these logits to list of predicted labels values.\n",
    "    predictions_labels += logits.argmax(axis=-1).flatten().tolist()\n",
    "\n",
    "  # Calculate the average loss over the training data.\n",
    "  avg_epoch_loss = total_loss / len(dataloader)\n",
    "  \n",
    "  # Return all true labels and prediction for future evaluations.\n",
    "  return true_labels, predictions_labels, avg_epoch_loss\n",
    "\n",
    "\n",
    "\n",
    "def validation(dataloader, device_):\n",
    "  r\"\"\"Validation function to evaluate model performance on a \n",
    "  separate set of data.\n",
    "\n",
    "  This function will return the true and predicted labels so we can use later\n",
    "  to evaluate the model's performance.\n",
    "\n",
    "  This function is built with reusability in mind: it can be used as is as long\n",
    "    as the `dataloader` outputs a batch in dictionary format that can be passed \n",
    "    straight into the model - `model(**batch)`.\n",
    "\n",
    "  Arguments:\n",
    "\n",
    "    dataloader (:obj:`torch.utils.data.dataloader.DataLoader`):\n",
    "          Parsed data into batches of tensors.\n",
    "\n",
    "    device_ (:obj:`torch.device`):\n",
    "          Device used to load tensors before feeding to model.\n",
    "\n",
    "  Returns:\n",
    "    \n",
    "    :obj:`List[List[int], List[int], float]`: List of [True Labels, Predicted\n",
    "        Labels, Train Average Loss]\n",
    "  \"\"\"\n",
    "\n",
    "  # Use global variable for model.\n",
    "  global model\n",
    "\n",
    "  # Tracking variables\n",
    "  predictions_labels = []\n",
    "  true_labels = []\n",
    "  #total loss for this epoch.\n",
    "  total_loss = 0\n",
    "\n",
    "  # Put the model in evaluation mode--the dropout layers behave differently\n",
    "  # during evaluation.\n",
    "  model.eval()\n",
    "\n",
    "  # Evaluate data for one epoch\n",
    "  for batch in tqdm(dataloader, total=len(dataloader)):\n",
    "\n",
    "    # add original labels\n",
    "    true_labels += batch['labels'].numpy().flatten().tolist()\n",
    "\n",
    "    # move batch to device\n",
    "    batch = {k:v.type(torch.long).to(device_) for k,v in batch.items()}\n",
    "\n",
    "    # Telling the model not to compute or store gradients, saving memory and\n",
    "    # speeding up validation\n",
    "    with torch.no_grad():        \n",
    "\n",
    "        # Forward pass, calculate logit predictions.\n",
    "        # This will return the logits rather than the loss because we have\n",
    "        # not provided labels.\n",
    "        # token_type_ids is the same as the \"segment ids\", which \n",
    "        # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        # The call to `model` always returns a tuple, so we need to pull the \n",
    "        # loss value out of the tuple along with the logits. We will use logits\n",
    "        # later to to calculate training accuracy.\n",
    "        loss, logits = outputs[:2]\n",
    "        \n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # get predicitons to list\n",
    "        predict_content = logits.argmax(axis=-1).flatten().tolist()\n",
    "\n",
    "        # update list\n",
    "        predictions_labels += predict_content\n",
    "\n",
    "  # Calculate the average loss over the training data.\n",
    "  avg_epoch_loss = total_loss / len(dataloader)\n",
    "\n",
    "  # Return all true labels and prediciton for future evaluations.\n",
    "  return true_labels, predictions_labels, avg_epoch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2fa733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUniques(data):\n",
    "    allWords = []\n",
    "    for i in range(len(data)):\n",
    "        for j in range(2, len(data[i])):\n",
    "            allWords.append(data[i][j])\n",
    "    \n",
    "    vc = pd.Series(allWords).value_counts()\n",
    "    uniques=vc.index.values.tolist()        \n",
    "    return allWords, uniques, vc\n",
    "\n",
    "def getVocab(data):\n",
    "    ## split dataset to train-val-test sets\n",
    "    ### split data into train and test (85% train, 15% test)\n",
    "    train_val_indices, test_indices = train_test_split(range(len(data)), test_size=val_ratio, random_state=seed)\n",
    "    train_val_data = [data[i] for i in train_val_indices]\n",
    "\n",
    "    allWords, vocab, freqs = getUniques(train_val_data)\n",
    "    return vocab\n",
    "\n",
    "def addTokens(data, tokenizer, model):\n",
    "    new_tokens = getVocab(data)\n",
    "\n",
    "    for new_token in new_tokens:\n",
    "        if new_token not in tokenizer.get_vocab().keys():\n",
    "            tokenizer.add_tokens(new_token)\n",
    "\n",
    "    # resize model embedding to match new tokenizer\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41358f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configuraiton...\n",
      "Loading tokenizer...\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./model_logs_fromScratch were not used when initializing GPT2ForSequenceClassification: ['lm_head.weight']\n",
      "- This IS expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at ./model_logs_fromScratch and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded to `cuda`\n"
     ]
    }
   ],
   "source": [
    "# Get model configuration.\n",
    "print('Loading configuraiton...')\n",
    "model_config = GPT2Config.from_pretrained(pretrained_model_name_or_path=model_name_or_path, num_labels=n_labels)\n",
    "\n",
    "# Get model's tokenizer.\n",
    "print('Loading tokenizer...')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_name_or_path='./model_logs', do_lower_case = True)\n",
    "# default to left padding\n",
    "tokenizer.padding_side = \"left\"\n",
    "# Define PAD Token = EOS Token = 50256\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "# Get the actual model.\n",
    "print('Loading model...')\n",
    "model = GPT2ForSequenceClassification.from_pretrained(pretrained_model_name_or_path=model_name_or_path, config=model_config)\n",
    "\n",
    "# resize model embedding to match new tokenizer\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# fix model padding token id\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# Define New tokens for string and numerical i.e., strId$ and numId$\n",
    "new_tokens = [\"strId$\", \"numId$\"]\n",
    "for new_token in new_tokens:\n",
    "    if new_token not in tokenizer.get_vocab().keys():\n",
    "        tokenizer.add_tokens(new_token)\n",
    "\n",
    "# resize model embedding to match new tokenizer\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# add all new tokens\n",
    "#tokenizer, model = addTokens(data, tokenizer, model)\n",
    "\n",
    "# fix model new tokens ids\n",
    "\n",
    "# Load model to defined device.\n",
    "model.to(device)\n",
    "print('Model loaded to `%s`'%device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8beaa7da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 4342,   318,   281,  1672,  6827,   351, 50259,   392, 50260,    83,\n",
       "           482,   641,    13]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input = tokenizer(\"Here is an example sentence with strId$ and numId$ tokens.\", padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89434919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dealing with Train...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49657c1224c849c7952b5cc244eeba4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vuln files:   0%|          | 0/721 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a8e1ab2ace145738bc79ac923c9477d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "clean files:   0%|          | 0/2301 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created `train_dataset` with 3022 examples!\n",
      "Created `train_dataloader` with 504 batches!\n",
      "\n",
      "Dealing with Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "effc46364e2c44a68e87f9718d1876d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vuln files:   0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d609f7abb5644aa19cc1c046a626bf14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "clean files:   0%|          | 0/407 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created `valid_dataset` with 534 examples!\n",
      "Created `eval_dataloader` with 89 batches!\n",
      "Dealing with Testing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b38597ba70384a83a62287dc48cd3a2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vuln files:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cba7b02f1fd46c4ae0036bee05c48ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "clean files:   0%|          | 0/478 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created `test_dataset` with 628 examples!\n",
      "Created `eval_dataloader` with 105 batches!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc558c7781f44c06a7bf413cbbc71463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vuln files:   0%|          | 0/848 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d3806f335954da9b65604061e73094f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "clean files:   0%|          | 0/2708 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created `train_dataset` with 3556 examples!\n",
      "Created `train_dataloader` with 593 batches!\n"
     ]
    }
   ],
   "source": [
    "# Create data collator to encode text and labels into numbers.\n",
    "gpt2_classificaiton_collator = Gpt2ClassificationCollator(use_tokenizer=tokenizer, \n",
    "                                                          labels_encoder=labels_ids, \n",
    "                                                          max_sequence_len=max_length)\n",
    "\n",
    "\n",
    "print('Dealing with Train...')\n",
    "# Create pytorch dataset.\n",
    "train_dataset = VulnCodeDataset(path='pythonvp\\\\train', \n",
    "                               use_tokenizer=tokenizer)\n",
    "print('Created `train_dataset` with %d examples!'%len(train_dataset))\n",
    "\n",
    "# Move pytorch dataset into dataloader.\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=gpt2_classificaiton_collator)\n",
    "print('Created `train_dataloader` with %d batches!'%len(train_dataloader))\n",
    "\n",
    "print()\n",
    "\n",
    "print('Dealing with Validation...')\n",
    "# Create pytorch dataset.\n",
    "valid_dataset =  VulnCodeDataset(path='pythonvp\\\\val', \n",
    "                               use_tokenizer=tokenizer)\n",
    "print('Created `valid_dataset` with %d examples!'%len(valid_dataset))\n",
    "\n",
    "# Move pytorch dataset into dataloader.\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=gpt2_classificaiton_collator)\n",
    "print('Created `eval_dataloader` with %d batches!'%len(valid_dataloader))\n",
    "\n",
    "print('Dealing with Testing...')\n",
    "# Create pytorch dataset.\n",
    "test_dataset =  VulnCodeDataset(path='pythonvp\\\\test', \n",
    "                               use_tokenizer=tokenizer)\n",
    "print('Created `test_dataset` with %d examples!'%len(test_dataset))\n",
    "\n",
    "# Move pytorch dataset into dataloader.\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=gpt2_classificaiton_collator)\n",
    "print('Created `eval_dataloader` with %d batches!'%len(test_dataloader))\n",
    "\n",
    "\n",
    "\n",
    "# Create pytorch dataset.\n",
    "train_val_dataset = VulnCodeDataset(path='pythonvp\\\\train_val', \n",
    "                               use_tokenizer=tokenizer)\n",
    "print('Created `train_dataset` with %d examples!'%len(train_val_dataset))\n",
    "\n",
    "# Move pytorch dataset into dataloader.\n",
    "train_val_dataloader = DataLoader(train_val_dataset, batch_size=batch_size, shuffle=True, collate_fn=gpt2_classificaiton_collator)\n",
    "print('Created `train_dataloader` with %d batches!'%len(train_val_dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c6d34f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ilias\\anaconda3\\envs\\torchenv\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # default is 1e-8.\n",
    "                  )\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "# `train_dataloader` contains batched data so `len(train_dataloader)` gives \n",
    "# us the number of batches.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "# Store the average loss after each epoch so we can plot them.\n",
    "all_loss = {'train_loss':[], 'val_loss':[]}\n",
    "all_acc = {'train_acc':[], 'val_acc':[]}\n",
    "valid_f2 = {'val_f2':[]}\n",
    "\n",
    "# Loop through each epoch.\n",
    "# ep=0\n",
    "# count = 0\n",
    "# for epoch in tqdm(range(epochs)):\n",
    "#   ep=ep+1\n",
    "#   print(\"Epoch \", ep)\n",
    "#   print('Training on batches...')\n",
    "#   # Perform one full pass over the training set.\n",
    "#   train_labels, train_predict, train_loss = train(train_dataloader, optimizer, scheduler, device)\n",
    "#   train_acc = accuracy_score(train_labels, train_predict)\n",
    "\n",
    "#   # Get prediction form model on validation data. \n",
    "#   print('Validation on batches...')\n",
    "#   valid_labels, valid_predict, val_loss = validation(valid_dataloader, device)\n",
    "#   val_acc = accuracy_score(valid_labels, valid_predict)\n",
    "\n",
    "#   # Print loss and accuracy values to see how training evolves.\n",
    "#   print(\"  train_loss: %.5f - val_loss: %.5f - train_acc: %.5f - valid_acc: %.5f\"%(train_loss, val_loss, train_acc, val_acc))\n",
    "#   print()\n",
    "  \n",
    "#   tn, fp, fn, tp = confusion_matrix(valid_labels, valid_predict).ravel()\n",
    "#   val_acc=(tp+tn)/(tp+tn+fp+fn)\n",
    "#   val_prec=tp/(tp+fp)\n",
    "#   val_rec=tp/(tp+fn)\n",
    "#   val_f1=2*val_prec*val_rec / (val_prec+val_rec)\n",
    "#   val_f2=5*val_prec*val_rec / (4*val_prec+val_rec)\n",
    "#   #print(\"Accuracy: \", val_acc)\n",
    "#   #print(\"Precision: \", val_prec)\n",
    "#   #print(\"Recall: \", val_rec)\n",
    "#   #print(\"F1-score: \", val_f1)\n",
    "#   print(\"Validation F2-score: \", val_f2)\n",
    "\n",
    "#   # Store the loss value for plotting the learning curve.\n",
    "#   all_loss['train_loss'].append(train_loss)\n",
    "#   all_loss['val_loss'].append(val_loss)\n",
    "#   all_acc['train_acc'].append(train_acc)\n",
    "#   all_acc['val_acc'].append(val_acc)\n",
    "#   valid_f2['val_f2'].append(val_f2)\n",
    "\n",
    "# #   if len(valid_f2['val_f2']) > 2:\n",
    "# #       if val_f2 < valid_f2['val_f2'][-2]:\n",
    "# #           count = count + 1\n",
    "# #           if count >= patience:\n",
    "# #             break\n",
    "    \n",
    "# # Plot loss curves.\n",
    "# plot_dict(all_loss, use_xlabel='Epochs', use_ylabel='Value', use_linestyles=['-', '--'])\n",
    "\n",
    "# # Plot accuracy curves.\n",
    "# plot_dict(all_acc, use_xlabel='Epochs', use_ylabel='Value', use_linestyles=['-', '--'])\n",
    "\n",
    "# # Plot f2 curve.\n",
    "# plot_dict(valid_f2, use_xlabel='Epochs', use_ylabel='Value', use_linestyles=['-', '--'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51a4b224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get prediction form model on validation data. This is where you should use your val data.\n",
    "\n",
    "# # Create the evaluation report.\n",
    "# evaluation_report = classification_report(valid_labels, valid_predict, labels=list(labels_ids.values()), target_names=list(labels_ids.keys()))\n",
    "# # Show the evaluation report.\n",
    "# print(evaluation_report)\n",
    "\n",
    "# # Plot confusion matrix.\n",
    "# plot_confusion_matrix(y_true=valid_labels, y_pred=valid_predict, \n",
    "#                       classes=list(labels_ids.keys()),\n",
    "#                       );\n",
    "\n",
    "# cm = confusion_matrix(valid_labels, valid_predict, labels=[0, 1])\n",
    "# tn, fp, fn, tp = confusion_matrix(valid_labels, valid_predict).ravel()\n",
    "# print(tn, fp, fn, tp)\n",
    "\n",
    "# val_acc=(tp+tn)/(tp+tn+fp+fn)\n",
    "# val_prec=tp/(tp+fp)\n",
    "# val_rec=tp/(tp+fn)\n",
    "# val_f1=2*val_prec*val_rec / (val_prec+val_rec)\n",
    "# val_f2=5*val_prec*val_rec / (4*val_prec+val_rec)\n",
    "# print(\"Accuracy: \", val_acc)\n",
    "# print(\"Precision: \", val_prec)\n",
    "# print(\"Recall: \", val_rec)\n",
    "# print(\"F1-score: \", val_f1)\n",
    "# print(\"F2-score: \", val_f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04d23a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6382a7a229bf4b72b31f9bf058af7b78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on batches...\n",
      "Epoch  1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c67ddd9e0734ff8bf375356ff007fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/593 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train_loss: 0.04958 - train_acc: 0.99494:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fbf27f1fc474a06bba40f8da58e7c94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7611464968152867\n",
      "Precision:  nan\n",
      "Recall:  0.0\n",
      "F1-score:  nan\n",
      "F2-score:  nan\n",
      "\n",
      "Training on batches...\n",
      "Epoch  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ilias\\AppData\\Local\\Temp\\ipykernel_17892\\3309722742.py:38: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  prec=tp/(tp+fp)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a387cfd252142ae92f7c4b89b13b91c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/593 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  train_loss: 0.13226 - train_acc: 0.98678:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f952cdcd489408cbac2ea2ce1c8ecee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7611464968152867\n",
      "Precision:  nan\n",
      "Recall:  0.0\n",
      "F1-score:  nan\n",
      "F2-score:  nan\n",
      "\n",
      "Training on batches...\n",
      "Epoch  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ilias\\AppData\\Local\\Temp\\ipykernel_17892\\3309722742.py:38: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  prec=tp/(tp+fp)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d6de0c8295249e7b42d09c4e86caabf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/593 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;124m\"\u001b[39m, epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Perform one full pass over the training set.\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m train_val_labels, train_val_predict, train_val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_val_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m train_val_acc \u001b[38;5;241m=\u001b[39m accuracy_score(train_val_labels, train_val_predict)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Print loss and accuracy values to see how training evolves.\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 207\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataloader, optimizer_, scheduler_, device_)\u001b[0m\n\u001b[0;32m    200\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    202\u001b[0m \u001b[38;5;66;03m# Perform a forward pass (evaluate the model on this training batch).\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;66;03m# This will return the loss (rather than the model output) because we\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;66;03m# have provided the `labels`.\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;66;03m# The documentation for this a bert model function is here: \u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;66;03m# https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\u001b[39;00m\n\u001b[1;32m--> 207\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[0;32m    209\u001b[0m \u001b[38;5;66;03m# The call to `model` always returns a tuple, so we need to pull the \u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# loss value out of the tuple along with the logits. We will use logits\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;66;03m# later to calculate training accuracy.\u001b[39;00m\n\u001b[0;32m    212\u001b[0m loss, logits \u001b[38;5;241m=\u001b[39m outputs[:\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torchenv\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1370\u001b[0m, in \u001b[0;36mGPT2ForSequenceClassification.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1362\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1363\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1364\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1365\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1366\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1367\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1368\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1370\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1371\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1377\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1379\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1380\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1382\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1383\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1384\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore(hidden_states)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torchenv\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:887\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    877\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    878\u001b[0m         create_custom_forward(block),\n\u001b[0;32m    879\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    884\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    885\u001b[0m     )\n\u001b[0;32m    886\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 887\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    896\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    898\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    899\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torchenv\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:388\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    386\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m    387\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[1;32m--> 388\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    396\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[0;32m    397\u001b[0m outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torchenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torchenv\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:329\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    327\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 329\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    331\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_heads(attn_output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[0;32m    332\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(attn_output)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torchenv\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:189\u001b[0m, in \u001b[0;36mGPT2Attention._attn\u001b[1;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[0;32m    184\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m attn_weights \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull(\n\u001b[0;32m    185\u001b[0m         [], value\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mattn_weights\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mattn_weights\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m    186\u001b[0m     )\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m# Layer-wise attention scaling\u001b[39;00m\n\u001b[1;32m--> 189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_attn_by_inverse_layer_idx:\n\u001b[0;32m    190\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m attn_weights \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_cross_attention:\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;66;03m# if only \"normal\" attention layer implements causal mask\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "new_epochs = 15 # selected number by validation\n",
    "\n",
    "# train on train_val set for optimal n_epochs\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, \n",
    "                  eps = 1e-8 #\n",
    "                  )\n",
    "\n",
    "total_steps = len(train_val_dataloader) * new_epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "\n",
    "# Loop through each epoch.\n",
    "print('Epoch')\n",
    "for epoch in tqdm(range(new_epochs)):\n",
    "  print()\n",
    "  print('Training on batches...')\n",
    "  print(\"Epoch \", epoch + 1)\n",
    "  # Perform one full pass over the training set.\n",
    "  train_val_labels, train_val_predict, train_val_loss = train(train_val_dataloader, optimizer, scheduler, device)\n",
    "  train_val_acc = accuracy_score(train_val_labels, train_val_predict)\n",
    "\n",
    "\n",
    "  # Print loss and accuracy values to see how training evolves.\n",
    "  print(\"  train_loss: %.5f - train_acc: %.5f:\"%(train_val_loss, train_val_acc))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  true_labels, predictions_labels, avg_epoch_loss = validation(test_dataloader, device)\n",
    "  tn, fp, fn, tp = confusion_matrix(true_labels, predictions_labels).ravel()\n",
    "  acc=(tp+tn)/(tp+tn+fp+fn)\n",
    "  prec=tp/(tp+fp)\n",
    "  rec=tp/(tp+fn)\n",
    "  f1=2*prec*rec / (prec+rec)\n",
    "  f2=5*prec*rec / (4*prec+rec)\n",
    "  print(\"Accuracy: \", acc)\n",
    "  print(\"Precision: \", prec)\n",
    "  print(\"Recall: \", rec)\n",
    "  print(\"F1-score: \", f1)\n",
    "  print(\"F2-score: \", f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46519436",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get prediction form model on testing data. This is where you should use\n",
    "# your test data.\n",
    "true_labels, predictions_labels, avg_epoch_loss = validation(test_dataloader, device)\n",
    "\n",
    "# Create the evaluation report.\n",
    "evaluation_report = classification_report(true_labels, predictions_labels, labels=list(labels_ids.values()), target_names=list(labels_ids.keys()))\n",
    "# Show the evaluation report.\n",
    "print(evaluation_report)\n",
    "\n",
    "# Plot confusion matrix.\n",
    "plot_confusion_matrix(y_true=true_labels, y_pred=predictions_labels, \n",
    "                      classes=list(labels_ids.keys()),\n",
    "                      );\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c084ca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(true_labels, predictions_labels, labels=[0, 1])\n",
    "tn, fp, fn, tp = confusion_matrix(true_labels, predictions_labels).ravel()\n",
    "(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053d9a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc=(tp+tn)/(tp+tn+fp+fn)\n",
    "prec=tp/(tp+fp)\n",
    "rec=tp/(tp+fn)\n",
    "f1=2*prec*rec / (prec+rec)\n",
    "f2=5*prec*rec / (4*prec+rec)\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"Precision: \", prec)\n",
    "print(\"Recall: \", rec)\n",
    "print(\"F1-score: \", f1)\n",
    "print(\"F2-score: \", f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e0825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "model_name = \"./model_logs\"  # Or use any other GPT-2 variant ./model_logs_fromScratch\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d594d36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"I am a mad king.\"\n",
    "input_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors=\"tf\").numpy()\n",
    "\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb166130",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = model.transformer.wte.weight\n",
    "word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d099cfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_embeddings = model.transformer.wpe.weight\n",
    "pos_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872b3fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_embedding = model.transformer.wte.weight[input_ids, :]\n",
    "word_embedding = model.transformer.wte.weight[8805, :]\n",
    "word_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78ff84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding.size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
