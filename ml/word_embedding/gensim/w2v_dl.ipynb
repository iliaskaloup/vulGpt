{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "78181764-bb06-4f1d-904b-8d2132d6d3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import json, os, io\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, \\\n",
    "roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import LSTM, SimpleRNN\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import Masking\n",
    "from tensorflow.keras.layers import Embedding, MaxPool1D\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.layers import Bidirectional, BatchNormalization\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.initializers import glorot_uniform, RandomUniform, lecun_uniform, Constant\n",
    "from collections import OrderedDict\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, GlobalMaxPool1D\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalMaxPool1D\n",
    "from keras_preprocessing.text import tokenizer_from_json\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import defaultdict\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5928800-7629-4bd3-8c8b-2153ca615eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeders = [123456, 789012, 345678, 901234, 567890, 123, 456, 789, 123, 456]\n",
    "\n",
    "seed = seeders[0]\n",
    "\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2a363a04-b758-4cda-826b-3f4b0299baaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"w2v\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c802ac2-a13b-477b-9174-8dd29e4c9fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = os.path.join('..', '..', '..')\n",
    "dataset = pd.read_csv(os.path.join(root_path, 'data', 'train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4bccd858-290e-4ec5-88ea-f3322ea866f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    index Access Gained Attack Origin Authentication Required Availability  \\\n",
      "0   22328           NaN         Local            Not required     Complete   \n",
      "1   39415           NaN         Local            Not required          NaN   \n",
      "2   60861           NaN        Remote           Single system          NaN   \n",
      "3   84364           NaN         Local            Not required      Partial   \n",
      "4  177580           NaN        Remote            Not required     Complete   \n",
      "\n",
      "           CVE ID                                        CVE Page   CWE ID  \\\n",
      "0   CVE-2011-4621   https://www.cvedetails.com/cve/CVE-2011-4621/      NaN   \n",
      "1   CVE-2014-1738   https://www.cvedetails.com/cve/CVE-2014-1738/  CWE-264   \n",
      "2  CVE-2017-14604  https://www.cvedetails.com/cve/CVE-2017-14604/   CWE-20   \n",
      "3   CVE-2018-6560   https://www.cvedetails.com/cve/CVE-2018-6560/  CWE-436   \n",
      "4   CVE-2016-1621   https://www.cvedetails.com/cve/CVE-2016-1621/  CWE-119   \n",
      "\n",
      "  Complexity Confidentiality  ... parentID  \\\n",
      "0        Low             NaN  ...      NaN   \n",
      "1        Low        Complete  ...      NaN   \n",
      "2        Low             NaN  ...      NaN   \n",
      "3        Low         Partial  ...      NaN   \n",
      "4        Low        Complete  ...      NaN   \n",
      "\n",
      "                                               patch   project  \\\n",
      "0  @@ -641,17 +641,18 @@ static void sched_irq_ti...     linux   \n",
      "1  @@ -3067,7 +3067,10 @@ static int raw_cmd_copy...     linux   \n",
      "2  @@ -30,6 +30,7 @@\\n #include \"nautilus-global-...  nautilus   \n",
      "3  @@ -173,10 +173,11 @@\\n \\n typedef struct Flat...   flatpak   \n",
      "4  @@ -13,18 +13,18 @@\\n\\n #include <string.h>\\n ...   Android   \n",
      "\n",
      "                                       project_after  \\\n",
      "0           f26f9aff6aaf67e9a430d16c266f91b13a5bff64   \n",
      "1           2145e15e0557a01b9195d1c7199a1b92cb9be81f   \n",
      "2           1630f53481f445ada0a455e9979236d31a8d3bb0   \n",
      "3           52346bf187b5a7f1c0fe9075b328b7ad6abe78f6   \n",
      "4  https://android.googlesource.com/platform/exte...   \n",
      "\n",
      "                                      project_before target  \\\n",
      "0           0f004f5a696a9434b7214d0d3cbd0525ee77d428      0   \n",
      "1           ef87dbe7614341c2e7bfe8d32fcb7028cc97442c      0   \n",
      "2           cc6910ff6511a5a2939cf36a49ca81fb62005382      0   \n",
      "3           3c9d3a316ea298c25e8756ab4f256b08879aff36      0   \n",
      "4  https://android.googlesource.com/platform/exte...      0   \n",
      "\n",
      "                                   vul_func_with_fix  \\\n",
      "0  void account_system_time(struct task_struct *p...   \n",
      "1  static void redo_fd_request(void)\\n{\\n\\tint dr...   \n",
      "2  cancel_filesystem_info_for_file (NautilusDirec...   \n",
      "3  buffer_write (ProxySide *side,\\n              ...   \n",
      "4                      virtual ~Trans16x16DCT() {}\\n   \n",
      "\n",
      "                                      processed_func flaw_line flaw_line_index  \n",
      "0  void account_system_time(struct task_struct *p...       NaN             NaN  \n",
      "1  static void redo_fd_request(void)\\n{\\n\\tint dr...       NaN             NaN  \n",
      "2  cancel_filesystem_info_for_file (NautilusDirec...       NaN             NaN  \n",
      "3  buffer_write (ProxySide *side,\\n              ...       NaN             NaN  \n",
      "4                      virtual ~Trans16x16DCT() {}\\n       NaN             NaN  \n",
      "\n",
      "[5 rows x 39 columns]\n",
      "150908\n"
     ]
    }
   ],
   "source": [
    "data = dataset.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "print(data.head())\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "733f7e3e-736d-4875-9a63-91c53f19530e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89145\n"
     ]
    }
   ],
   "source": [
    "data = data[data[\"project\"] != \"Chrome\"]\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "672cf8f0-0eea-4264-99d8-d809c3593e5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed_func</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>void account_system_time(struct task_struct *p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>static void redo_fd_request(void)\\n{\\n\\tint dr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cancel_filesystem_info_for_file (NautilusDirec...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>buffer_write (ProxySide *side,\\n              ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>virtual ~Trans16x16DCT() {}\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      processed_func  target\n",
       "0  void account_system_time(struct task_struct *p...       0\n",
       "1  static void redo_fd_request(void)\\n{\\n\\tint dr...       0\n",
       "2  cancel_filesystem_info_for_file (NautilusDirec...       0\n",
       "3  buffer_write (ProxySide *side,\\n              ...       0\n",
       "4                      virtual ~Trans16x16DCT() {}\\n       0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[[\"processed_func\", \"target\"]]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bb3e8455-fdf2-42a2-b8c2-bcbfe3d183c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset=[\"processed_func\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "75fe6e74-12a6-4dd7-b39b-42ee98718972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of words: 15441\n"
     ]
    }
   ],
   "source": [
    "word_counts = data[\"processed_func\"].apply(lambda x: len(x.split()))\n",
    "max_length = word_counts.max()\n",
    "print(\"Maximum number of words:\", max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3639ee19-1df9-4ab4-a552-4987400952b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "0    83578\n",
      "1     5567\n",
      "Name: count, dtype: int64\n",
      "Percentage:  6.660843762712676 %\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "vc = data[\"target\"].value_counts()\n",
    "\n",
    "print(vc)\n",
    "\n",
    "print(\"Percentage: \", (vc[1] / vc[0])*100, '%')\n",
    "\n",
    "n_categories = len(vc)\n",
    "print(n_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cf7946a3-c961-4bd0-b2b7-037f4464827b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>void account_system_time(struct task_struct *p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>static void redo_fd_request(void)\\n{\\n\\tint dr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cancel_filesystem_info_for_file (NautilusDirec...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>buffer_write (ProxySide *side,\\n              ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>virtual ~Trans16x16DCT() {}\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Labels\n",
       "0  void account_system_time(struct task_struct *p...       0\n",
       "1  static void redo_fd_request(void)\\n{\\n\\tint dr...       0\n",
       "2  cancel_filesystem_info_for_file (NautilusDirec...       0\n",
       "3  buffer_write (ProxySide *side,\\n              ...       0\n",
       "4                      virtual ~Trans16x16DCT() {}\\n       0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.DataFrame(({'Text': data['processed_func'], 'Labels': data['target']}))\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4b3f7f0b-4396-42ec-b6bf-bb6ba6d535e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>int iwlagn_add_bssid_station(struct iwl_priv *...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>static int dnxhd_init_vlc(DNXHDContext *ctx, u...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>void CameraService::onFirstRef()\\n{\\n    LOG1(...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>int EmbedStream::getChar() {\\n  if (limited &amp;&amp;...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>json_t *json_rpc_call(CURL *curl, const char *...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Labels\n",
       "1  int iwlagn_add_bssid_station(struct iwl_priv *...       0\n",
       "2  static int dnxhd_init_vlc(DNXHDContext *ctx, u...       0\n",
       "3  void CameraService::onFirstRef()\\n{\\n    LOG1(...       0\n",
       "4  int EmbedStream::getChar() {\\n  if (limited &&...       0\n",
       "5  json_t *json_rpc_call(CURL *curl, const char *...       0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data = pd.read_csv(os.path.join(root_path, 'data', 'val.csv'))\n",
    "\n",
    "val_data = val_data[val_data[\"project\"] != \"Chrome\"]\n",
    "\n",
    "val_data = pd.DataFrame(({'Text': val_data['processed_func'], 'Labels': val_data['target']}))\n",
    "val_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0a769dab-548b-4e6b-9d40-ea49a28d8095",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(os.path.join(root_path, 'data', 'test.csv'))\n",
    "\n",
    "test_data = test_data[test_data[\"project\"] != \"Chrome\"]\n",
    "\n",
    "test_data = pd.DataFrame(({'Text': test_data['processed_func'], 'Labels': test_data['target']}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ab251257-55d0-44a7-86cc-e8de54fc2202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution  Labels\n",
      "0    83578\n",
      "1     5567\n",
      "Name: count, dtype: int64\n",
      "Majority class  0\n",
      "Minority class  1\n",
      "Targeted number of majority class 22268\n",
      "Class distribution after augmentation Labels\n",
      "0    22268\n",
      "1     5567\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "sampling = False\n",
    "if n_categories == 2 and sampling == True:\n",
    "    # Apply under-sampling with the specified strategy\n",
    "    class_counts = pd.Series(train_data[\"Labels\"]).value_counts()\n",
    "    print(\"Class distribution \", class_counts)\n",
    "\n",
    "    majority_class = class_counts.idxmax()\n",
    "    print(\"Majority class \", majority_class)\n",
    "\n",
    "    minority_class = class_counts.idxmin()\n",
    "    print(\"Minority class \", minority_class)\n",
    "\n",
    "    target_count = 4 * class_counts[class_counts.idxmin()] # int(class_counts[class_counts.idxmax()] / 2) # 2 * class_counts[class_counts.idxmin()] # class_counts[class_counts.idxmin()] # int(class_counts.iloc[0] / 2)  \n",
    "    print(\"Targeted number of majority class\", target_count)\n",
    "\n",
    "    # under\n",
    "    sampling_strategy = {majority_class: target_count}        \n",
    "    rus = RandomUnderSampler(random_state=seed, sampling_strategy=sampling_strategy)\n",
    "\n",
    "    x_train_resampled, y_train_resampled = rus.fit_resample(np.array(train_data[\"Text\"]).reshape(-1, 1), train_data[\"Labels\"]) \n",
    "    print(\"Class distribution after augmentation\", pd.Series(y_train_resampled).value_counts())\n",
    "\n",
    "\n",
    "    # Shuffle the resampled data while preserving the correspondence between features and labels\n",
    "    x_train_resampled, y_train_resampled = shuffle(x_train_resampled, y_train_resampled, random_state=seed)\n",
    "\n",
    "    # rename\n",
    "    X_train = x_train_resampled\n",
    "    Y_train = y_train_resampled\n",
    "\n",
    "    X_train = pd.Series(X_train.reshape(-1))\n",
    "\n",
    "else:\n",
    "    X_train = train_data[\"Text\"]\n",
    "    Y_train = train_data[\"Labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dde8ad49-e0bf-432c-b713-538b620d4d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringToList(string):\n",
    "    codeLinesList = []\n",
    "    for line in string.split():\n",
    "        codeLinesList.append(line)\n",
    "    return codeLinesList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9c833ba8-c994-4d7f-a912-e51d0149e498",
   "metadata": {},
   "outputs": [],
   "source": [
    "allTokens = []\n",
    "for seq in X_train:\n",
    "    listSeq = stringToList(seq)\n",
    "    allTokens.append(listSeq)\n",
    "\n",
    "X_train = allTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2f20c54e-ee46-447f-8bab-26eb95b3da1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.Series(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "83cd9d90-6a11-4ac6-ab79-df6bd07171c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "allTokens = []\n",
    "for seq in val_data[\"Text\"]:\n",
    "    listSeq = stringToList(seq)\n",
    "    allTokens.append(listSeq)\n",
    "\n",
    "val_data[\"Tokens\"] = allTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "763fd96f-6931-42c9-9a66-9440ced33feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "allTokens = []\n",
    "for seq in test_data[\"Text\"]:\n",
    "    listSeq = stringToList(seq)\n",
    "    allTokens.append(listSeq)\n",
    "\n",
    "test_data[\"Tokens\"] = allTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ecb6109d-59dc-4cd8-9051-8e976b399b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word embedding \n",
    "embeddings_index = {}\n",
    "f = open('w2v_embeddings.txt', encoding=\"utf-8\")\n",
    "for line in f:    \n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:])\n",
    "    embeddings_index[word] = coefs   \n",
    "f.close() \n",
    "\n",
    "dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5ba3a732-181d-4250-af68-89c5c53b464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_data = pd.concat([X_train, val_data[\"Tokens\"], test_data[\"Tokens\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e150b3b3-074e-46a2-a039-37036cf7f8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_obj = Tokenizer()   \n",
    "tokenizer_obj.fit_on_texts(concatenated_data)\n",
    "\n",
    "tokenizer_json = tokenizer_obj.to_json()\n",
    "tokenizerFile = 'w2v_tokenizer.json'\n",
    "\n",
    "with io.open(tokenizerFile, 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
    "\n",
    "with open(tokenizerFile) as f:\n",
    "    dataTokenizer = json.load(f)\n",
    "    tokenizer_obj = tokenizer_from_json(dataTokenizer)\n",
    "\n",
    "sequences = tokenizer_obj.texts_to_sequences(concatenated_data)\n",
    "word_index = tokenizer_obj.word_index\n",
    "\n",
    "lines_pad = pad_sequences(sequences, padding = 'post', maxlen = max_length)\n",
    "\n",
    "num_words = len(word_index) + 1 # +1 for the unknown-zeros\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, dim))\n",
    "for word, i in word_index.items():\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    #embedding_vector = embeddings_index.get(word)\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a9092284-20a8-4630-88e1-25c0199003ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate sequences\n",
    "max_len = 512\n",
    "lines_pad = lines_pad[:, 0:512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b6e573af-8cb0-44bc-9ef7-ffeb5293eb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = lines_pad[0:len(X_train)]\n",
    "val_x = lines_pad[len(X_train):len(X_train)+len(val_data[\"Tokens\"])]\n",
    "test_x = lines_pad[len(X_train)+len(val_data[\"Tokens\"]):len(X_train)+len(val_data[\"Tokens\"])+len(test_data[\"Tokens\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "14b899f5-6bf5-4c68-8c22-d1aa4b981ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename\n",
    "x_train = train_x\n",
    "x_val = val_x\n",
    "x_test = test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "34c4c431-6369-4ccf-b8dc-c8d89c29e8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(Y_train)\n",
    "y_val = np.array(val_data[\"Labels\"])\n",
    "y_test = np.array(test_data[\"Labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "15307714-e370-4e53-8452-b15bca190f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation functions\n",
    "def recall_metric(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = (true_positives + K.epsilon()) / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_metric(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = (true_positives + K.epsilon()) / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_metric(y_true, y_pred):\n",
    "\n",
    "    prec = precision_metric(y_true, y_pred)\n",
    "    rec = recall_metric(y_true, y_pred)\n",
    "    f1 = 2*((prec*rec)/(prec+rec+K.epsilon()))\n",
    "    return f1\n",
    "\n",
    "def f2_metric(y_true, y_pred):\n",
    "\n",
    "    prec = precision_metric(y_true, y_pred)\n",
    "    rec = recall_metric(y_true, y_pred)\n",
    "    f2 = 5*((prec*rec)/(4*prec+rec+K.epsilon()))\n",
    "    return f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "86e0b546-db21-4179-b278-87c5299efb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "patience = 10\n",
    "batch_size = 64\n",
    "lr = 0.001\n",
    "optimizer = optimizers.Adam(learning_rate=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "94719824-4409-4f20-8b20-decb1ddb25cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning Models - Classifiers\n",
    "def buildLstm(max_len, top_words, dim, seed, embedding_matrix, optimizer, n_categories):\n",
    "    model=Sequential()\n",
    "    kernel_initializer = glorot_uniform() # glorot_uniform, RandomUniform, lecun_uniform, Constant, TruncatedNormal\n",
    "    model.add(Embedding(input_dim=top_words, output_dim=dim, input_length=None, weights=[embedding_matrix], mask_zero=True, trainable=False))\n",
    "    model.add(LSTM(500, activation='tanh', dropout=0.2, return_sequences=True, stateful=False, kernel_constraint=max_norm(3), bias_constraint=max_norm(3), kernel_initializer=kernel_initializer)) # , recurrent_constraint=max_norm(3)\n",
    "    model.add(LSTM(100, activation='tanh', dropout=0.1, return_sequences=True, stateful=False, kernel_initializer=kernel_initializer))\n",
    "    model.add(LSTM(200, activation='tanh', dropout=0.1, stateful=False, kernel_initializer=kernel_initializer))\n",
    "    model.add(BatchNormalization()) # default momentum=0.99\n",
    "    #model.add(Dropout(0.2))\n",
    "    \n",
    "    #optimizer = optimizers.SGD(lr=learning_rate, decay=0.1, momentum=0.2, nesterov=True)\n",
    "    #optimizer = optimizers.RMSprop(lr=learning_rate, rho=0.9, epsilon=1e-8, decay=0.0)\n",
    "    #optimizer = optimizers.Adagrad(lr=learning_rate, epsilon=None, decay=0.004)\n",
    "    #optimizer = optimizers.Nadam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "    \n",
    "    if n_categories > 2:\n",
    "        model.add(Dense(units = n_categories, activation = 'softmax', kernel_initializer=kernel_initializer))\n",
    "        model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
    "    else:\n",
    "        model.add(Dense(units = 1, activation = 'sigmoid', kernel_initializer=kernel_initializer))\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[f1_metric])\n",
    "    return model\n",
    "\n",
    "def buildGru(max_len, top_words, dim, seed, embedding_matrix, optimizer, n_categories):\n",
    "    model=Sequential()\n",
    "    kernel_initializer = glorot_uniform() # glorot_uniform, RandomUniform, lecun_uniform, Constant, TruncatedNormal\n",
    "    model.add(Embedding(input_dim=top_words, output_dim=dim, input_length=None, weights=[embedding_matrix], mask_zero=True, trainable=False))\n",
    "    model.add(GRU(500, activation='tanh', dropout=0.2, return_sequences=True, stateful=False, kernel_constraint=max_norm(3), bias_constraint=max_norm(3), kernel_initializer=kernel_initializer)) # , recurrent_constraint=max_norm(3)\n",
    "    model.add(GRU(100, activation='tanh', dropout=0.1, return_sequences=True, stateful=False, kernel_initializer=kernel_initializer))\n",
    "    model.add(GRU(200, activation='tanh', dropout=0.1, stateful=False, kernel_initializer=kernel_initializer))\n",
    "    model.add(BatchNormalization()) # default momentum=0.99\n",
    "    #model.add(Dropout(0.2))\n",
    "    \n",
    "    #optimizer = optimizers.SGD(lr=learning_rate, decay=0.1, momentum=0.2, nesterov=True)\n",
    "    #optimizer = optimizers.RMSprop(lr=learning_rate, rho=0.9, epsilon=1e-8, decay=0.0)\n",
    "    #optimizer = optimizers.Adagrad(lr=learning_rate, epsilon=None, decay=0.004)\n",
    "    #optimizer = optimizers.Nadam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "    \n",
    "    if n_categories > 2:\n",
    "        model.add(Dense(units = n_categories, activation = 'softmax', kernel_initializer=kernel_initializer))\n",
    "        model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
    "    else:\n",
    "        model.add(Dense(units = 1, activation = 'sigmoid', kernel_initializer=kernel_initializer))\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[f1_metric]) \n",
    "    return model\n",
    "\n",
    "def buildBiLstm(max_len, top_words, dim, seed, embedding_matrix, optimizer, n_categories):\n",
    "    model=Sequential()\n",
    "    kernel_initializer = glorot_uniform() # glorot_uniform, RandomUniform, lecun_uniform, Constant, TruncatedNormal\n",
    "    model.add(Embedding(input_dim=top_words, output_dim=dim, input_length=None, weights=[embedding_matrix], mask_zero=True, trainable=False))\n",
    "    model.add(Bidirectional(LSTM(500, activation='tanh', dropout=0.2, return_sequences=True, stateful=False, kernel_constraint=max_norm(3), bias_constraint=max_norm(3), kernel_initializer=kernel_initializer))) # , recurrent_constraint=max_norm(3)\n",
    "    model.add(Bidirectional(LSTM(100, activation='tanh', dropout=0.1, return_sequences=True, stateful=False, kernel_initializer=kernel_initializer)))\n",
    "    model.add(Bidirectional(LSTM(200, activation='tanh', dropout=0.1, stateful=False, kernel_initializer=kernel_initializer)))\n",
    "    model.add(BatchNormalization()) # default momentum=0.99\n",
    "    #model.add(Dropout(0.2))\n",
    "    \n",
    "    #optimizer = optimizers.SGD(lr=learning_rate, decay=0.1, momentum=0.2, nesterov=True)\n",
    "    #optimizer = optimizers.RMSprop(lr=learning_rate, rho=0.9, epsilon=1e-8, decay=0.0)\n",
    "    #optimizer = optimizers.Adagrad(lr=learning_rate, epsilon=None, decay=0.004)\n",
    "    #optimizer = optimizers.Nadam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "    \n",
    "    if n_categories > 2:\n",
    "        model.add(Dense(units = n_categories, activation = 'softmax', kernel_initializer=kernel_initializer))\n",
    "        model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
    "    else:\n",
    "        model.add(Dense(units = 1, activation = 'sigmoid', kernel_initializer=kernel_initializer))\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[f1_metric]) \n",
    "    return model\n",
    "\n",
    "def buildBiGru(max_len, top_words, dim, seed, embedding_matrix, optimizer, n_categories):\n",
    "    model=Sequential()\n",
    "    kernel_initializer = glorot_uniform() # glorot_uniform, RandomUniform, lecun_uniform, Constant, TruncatedNormal\n",
    "    model.add(Embedding(input_dim=top_words, output_dim=dim, input_length=None, weights=[embedding_matrix], mask_zero=True, trainable=False))\n",
    "    model.add(Bidirectional(GRU(500, activation='tanh', dropout=0.2, return_sequences=True, stateful=False, kernel_constraint=max_norm(3), bias_constraint=max_norm(3), kernel_initializer=kernel_initializer))) # , recurrent_constraint=max_norm(3)\n",
    "    model.add(Bidirectional(GRU(100, activation='tanh', dropout=0.1, return_sequences=True, stateful=False, kernel_initializer=kernel_initializer)))\n",
    "    model.add(Bidirectional(GRU(200, activation='tanh', dropout=0.1, stateful=False, kernel_initializer=kernel_initializer)))\n",
    "    model.add(BatchNormalization()) # default momentum=0.99\n",
    "    #model.add(Dropout(0.2))\n",
    "    \n",
    "    #optimizer = optimizers.SGD(lr=learning_rate, decay=0.1, momentum=0.2, nesterov=True)\n",
    "    #optimizer = optimizers.RMSprop(lr=learning_rate, rho=0.9, epsilon=1e-8, decay=0.0)\n",
    "    #optimizer = optimizers.Adagrad(lr=learning_rate, epsilon=None, decay=0.004)\n",
    "    #optimizer = optimizers.Nadam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "    \n",
    "    if n_categories > 2:\n",
    "        model.add(Dense(units = n_categories, activation = 'softmax', kernel_initializer=kernel_initializer))\n",
    "        model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
    "    else:\n",
    "        model.add(Dense(units = 1, activation = 'sigmoid', kernel_initializer=kernel_initializer))\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[f1_metric])  \n",
    "    return model\n",
    "\n",
    "def buildCnn(max_len, top_words, dim, seed, embedding_matrix, optimizer, n_categories):\n",
    "    cnn_model = Sequential()\n",
    "    cnn_model.add(Embedding(top_words, dim, input_length=None, weights=[embedding_matrix], mask_zero=True, trainable=False))\n",
    "    cnn_model.add(Conv1D(filters = 128, kernel_size = 5, activation = 'relu'))\n",
    "    '''cnn_model.add(MaxPooling1D(pool_size = 5))\n",
    "    cnn_model.add(Conv1D(filters = 128, kernel_size = 5, activation = 'relu'))\n",
    "    cnn_model.add(MaxPooling1D(pool_size = 5))\n",
    "    cnn_model.add(Conv1D(filters = 128, kernel_size = 5, activation = 'relu'))'''\n",
    "    cnn_model.add(GlobalMaxPool1D())\n",
    "    #cnn_model.add(Dense(units = 128, activation = 'relu'))\n",
    "    \n",
    "    if n_categories > 2:\n",
    "        cnn_model.add(Dense(units = n_categories, activation = 'softmax'))\n",
    "        cnn_model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
    "    else:\n",
    "        cnn_model.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "        cnn_model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[f1_metric])\n",
    "    return cnn_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0b6f3635-99f4-4185-9e9b-b42b928dbf22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, None, 100)         86851700  \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, None, 128)         64128     \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Glo  (None, 128)              0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 86,915,957\n",
      "Trainable params: 64,257\n",
      "Non-trainable params: 86,851,700\n",
      "_________________________________________________________________\n",
      "model summary\\m None\n",
      "Epoch 1/100\n",
      "435/435 [==============================] - ETA: 0s - loss: 0.4751 - f1_metric: 0.3284\n",
      "Epoch 1: val_f1_metric improved from -inf to 0.26004, saving model to best_model.h5\n",
      "435/435 [==============================] - 5s 10ms/step - loss: 0.4751 - f1_metric: 0.3284 - val_loss: 0.3048 - val_f1_metric: 0.2600\n",
      "Epoch 2/100\n",
      "434/435 [============================>.] - ETA: 0s - loss: 0.4007 - f1_metric: 0.4468\n",
      "Epoch 2: val_f1_metric improved from 0.26004 to 0.27396, saving model to best_model.h5\n",
      "435/435 [==============================] - 4s 9ms/step - loss: 0.4009 - f1_metric: 0.4469 - val_loss: 0.3277 - val_f1_metric: 0.2740\n",
      "Epoch 3/100\n",
      "427/435 [============================>.] - ETA: 0s - loss: 0.3609 - f1_metric: 0.5164\n",
      "Epoch 3: val_f1_metric improved from 0.27396 to 0.28270, saving model to best_model.h5\n",
      "435/435 [==============================] - 4s 9ms/step - loss: 0.3602 - f1_metric: 0.5189 - val_loss: 0.3329 - val_f1_metric: 0.2827\n",
      "Epoch 4/100\n",
      "429/435 [============================>.] - ETA: 0s - loss: 0.3302 - f1_metric: 0.5766\n",
      "Epoch 4: val_f1_metric improved from 0.28270 to 0.29169, saving model to best_model.h5\n",
      "435/435 [==============================] - 4s 10ms/step - loss: 0.3298 - f1_metric: 0.5777 - val_loss: 0.3478 - val_f1_metric: 0.2917\n",
      "Epoch 5/100\n",
      "433/435 [============================>.] - ETA: 0s - loss: 0.3015 - f1_metric: 0.6194\n",
      "Epoch 5: val_f1_metric did not improve from 0.29169\n",
      "435/435 [==============================] - 3s 7ms/step - loss: 0.3018 - f1_metric: 0.6187 - val_loss: 0.3584 - val_f1_metric: 0.2915\n",
      "Epoch 6/100\n",
      "430/435 [============================>.] - ETA: 0s - loss: 0.2768 - f1_metric: 0.6691\n",
      "Epoch 6: val_f1_metric improved from 0.29169 to 0.29455, saving model to best_model.h5\n",
      "435/435 [==============================] - 4s 9ms/step - loss: 0.2766 - f1_metric: 0.6691 - val_loss: 0.3549 - val_f1_metric: 0.2945\n",
      "Epoch 7/100\n",
      "434/435 [============================>.] - ETA: 0s - loss: 0.2533 - f1_metric: 0.7075\n",
      "Epoch 7: val_f1_metric improved from 0.29455 to 0.30032, saving model to best_model.h5\n",
      "435/435 [==============================] - 4s 9ms/step - loss: 0.2535 - f1_metric: 0.7075 - val_loss: 0.3377 - val_f1_metric: 0.3003\n",
      "Epoch 8/100\n",
      "430/435 [============================>.] - ETA: 0s - loss: 0.2347 - f1_metric: 0.7452\n",
      "Epoch 8: val_f1_metric did not improve from 0.30032\n",
      "435/435 [==============================] - 3s 6ms/step - loss: 0.2343 - f1_metric: 0.7461 - val_loss: 0.3570 - val_f1_metric: 0.2982\n",
      "Epoch 9/100\n",
      "428/435 [============================>.] - ETA: 0s - loss: 0.2185 - f1_metric: 0.7699\n",
      "Epoch 9: val_f1_metric did not improve from 0.30032\n",
      "435/435 [==============================] - 3s 7ms/step - loss: 0.2181 - f1_metric: 0.7709 - val_loss: 0.3776 - val_f1_metric: 0.2994\n",
      "Epoch 10/100\n",
      "433/435 [============================>.] - ETA: 0s - loss: 0.2109 - f1_metric: 0.7763\n",
      "Epoch 10: val_f1_metric did not improve from 0.30032\n",
      "435/435 [==============================] - 3s 6ms/step - loss: 0.2112 - f1_metric: 0.7762 - val_loss: 0.4557 - val_f1_metric: 0.2809\n",
      "Epoch 11/100\n",
      "431/435 [============================>.] - ETA: 0s - loss: 0.2308 - f1_metric: 0.7535\n",
      "Epoch 11: val_f1_metric did not improve from 0.30032\n",
      "435/435 [==============================] - 3s 6ms/step - loss: 0.2312 - f1_metric: 0.7528 - val_loss: 0.3366 - val_f1_metric: 0.2864\n",
      "Epoch 12/100\n",
      "428/435 [============================>.] - ETA: 0s - loss: 0.2613 - f1_metric: 0.7114\n",
      "Epoch 12: val_f1_metric did not improve from 0.30032\n",
      "435/435 [==============================] - 3s 6ms/step - loss: 0.2611 - f1_metric: 0.7125 - val_loss: 0.3078 - val_f1_metric: 0.3000\n",
      "Epoch 13/100\n",
      "431/435 [============================>.] - ETA: 0s - loss: 0.2313 - f1_metric: 0.7368\n",
      "Epoch 13: val_f1_metric did not improve from 0.30032\n",
      "435/435 [==============================] - 3s 6ms/step - loss: 0.2314 - f1_metric: 0.7371 - val_loss: 0.3904 - val_f1_metric: 0.2979\n",
      "Epoch 14/100\n",
      "425/435 [============================>.] - ETA: 0s - loss: 0.2003 - f1_metric: 0.7801\n",
      "Epoch 14: val_f1_metric did not improve from 0.30032\n",
      "435/435 [==============================] - 2s 6ms/step - loss: 0.2003 - f1_metric: 0.7807 - val_loss: 0.4725 - val_f1_metric: 0.2808\n",
      "Epoch 15/100\n",
      "435/435 [==============================] - ETA: 0s - loss: 0.1690 - f1_metric: 0.8277\n",
      "Epoch 15: val_f1_metric did not improve from 0.30032\n",
      "435/435 [==============================] - 3s 7ms/step - loss: 0.1690 - f1_metric: 0.8277 - val_loss: 0.4907 - val_f1_metric: 0.2759\n",
      "Epoch 16/100\n",
      "435/435 [==============================] - ETA: 0s - loss: 0.1515 - f1_metric: 0.8561\n",
      "Epoch 16: val_f1_metric did not improve from 0.30032\n",
      "435/435 [==============================] - 3s 6ms/step - loss: 0.1515 - f1_metric: 0.8561 - val_loss: 0.4786 - val_f1_metric: 0.2794\n",
      "Epoch 17/100\n",
      "435/435 [==============================] - ETA: 0s - loss: 0.1434 - f1_metric: 0.8713\n",
      "Epoch 17: val_f1_metric did not improve from 0.30032\n",
      "435/435 [==============================] - 3s 6ms/step - loss: 0.1434 - f1_metric: 0.8713 - val_loss: 0.4964 - val_f1_metric: 0.2834\n",
      "Epoch 17: early stopping\n",
      "Training is completed after 55074\n"
     ]
    }
   ],
   "source": [
    "print(\"Training...\")\n",
    "milli_sec1 = int(round(time.time() * 1000))\n",
    "\n",
    "userModel = \"cnn\"\n",
    "\n",
    "if userModel == \"cnn\":\n",
    "    myModel = buildCnn(max_len, num_words, dim, seed, embedding_matrix, optimizer, n_categories) \n",
    "elif userModel == \"lstm\":\n",
    "    myModel = buildLstm(max_len, num_words, dim, seed, embedding_matrix, optimizer, n_categories)\n",
    "elif userModel == \"bilstm\":\n",
    "    myModel = buildBiLstm(max_len, num_words, dim, seed, embedding_matrix, optimizer, n_categories)\n",
    "elif userModel == \"gru\":\n",
    "    myModel = buildGru(max_len, num_words, dim, seed, embedding_matrix, optimizer, n_categories)\n",
    "elif userModel == \"bigru\":\n",
    "    myModel = buildBiGru(max_len, num_words, dim, seed, embedding_matrix, optimizer, n_categories)\n",
    "    \n",
    "print(\"model summary\\m\", myModel.summary())\n",
    "\n",
    "csv_logger = CSVLogger('log.csv', append=True, separator=',')\n",
    "es = EarlyStopping(monitor='val_f1_metric', mode='max', verbose=1, patience=patience)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_f1_metric', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "history = myModel.fit(x_train, y_train, validation_data=(x_val, y_val), epochs = n_epochs, batch_size = batch_size, shuffle=False, verbose=1, callbacks=[csv_logger,es,mc]) #, class_weight=class_weights\n",
    "\n",
    "milli_sec2 = int(round(time.time() * 1000))\n",
    "print(\"Training is completed after\", milli_sec2-milli_sec1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7965f1b8-a6b4-4826-b419-12377ca89ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = load_model('best_model.h5')\n",
    "myModel.load_weights(\"best_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c7bed507-9b78-4761-a365-a583ad8c1151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348/348 [==============================] - 1s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.89      0.92     10432\n",
      "           1       0.23      0.50      0.31       692\n",
      "\n",
      "    accuracy                           0.86     11124\n",
      "   macro avg       0.60      0.70      0.62     11124\n",
      "weighted avg       0.92      0.86      0.89     11124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, (myModel.predict(x_val) > 0.5).astype(\"int32\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f2d3342f-667c-4687-a694-3104519a8503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/350 [==============================] - 0s 1ms/step\n",
      "TP= 315\n",
      "TN= 9364\n",
      "FP= 1128\n",
      "FN= 387\n",
      "Accuracy:86.47%\n",
      "Precision:21.83%\n",
      "Recall:44.87%\n",
      "F1 score:29.37%\n",
      "Roc_Auc score:67.06%\n",
      "F2 score:37.05%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.89      0.93     10492\n",
      "           1       0.22      0.45      0.29       702\n",
      "\n",
      "    accuracy                           0.86     11194\n",
      "   macro avg       0.59      0.67      0.61     11194\n",
      "weighted avg       0.91      0.86      0.89     11194\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#scores = myModel.evaluate(lines_pad_x_test, Y_test, verbose=0)\n",
    "#predictions = myModel.predict_classes(X_test, verbose=0)\n",
    "predScores = myModel.predict(x_test)\n",
    "predictions = (predScores > 0.5).astype(\"int32\")\n",
    "\n",
    "accuracy=accuracy_score(y_test, predictions)\n",
    "if n_categories > 2:\n",
    "    precision=precision_score(y_test, predictions, average='macro')\n",
    "    recall=recall_score(y_test, predictions, average='macro')\n",
    "    f1=f1_score(y_test, predictions, average='macro')\n",
    "else:\n",
    "    precision=precision_score(y_test, predictions)\n",
    "    recall=recall_score(y_test, predictions)\n",
    "    f1=f1_score(y_test, predictions)\n",
    "    roc_auc=roc_auc_score(y_test, predictions)\n",
    "f2=5*precision*recall / (4*precision+recall)\n",
    "\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "#print(cm)\n",
    "sn.heatmap(cm, annot=True)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(\"TP=\",tp)\n",
    "print(\"TN=\",tn)\n",
    "print(\"FP=\",fp)\n",
    "print(\"FN=\",fn)\n",
    "\n",
    "acc = ((tp+tn)/(tp+tn+fp+fn))\n",
    "\n",
    "print(\"Accuracy:%.2f%%\"%(acc*100))\n",
    "print(\"Precision:%.2f%%\"%(precision*100))\n",
    "print(\"Recall:%.2f%%\"%(recall*100))\n",
    "print(\"F1 score:%.2f%%\"%(f1*100))\n",
    "print(\"Roc_Auc score:%.2f%%\"%(roc_auc*100))\n",
    "print(\"F2 score:%.2f%%\"%(f2*100))\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "45b39ce6-5fc3-4d2d-bea4-48dcb7be061c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the path\n",
    "path = os.path.join(root_path, 'results', userModel, method, str(seed))\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# Define the CSV file path\n",
    "csv_file_path = os.path.join(path, f\"{seed}.csv\")\n",
    "\n",
    "# Write data to CSV\n",
    "data = {\n",
    "    \"accuracy\": accuracy,\n",
    "    \"precision\": precision,\n",
    "    \"recall\": recall,\n",
    "    \"f1\": f1,\n",
    "    \"f2\": f2,\n",
    "    \"roc_auc\": roc_auc\n",
    "}\n",
    "\n",
    "# Write to CSV\n",
    "with open(csv_file_path, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=data.keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerow(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "67f2d15e-d222-4aa6-8c6d-a3c0ee36ee40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8646596390923709, 'precision': 0.2182952182952183, 'recall': 0.44871794871794873, 'f1': 0.29370629370629375, 'f2': 0.3705010585744531, 'roc_auc': 0.6706037322697636}\n"
     ]
    }
   ],
   "source": [
    "# Define a dictionary to store cumulative sum of metrics\n",
    "cumulative_metrics = defaultdict(float)\n",
    "count = 0  # Counter to keep track of number of CSV files\n",
    "\n",
    "# Iterate over all CSV files in the results folder\n",
    "results_folder = os.path.join(root_path, \"results\", userModel, method, str(seed))\n",
    "\n",
    "for root, dirs, files in os.walk(results_folder):\n",
    "    for filename in files:\n",
    "        if filename.endswith(\".csv\") and filename != \"avg.csv\":\n",
    "            csv_file_path = os.path.join(root, filename)\n",
    "\n",
    "            with open(csv_file_path, \"r\", newline=\"\") as csvfile:\n",
    "                reader = csv.DictReader(csvfile)\n",
    "\n",
    "                for row in reader:\n",
    "                    for metric, value in row.items():\n",
    "                        cumulative_metrics[metric] += float(value)\n",
    "            count += 1\n",
    "        \n",
    "# Compute average values\n",
    "average_metrics = {metric: total / count for metric, total in cumulative_metrics.items()}\n",
    "\n",
    "# Print average values \n",
    "print(average_metrics)\n",
    "\n",
    "# Define the path for the average CSV file\n",
    "avg_csv_file_path = os.path.join(root_path, \"results\", userModel, method, \"avg.csv\")\n",
    "\n",
    "# Write average metrics to CSV\n",
    "with open(avg_csv_file_path, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=average_metrics.keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerow(average_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0062447d-6990-451b-9119-126220a8e585",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
