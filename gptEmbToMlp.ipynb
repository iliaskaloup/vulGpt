{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7847600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Friday Febr 22 12:16:19 2023\n",
    "\n",
    "@author: iliaskaloup\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow\n",
    "\n",
    "import os, json, glob, time, sys, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import math\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Masking\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.layers import Bidirectional, BatchNormalization\n",
    "from tensorflow.keras.initializers import glorot_uniform, RandomUniform, lecun_uniform, Constant\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "\n",
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "#from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, \\\n",
    "roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "import random\n",
    "\n",
    "\n",
    "# define seeder\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tensorflow.random.set_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bd9067a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_metric(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = (true_positives + K.epsilon()) / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_metric(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = (true_positives + K.epsilon()) / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_metric(y_true, y_pred):\n",
    "\n",
    "    prec = precision_metric(y_true, y_pred)\n",
    "    rec = recall_metric(y_true, y_pred)\n",
    "    f1 = 2*((prec*rec)/(prec+rec+K.epsilon()))\n",
    "    return f1\n",
    "\n",
    "def f2_metric(y_true, y_pred):\n",
    "\n",
    "    prec = precision_metric(y_true, y_pred)\n",
    "    rec = recall_metric(y_true, y_pred)\n",
    "    f2 = 5*((prec*rec)/(4*prec+rec+K.epsilon()))\n",
    "    return f2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61e3e0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildMLP():\n",
    "    learning_rate = 0.001\n",
    "    nIn = 768\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=nIn)) # hidden\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(Dropout(0.15))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid')) # Output\n",
    "    #model.add(Dropout(0.15))\n",
    "    sgd = optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=[f2_metric])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10214299",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildLstm():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, input_dim=1, input_length=768, stateful=False))\n",
    "    model.add(Activation('relu')) #dropout=0.2, recurrent_dropout=0.2, kernel_constraint=max_norm(3), bias_constraint=max_norm(3)\n",
    "    #model.add(BatchNormalization(momentum=0.0))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    #model.compile(loss=f2_loss, optimizer='adam', metrics=[f2_metric])\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=[f2_metric])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a24d5a49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.7029777e-03,  4.0793600e-01, -2.5765502e-01, ...,\n",
       "        -3.2464272e-01, -4.6883753e-01,  6.6156566e-01],\n",
       "       [-5.4515070e-01,  3.5717532e-01, -3.4790817e-01, ...,\n",
       "         4.3896335e-01, -2.9661950e-01, -3.9072060e-01],\n",
       "       [-4.5674380e-01,  5.6255543e-01, -1.0349541e+00, ...,\n",
       "        -1.0931984e-01, -8.8110840e-03, -6.4844990e-01],\n",
       "       ...,\n",
       "       [-1.1811732e+00,  1.1784925e+00, -2.5114307e+00, ...,\n",
       "         2.3985744e+00,  2.2563240e-01, -3.8030213e-01],\n",
       "       [ 4.5845336e-01,  3.5698715e-01,  3.5300934e-01, ...,\n",
       "         3.6557147e-01, -1.3089369e-01,  1.9057815e-01],\n",
       "       [ 9.6241530e-02,  3.5551400e-01,  4.9386650e-04, ...,\n",
       "        -9.7265035e-02,  7.0922000e-02, -3.7174487e-01]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val = pd.read_csv('train_val_embeddings.csv', sep =',')\n",
    "train_val = train_val.sample(random_state=seed, frac=1).reset_index(drop=True)\n",
    "train_val_y = train_val['Label'].values.tolist()\n",
    "train_val_y = pd.DataFrame(train_val_y)\n",
    "train_val_y = np.array(train_val_y)\n",
    "train_val_X = train_val.drop('Label', axis=1)\n",
    "train_val_X = np.array(train_val_X)\n",
    "\n",
    "test = pd.read_csv('test_embeddings.csv', sep =',')\n",
    "test = test.sample(random_state=seed, frac=1).reset_index(drop=True)\n",
    "test_y = test['Label'].values.tolist()\n",
    "test_y = pd.DataFrame(test_y)\n",
    "test_y = np.array(test_y)\n",
    "test_X = test.drop('Label', axis=1)\n",
    "test_X = np.array(test_X)\n",
    "\n",
    "train = pd.read_csv('train_embeddings.csv', sep =',')\n",
    "train = train.sample(random_state=seed, frac=1).reset_index(drop=True)\n",
    "train_y = train['Label'].values.tolist()\n",
    "train_y = pd.DataFrame(train_y)\n",
    "train_y = np.array(train_y)\n",
    "train_X = train.drop('Label', axis=1)\n",
    "train_X = np.array(train_X)\n",
    "\n",
    "val = pd.read_csv('val_embeddings.csv', sep =',')\n",
    "val = val.sample(random_state=seed, frac=1).reset_index(drop=True)\n",
    "val_y = val['Label'].values.tolist()\n",
    "val_y = pd.DataFrame(val_y)\n",
    "val_y = np.array(val_y)\n",
    "val_X = val.drop('Label', axis=1)\n",
    "val_X = np.array(val_X)\n",
    "\n",
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1af7ff89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train_val_X = train_val_X.reshape((train_val_X.shape[0], 768, 1))\\n\\ntrain_X = train_X.reshape((train_X.shape[0], 768, 1))\\n\\nval_X = val_X.reshape((val_X.shape[0], 768, 1))\\n\\ntest_X = test_X.reshape((test_X.shape[0], 768, 1))'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''train_val_X = train_val_X.reshape((train_val_X.shape[0], 768, 1))\n",
    "\n",
    "train_X = train_X.reshape((train_X.shape[0], 768, 1))\n",
    "\n",
    "val_X = val_X.reshape((val_X.shape[0], 768, 1))\n",
    "\n",
    "test_X = test_X.reshape((test_X.shape[0], 768, 1))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "566e59ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 100)               76900     \n",
      "                                                                 \n",
      " activation (Activation)     (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 77,001\n",
      "Trainable params: 77,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "model summary\\m None\n",
      "Epoch 1/100\n",
      "59/61 [============================>.] - ETA: 0s - loss: 1.0631 - f2_metric: 0.3112\n",
      "Epoch 1: val_f2_metric improved from -inf to 0.42611, saving model to best_mlp.h5\n",
      "61/61 [==============================] - 2s 7ms/step - loss: 1.0522 - f2_metric: 0.3118 - val_loss: 0.4165 - val_f2_metric: 0.4261\n",
      "Epoch 2/100\n",
      "48/61 [======================>.......] - ETA: 0s - loss: 0.4159 - f2_metric: 0.4931\n",
      "Epoch 2: val_f2_metric improved from 0.42611 to 0.67145, saving model to best_mlp.h5\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 0.4192 - f2_metric: 0.4899 - val_loss: 0.3718 - val_f2_metric: 0.6714\n",
      "Epoch 3/100\n",
      "59/61 [============================>.] - ETA: 0s - loss: 0.3577 - f2_metric: 0.5702\n",
      "Epoch 3: val_f2_metric did not improve from 0.67145\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 0.3587 - f2_metric: 0.5709 - val_loss: 0.3291 - val_f2_metric: 0.6610\n",
      "Epoch 4/100\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.3222 - f2_metric: 0.6191\n",
      "Epoch 4: val_f2_metric did not improve from 0.67145\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 0.3222 - f2_metric: 0.6191 - val_loss: 0.3102 - val_f2_metric: 0.5728\n",
      "Epoch 5/100\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.3003 - f2_metric: 0.6543\n",
      "Epoch 5: val_f2_metric improved from 0.67145 to 0.75728, saving model to best_mlp.h5\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 0.3003 - f2_metric: 0.6543 - val_loss: 0.2628 - val_f2_metric: 0.7573\n",
      "Epoch 6/100\n",
      "46/61 [=====================>........] - ETA: 0s - loss: 0.2646 - f2_metric: 0.7341\n",
      "Epoch 6: val_f2_metric did not improve from 0.75728\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 0.2663 - f2_metric: 0.7139 - val_loss: 0.2482 - val_f2_metric: 0.7447\n",
      "Epoch 7/100\n",
      "48/61 [======================>.......] - ETA: 0s - loss: 0.2528 - f2_metric: 0.7277\n",
      "Epoch 7: val_f2_metric improved from 0.75728 to 0.79335, saving model to best_mlp.h5\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 0.2607 - f2_metric: 0.7301 - val_loss: 0.4690 - val_f2_metric: 0.7933\n",
      "Epoch 8/100\n",
      "60/61 [============================>.] - ETA: 0s - loss: 0.2714 - f2_metric: 0.7116\n",
      "Epoch 8: val_f2_metric did not improve from 0.79335\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 0.2709 - f2_metric: 0.7090 - val_loss: 0.2512 - val_f2_metric: 0.7725\n",
      "Epoch 9/100\n",
      "48/61 [======================>.......] - ETA: 0s - loss: 0.2217 - f2_metric: 0.7927\n",
      "Epoch 9: val_f2_metric did not improve from 0.79335\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 0.2178 - f2_metric: 0.7757 - val_loss: 0.2373 - val_f2_metric: 0.7338\n",
      "Epoch 10/100\n",
      "47/61 [======================>.......] - ETA: 0s - loss: 0.2008 - f2_metric: 0.8195\n",
      "Epoch 10: val_f2_metric improved from 0.79335 to 0.86072, saving model to best_mlp.h5\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 0.2051 - f2_metric: 0.8049 - val_loss: 0.2467 - val_f2_metric: 0.8607\n",
      "Epoch 11/100\n",
      "47/61 [======================>.......] - ETA: 0s - loss: 0.2125 - f2_metric: 0.7987\n",
      "Epoch 11: val_f2_metric did not improve from 0.86072\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 0.2163 - f2_metric: 0.8100 - val_loss: 0.2411 - val_f2_metric: 0.7022\n",
      "Epoch 12/100\n",
      "48/61 [======================>.......] - ETA: 0s - loss: 0.1866 - f2_metric: 0.8163\n",
      "Epoch 12: val_f2_metric did not improve from 0.86072\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 0.1943 - f2_metric: 0.8093 - val_loss: 0.2047 - val_f2_metric: 0.8180\n",
      "Epoch 13/100\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.1628 - f2_metric: 0.8490\n",
      "Epoch 13: val_f2_metric improved from 0.86072 to 0.86712, saving model to best_mlp.h5\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 0.1628 - f2_metric: 0.8490 - val_loss: 0.1941 - val_f2_metric: 0.8671\n",
      "Epoch 14/100\n",
      "47/61 [======================>.......] - ETA: 0s - loss: 0.1421 - f2_metric: 0.8804\n",
      "Epoch 14: val_f2_metric improved from 0.86712 to 0.87931, saving model to best_mlp.h5\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 0.1513 - f2_metric: 0.8699 - val_loss: 0.2236 - val_f2_metric: 0.8793\n",
      "Epoch 15/100\n",
      "48/61 [======================>.......] - ETA: 0s - loss: 0.1667 - f2_metric: 0.8595\n",
      "Epoch 15: val_f2_metric did not improve from 0.87931\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 0.1744 - f2_metric: 0.8356 - val_loss: 0.2055 - val_f2_metric: 0.8474\n",
      "Epoch 16/100\n",
      "48/61 [======================>.......] - ETA: 0s - loss: 0.1422 - f2_metric: 0.8782\n",
      "Epoch 16: val_f2_metric did not improve from 0.87931\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 0.1402 - f2_metric: 0.8854 - val_loss: 0.2254 - val_f2_metric: 0.7770\n",
      "Epoch 17/100\n",
      "57/61 [===========================>..] - ETA: 0s - loss: 0.1471 - f2_metric: 0.8644\n",
      "Epoch 17: val_f2_metric improved from 0.87931 to 0.88159, saving model to best_mlp.h5\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 0.1486 - f2_metric: 0.8691 - val_loss: 0.1812 - val_f2_metric: 0.8816\n",
      "Epoch 18/100\n",
      "57/61 [===========================>..] - ETA: 0s - loss: 0.1420 - f2_metric: 0.8727\n",
      "Epoch 18: val_f2_metric improved from 0.88159 to 0.88566, saving model to best_mlp.h5\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 0.1424 - f2_metric: 0.8674 - val_loss: 0.1852 - val_f2_metric: 0.8857\n",
      "Epoch 19/100\n",
      "57/61 [===========================>..] - ETA: 0s - loss: 0.1117 - f2_metric: 0.9106\n",
      "Epoch 19: val_f2_metric improved from 0.88566 to 0.88674, saving model to best_mlp.h5\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 0.1135 - f2_metric: 0.9112 - val_loss: 0.1774 - val_f2_metric: 0.8867\n",
      "Epoch 20/100\n",
      "57/61 [===========================>..] - ETA: 0s - loss: 0.1077 - f2_metric: 0.9148\n",
      "Epoch 20: val_f2_metric did not improve from 0.88674\n",
      "61/61 [==============================] - 0s 4ms/step - loss: 0.1097 - f2_metric: 0.9121 - val_loss: 0.1665 - val_f2_metric: 0.8671\n",
      "Epoch 21/100\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.1022 - f2_metric: 0.9242\n",
      "Epoch 21: val_f2_metric did not improve from 0.88674\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 0.1043 - f2_metric: 0.9217 - val_loss: 0.2186 - val_f2_metric: 0.7537\n",
      "Epoch 22/100\n",
      "50/61 [=======================>......] - ETA: 0s - loss: 0.1141 - f2_metric: 0.8948\n",
      "Epoch 22: val_f2_metric improved from 0.88674 to 0.88721, saving model to best_mlp.h5\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 0.1135 - f2_metric: 0.8925 - val_loss: 0.1679 - val_f2_metric: 0.8872\n",
      "Epoch 23/100\n",
      "56/61 [==========================>...] - ETA: 0s - loss: 0.0805 - f2_metric: 0.9479\n",
      "Epoch 23: val_f2_metric improved from 0.88721 to 0.90561, saving model to best_mlp.h5\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 0.0836 - f2_metric: 0.9428 - val_loss: 0.1708 - val_f2_metric: 0.9056\n",
      "Epoch 24/100\n",
      "57/61 [===========================>..] - ETA: 0s - loss: 0.0769 - f2_metric: 0.9530\n",
      "Epoch 24: val_f2_metric improved from 0.90561 to 0.91526, saving model to best_mlp.h5\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 0.0769 - f2_metric: 0.9521 - val_loss: 0.1940 - val_f2_metric: 0.9153\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58/61 [===========================>..] - ETA: 0s - loss: 0.0737 - f2_metric: 0.9503\n",
      "Epoch 25: val_f2_metric improved from 0.91526 to 0.92962, saving model to best_mlp.h5\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 0.0742 - f2_metric: 0.9509 - val_loss: 0.1583 - val_f2_metric: 0.9296\n",
      "Epoch 26/100\n",
      "54/61 [=========================>....] - ETA: 0s - loss: 0.0679 - f2_metric: 0.9579\n",
      "Epoch 26: val_f2_metric did not improve from 0.92962\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 0.0710 - f2_metric: 0.9526 - val_loss: 0.1594 - val_f2_metric: 0.9205\n",
      "Epoch 27/100\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.0720 - f2_metric: 0.9541\n",
      "Epoch 27: val_f2_metric did not improve from 0.92962\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 0.0720 - f2_metric: 0.9541 - val_loss: 0.1667 - val_f2_metric: 0.9257\n",
      "Epoch 28/100\n",
      "56/61 [==========================>...] - ETA: 0s - loss: 0.0596 - f2_metric: 0.9646\n",
      "Epoch 28: val_f2_metric did not improve from 0.92962\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 0.0586 - f2_metric: 0.9617 - val_loss: 0.1623 - val_f2_metric: 0.9216\n",
      "Epoch 29/100\n",
      "54/61 [=========================>....] - ETA: 0s - loss: 0.0665 - f2_metric: 0.9627\n",
      "Epoch 29: val_f2_metric did not improve from 0.92962\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 0.0649 - f2_metric: 0.9655 - val_loss: 0.1637 - val_f2_metric: 0.8983\n",
      "Epoch 30/100\n",
      "56/61 [==========================>...] - ETA: 0s - loss: 0.0533 - f2_metric: 0.9698\n",
      "Epoch 30: val_f2_metric did not improve from 0.92962\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 0.0532 - f2_metric: 0.9689 - val_loss: 0.1731 - val_f2_metric: 0.9234\n",
      "Epoch 31/100\n",
      "52/61 [========================>.....] - ETA: 0s - loss: 0.0543 - f2_metric: 0.9759\n",
      "Epoch 31: val_f2_metric did not improve from 0.92962\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 0.0541 - f2_metric: 0.9731 - val_loss: 0.1571 - val_f2_metric: 0.9212\n",
      "Epoch 32/100\n",
      "52/61 [========================>.....] - ETA: 0s - loss: 0.0476 - f2_metric: 0.9759\n",
      "Epoch 32: val_f2_metric did not improve from 0.92962\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 0.0480 - f2_metric: 0.9764 - val_loss: 0.1624 - val_f2_metric: 0.9197\n",
      "Epoch 33/100\n",
      "53/61 [=========================>....] - ETA: 0s - loss: 0.0518 - f2_metric: 0.9675\n",
      "Epoch 33: val_f2_metric did not improve from 0.92962\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 0.0496 - f2_metric: 0.9709 - val_loss: 0.1640 - val_f2_metric: 0.9241\n",
      "Epoch 34/100\n",
      "54/61 [=========================>....] - ETA: 0s - loss: 0.0460 - f2_metric: 0.9817\n",
      "Epoch 34: val_f2_metric improved from 0.92962 to 0.93006, saving model to best_mlp.h5\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 0.0450 - f2_metric: 0.9805 - val_loss: 0.1583 - val_f2_metric: 0.9301\n",
      "Epoch 35/100\n",
      "53/61 [=========================>....] - ETA: 0s - loss: 0.0397 - f2_metric: 0.9893\n",
      "Epoch 35: val_f2_metric did not improve from 0.93006\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 0.0410 - f2_metric: 0.9846 - val_loss: 0.1631 - val_f2_metric: 0.9240\n",
      "Epoch 36/100\n",
      "54/61 [=========================>....] - ETA: 0s - loss: 0.0359 - f2_metric: 0.9833\n",
      "Epoch 36: val_f2_metric did not improve from 0.93006\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 0.0370 - f2_metric: 0.9828 - val_loss: 0.1648 - val_f2_metric: 0.9170\n",
      "Epoch 37/100\n",
      "53/61 [=========================>....] - ETA: 0s - loss: 0.0350 - f2_metric: 0.9871\n",
      "Epoch 37: val_f2_metric did not improve from 0.93006\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 0.0338 - f2_metric: 0.9879 - val_loss: 0.1677 - val_f2_metric: 0.9104\n",
      "Epoch 38/100\n",
      "52/61 [========================>.....] - ETA: 0s - loss: 0.0449 - f2_metric: 0.9795\n",
      "Epoch 38: val_f2_metric did not improve from 0.93006\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 0.0428 - f2_metric: 0.9821 - val_loss: 0.1626 - val_f2_metric: 0.9143\n",
      "Epoch 39/100\n",
      "51/61 [========================>.....] - ETA: 0s - loss: 0.0282 - f2_metric: 0.9929\n",
      "Epoch 39: val_f2_metric did not improve from 0.93006\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 0.0280 - f2_metric: 0.9934 - val_loss: 0.1484 - val_f2_metric: 0.9257\n",
      "Epoch 40/100\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.0286 - f2_metric: 0.9904\n",
      "Epoch 40: val_f2_metric improved from 0.93006 to 0.93196, saving model to best_mlp.h5\n",
      "61/61 [==============================] - 0s 7ms/step - loss: 0.0277 - f2_metric: 0.9914 - val_loss: 0.1499 - val_f2_metric: 0.9320\n",
      "Epoch 41/100\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.0234 - f2_metric: 0.9938\n",
      "Epoch 41: val_f2_metric did not improve from 0.93196\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 0.0234 - f2_metric: 0.9938 - val_loss: 0.1695 - val_f2_metric: 0.9313\n",
      "Epoch 42/100\n",
      "52/61 [========================>.....] - ETA: 0s - loss: 0.0268 - f2_metric: 0.9939\n",
      "Epoch 42: val_f2_metric improved from 0.93196 to 0.93245, saving model to best_mlp.h5\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 0.0306 - f2_metric: 0.9925 - val_loss: 0.1594 - val_f2_metric: 0.9324\n",
      "Epoch 43/100\n",
      "50/61 [=======================>......] - ETA: 0s - loss: 0.0285 - f2_metric: 0.9902\n",
      "Epoch 43: val_f2_metric did not improve from 0.93245\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 0.0302 - f2_metric: 0.9905 - val_loss: 0.1536 - val_f2_metric: 0.9244\n",
      "Epoch 44/100\n",
      "53/61 [=========================>....] - ETA: 0s - loss: 0.0374 - f2_metric: 0.9825\n",
      "Epoch 44: val_f2_metric did not improve from 0.93245\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 0.0352 - f2_metric: 0.9848 - val_loss: 0.1615 - val_f2_metric: 0.9219\n",
      "Epoch 45/100\n",
      "51/61 [========================>.....] - ETA: 0s - loss: 0.0255 - f2_metric: 0.9900\n",
      "Epoch 45: val_f2_metric did not improve from 0.93245\n",
      "61/61 [==============================] - 0s 5ms/step - loss: 0.0270 - f2_metric: 0.9897 - val_loss: 0.1757 - val_f2_metric: 0.9202\n",
      "Epoch 46/100\n",
      "51/61 [========================>.....] - ETA: 0s - loss: 0.0335 - f2_metric: 0.9906\n",
      "Epoch 46: val_f2_metric did not improve from 0.93245\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 0.0312 - f2_metric: 0.9913 - val_loss: 0.1662 - val_f2_metric: 0.9207\n",
      "Epoch 47/100\n",
      "52/61 [========================>.....] - ETA: 0s - loss: 0.0209 - f2_metric: 0.9955\n",
      "Epoch 47: val_f2_metric did not improve from 0.93245\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 0.0197 - f2_metric: 0.9961 - val_loss: 0.1750 - val_f2_metric: 0.9237\n",
      "Epoch 48/100\n",
      "52/61 [========================>.....] - ETA: 0s - loss: 0.0189 - f2_metric: 0.9933\n",
      "Epoch 48: val_f2_metric did not improve from 0.93245\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 0.0178 - f2_metric: 0.9941 - val_loss: 0.1604 - val_f2_metric: 0.9305\n",
      "Epoch 49/100\n",
      "56/61 [==========================>...] - ETA: 0s - loss: 0.0247 - f2_metric: 0.9920\n",
      "Epoch 49: val_f2_metric did not improve from 0.93245\n",
      "61/61 [==============================] - 0s 7ms/step - loss: 0.0252 - f2_metric: 0.9913 - val_loss: 0.1628 - val_f2_metric: 0.9297\n",
      "Epoch 50/100\n",
      "54/61 [=========================>....] - ETA: 0s - loss: 0.0237 - f2_metric: 0.9917\n",
      "Epoch 50: val_f2_metric did not improve from 0.93245\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 0.0238 - f2_metric: 0.9919 - val_loss: 0.1609 - val_f2_metric: 0.9314\n",
      "Epoch 51/100\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.0257 - f2_metric: 0.9880\n",
      "Epoch 51: val_f2_metric did not improve from 0.93245\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 0.0257 - f2_metric: 0.9880 - val_loss: 0.1775 - val_f2_metric: 0.9290\n",
      "Epoch 52/100\n",
      "55/61 [==========================>...] - ETA: 0s - loss: 0.0235 - f2_metric: 0.9946\n",
      "Epoch 52: val_f2_metric did not improve from 0.93245\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 0.0222 - f2_metric: 0.9952 - val_loss: 0.1653 - val_f2_metric: 0.9287\n",
      "Epoch 52: early stopping\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "32/32 [==============================] - 0s 1ms/step\n",
      "[[747  20]\n",
      " [ 16 222]]\n",
      "Accuracy:96.42%\n",
      "Precision:91.74%\n",
      "Recall:93.28%\n",
      "F1 score:92.50%\n",
      "Roc_Auc score:95.33%\n",
      "F2 score:92.96%\n",
      "FPR score:2.61%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.98       767\n",
      "           1       0.92      0.93      0.93       238\n",
      "\n",
      "    accuracy                           0.96      1005\n",
      "   macro avg       0.95      0.95      0.95      1005\n",
      "weighted avg       0.96      0.96      0.96      1005\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train-val\n",
    "nb_epoch = 100\n",
    "BS = 64\n",
    "\n",
    "myModel = buildMLP() \n",
    "print(\"model summary\\m\",myModel.summary())\n",
    "\n",
    "csv_logger = CSVLogger('log.csv', append=True, separator=',')\n",
    "es = EarlyStopping(monitor='val_f2_metric', mode='max', verbose=1, patience=10)\n",
    "mc = ModelCheckpoint('best_mlp.h5', monitor='val_f2_metric', mode='max', verbose=1, save_best_only=True)\n",
    "history = myModel.fit(train_X, train_y, validation_data=(val_X, val_y), epochs = nb_epoch, batch_size = BS, shuffle=True, verbose=1, callbacks=[csv_logger,es,mc])\n",
    "\n",
    "#load best model\n",
    "#myModel = load_model('best_model.h5')\n",
    "myModel.load_weights(\"best_mlp.h5\")\n",
    "\n",
    "scores = myModel.evaluate(val_X, val_y, verbose=0)\n",
    "#predictions = myModel.predict_classes(X_test, verbose=0)\n",
    "predictions = (myModel.predict(val_X) > 0.5).astype(\"int32\")\n",
    "predScores = myModel.predict(val_X)\n",
    "\n",
    "accuracy=accuracy_score(val_y, predictions)\n",
    "precision=precision_score(val_y, predictions)\n",
    "recall=recall_score(val_y, predictions)\n",
    "f1=f1_score(val_y, predictions)\n",
    "roc_auc=roc_auc_score(val_y, predictions)\n",
    "f2 = 5*precision*recall / (4*precision+recall)\n",
    "#f2=fbeta_score(Y_test, predictions, beta=0.5)\n",
    "print(confusion_matrix(val_y, predictions, labels=[0, 1]))\n",
    "tn, fp, fn, tp = confusion_matrix(val_y, predictions).ravel()\n",
    "fpr = fp / (fp+tn)\n",
    "acc = ((tp+tn)/(tp+tn+fp+fn))\n",
    "print(\"Accuracy:%.2f%%\"%(acc*100))\n",
    "print(\"Precision:%.2f%%\"%(precision*100))\n",
    "print(\"Recall:%.2f%%\"%(recall*100))\n",
    "print(\"F1 score:%.2f%%\"%(f1*100))\n",
    "print(\"Roc_Auc score:%.2f%%\"%(roc_auc*100))\n",
    "print(\"F2 score:%.2f%%\"%(f2*100))\n",
    "print(\"FPR score:%.2f%%\"%(fpr*100))\n",
    "print(classification_report(val_y, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54976df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 100)               76900     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 77,001\n",
      "Trainable params: 77,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "model summary\\m None\n",
      "128/128 [==============================] - 1s 5ms/step - loss: 0.6687 - f2_metric: 0.3411\n",
      "37/37 [==============================] - 0s 1ms/step\n",
      "37/37 [==============================] - 0s 1ms/step\n",
      "[[857  33]\n",
      " [153 122]]\n",
      "Accuracy:84.03%\n",
      "Precision:78.71%\n",
      "Recall:44.36%\n",
      "F1 score:56.74%\n",
      "Roc_Auc score:70.33%\n",
      "F2 score:48.61%\n",
      "FPR score:3.71%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.96      0.90       890\n",
      "           1       0.79      0.44      0.57       275\n",
      "\n",
      "    accuracy                           0.84      1165\n",
      "   macro avg       0.82      0.70      0.73      1165\n",
      "weighted avg       0.83      0.84      0.82      1165\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train-test\n",
    "del myModel\n",
    "\n",
    "nb_epoch = 16\n",
    "BS = 64\n",
    "\n",
    "myModel = buildMLP() \n",
    "print(\"model summary\\m\",myModel.summary())\n",
    "\n",
    "myModel.fit(train_val_X, train_val_y)\n",
    "\n",
    "scores = myModel.evaluate(test_X, test_y, verbose=0)\n",
    "#predictions = myModel.predict_classes(X_test, verbose=0)\n",
    "predictions = (myModel.predict(test_X) > 0.5).astype(\"int32\")\n",
    "predScores = myModel.predict(test_X)\n",
    "\n",
    "accuracy=accuracy_score(test_y, predictions)\n",
    "precision=precision_score(test_y, predictions)\n",
    "recall=recall_score(test_y, predictions)\n",
    "f1=f1_score(test_y, predictions)\n",
    "roc_auc=roc_auc_score(test_y, predictions)\n",
    "f2 = 5*precision*recall / (4*precision+recall)\n",
    "#f2=fbeta_score(Y_test, predictions, beta=0.5)\n",
    "print(confusion_matrix(test_y, predictions, labels=[0, 1]))\n",
    "tn, fp, fn, tp = confusion_matrix(test_y, predictions).ravel()\n",
    "fpr = fp / (fp+tn)\n",
    "acc = ((tp+tn)/(tp+tn+fp+fn))\n",
    "print(\"Accuracy:%.2f%%\"%(acc*100))\n",
    "print(\"Precision:%.2f%%\"%(precision*100))\n",
    "print(\"Recall:%.2f%%\"%(recall*100))\n",
    "print(\"F1 score:%.2f%%\"%(f1*100))\n",
    "print(\"Roc_Auc score:%.2f%%\"%(roc_auc*100))\n",
    "print(\"F2 score:%.2f%%\"%(f2*100))\n",
    "print(\"FPR score:%.2f%%\"%(fpr*100))\n",
    "print(classification_report(test_y, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d09a7ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
