{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3354fc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import json, os\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "# from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "# from torch.optim import Adam\n",
    "# from transformers import get_linear_schedule_with_warmup\n",
    "# from torch.nn.utils import clip_grad_norm_\n",
    "# from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModel, TFGPT2LMHeadModel, RobertaTokenizer\n",
    "from transformers import set_seed\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import LSTM, SimpleRNN\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import Masking\n",
    "from tensorflow.keras.layers import Embedding, MaxPool1D\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.layers import Bidirectional, BatchNormalization\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.initializers import glorot_uniform, RandomUniform, lecun_uniform, Constant, TruncatedNormal\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, GlobalMaxPool1D, Flatten\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalMaxPool1D\n",
    "from keras_preprocessing.text import tokenizer_from_json\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, \\\n",
    "roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from collections import defaultdict\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbfd651",
   "metadata": {},
   "source": [
    "Define method name and root path of the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74683969",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"embeddingsExtraction\"\n",
    "\n",
    "root_path = os.path.join('..', '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "021a956c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_algorithm = \"bert\" # \"bert\" # \"gpt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48806676",
   "metadata": {},
   "source": [
    "Define specific seeder for all experiments and processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e3172d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "#torch.manual_seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b0f01a",
   "metadata": {},
   "source": [
    "Read data and shuffle them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29764f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                func  vul\n",
      "0  static int ipip_rcv(struct sk_buff *skb)\\n{\\n\\...    0\n",
      "1  bool LayerTreeHostImpl::IsUIResourceOpaque(UIR...    0\n",
      "2  error::Error GLES2DecoderPassthroughImpl::DoGe...    0\n",
      "3  void DocumentLoader::NotifyFinished(Resource* ...    0\n",
      "4  void conn_free(conn *c) {\\n    if (c) {\\n     ...    0\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(os.path.join(root_path, 'data', 'dataset.csv'))\n",
    "data = data.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f822c9e0-5ec0-4398-b140-7465e35af954",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset=[\"func\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbbb0cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of words: 15441\n"
     ]
    }
   ],
   "source": [
    "word_counts = data[\"func\"].apply(lambda x: len(x.split()))\n",
    "max_length = word_counts.max()\n",
    "print(\"Maximum number of words:\", max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63804820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    188636\n",
      "1     10900\n",
      "Name: vul, dtype: int64\n",
      "Percentage:  5.778324391950635 %\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "vc = data[\"vul\"].value_counts()\n",
    "\n",
    "print(vc)\n",
    "\n",
    "print(\"Percentage: \", (vc[1] / vc[0])*100, '%')\n",
    "\n",
    "n_categories = len(vc)\n",
    "print(n_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2405b1c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>static int ipip_rcv(struct sk_buff *skb)\\n{\\n\\...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bool LayerTreeHostImpl::IsUIResourceOpaque(UIR...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>error::Error GLES2DecoderPassthroughImpl::DoGe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>void DocumentLoader::NotifyFinished(Resource* ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>void conn_free(conn *c) {\\n    if (c) {\\n     ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  static int ipip_rcv(struct sk_buff *skb)\\n{\\n\\...      0\n",
       "1  bool LayerTreeHostImpl::IsUIResourceOpaque(UIR...      0\n",
       "2  error::Error GLES2DecoderPassthroughImpl::DoGe...      0\n",
       "3  void DocumentLoader::NotifyFinished(Resource* ...      0\n",
       "4  void conn_free(conn *c) {\\n    if (c) {\\n     ...      0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(({'text': data['func'], 'label': data['vul']}))\n",
    "#data = data[0:100]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bbec25",
   "metadata": {},
   "source": [
    "Train test split with seeder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "928f2aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ratio = 0.10\n",
    "\n",
    "#split to train-val-test\n",
    "# split dataset to train-test sets\n",
    "### split data into train and test (90% train, 10% test)\n",
    "shuffle_seeders = [seed, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "shuffle_seeder = shuffle_seeders[0]\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=val_ratio, random_state=shuffle_seeder, stratify=data['label'])\n",
    "# print(len(data))\n",
    "# print(len(train_data))\n",
    "# print(len(test_data))\n",
    "# print(len(test_data)+len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d36c93f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = train_test_split(train_data, test_size=val_ratio, random_state=shuffle_seeder, stratify=train_data['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37254222",
   "metadata": {},
   "source": [
    "Pre-processing step: Under-sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "782fb4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution  0    152794\n",
      "1      8829\n",
      "Name: label, dtype: int64\n",
      "Majority class  0\n",
      "Minority class  1\n",
      "Targeted number of majority class 17658\n",
      "Class distribution after augmentation 0    17658\n",
      "1     8829\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "sampling = True\n",
    "if n_categories == 2 and sampling == True:\n",
    "    # Apply under-sampling with the specified strategy\n",
    "    class_counts = pd.Series(train_data[\"label\"]).value_counts()\n",
    "    print(\"Class distribution \", class_counts)\n",
    "\n",
    "    majority_class = class_counts.idxmax()\n",
    "    print(\"Majority class \", majority_class)\n",
    "\n",
    "    minority_class = class_counts.idxmin()\n",
    "    print(\"Minority class \", minority_class)\n",
    "\n",
    "    target_count = 2 * class_counts[class_counts.idxmin()] # class_counts[class_counts.idxmin()] # int(class_counts.iloc[0] / 2) \n",
    "    print(\"Targeted number of majority class\", target_count)\n",
    "\n",
    "    # under\n",
    "    sampling_strategy = {majority_class: target_count}        \n",
    "    rus = RandomUnderSampler(random_state=seed, sampling_strategy=sampling_strategy)\n",
    "\n",
    "    x_train_resampled, y_train_resampled = rus.fit_resample(np.array(train_data[\"text\"]).reshape(-1, 1), train_data[\"label\"]) \n",
    "    print(\"Class distribution after augmentation\", pd.Series(y_train_resampled).value_counts())\n",
    "\n",
    "\n",
    "    # Shuffle the resampled data while preserving the correspondence between features and labels\n",
    "    x_train_resampled, y_train_resampled = shuffle(x_train_resampled, y_train_resampled, random_state=seed)\n",
    "\n",
    "    # rename\n",
    "    X_train = x_train_resampled\n",
    "    Y_train = y_train_resampled\n",
    "\n",
    "    X_train = pd.Series(X_train.reshape(-1))\n",
    "\n",
    "else:\n",
    "    X_train = train_data[\"text\"]\n",
    "    Y_train = train_data[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86325363",
   "metadata": {},
   "source": [
    "Choose transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cb1bc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# microsoft/codebert-base-mlm # microsoft/codebert-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "772f881d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PYTORCH\n",
    "# if embedding_algorithm == \"bert\":\n",
    "#     model_variation = \"microsoft/codebert-base-mlm\"\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_variation, do_lower_case=True) #Tokenizer\n",
    "#     #bert-base-uncased #bert-base #albert-base-v2 # roberta-base # distilbert-base-uncased #distilbert-base \n",
    "#     # Define New tokens for string and numerical i.e., strId$ and numId$\n",
    "#     new_tokens = [\"strId$\", \"numId$\"]\n",
    "#     for new_token in new_tokens:\n",
    "#         if new_token not in tokenizer.get_vocab().keys():\n",
    "#             tokenizer.add_tokens(new_token)\n",
    "            \n",
    "#     bert = AutoModel.from_pretrained(model_variation, num_labels=n_categories)\n",
    "\n",
    "#     bert.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "#     embedding_matrix = bert.embeddings.word_embeddings.weight.detach().cpu().numpy()\n",
    "    \n",
    "#     num_words = len(embedding_matrix)\n",
    "#     print(num_words)\n",
    "#     dim = len(embedding_matrix[0])\n",
    "#     print(dim)\n",
    "    \n",
    "#     sentences = X_train.tolist()\n",
    "#     sequences = [tokenizer.encode(sente, truncation=True, add_special_tokens=False, return_tensors=\"pt\").numpy() for sente in sentences] # Tokenize the complete sentences\n",
    "\n",
    "#     lines_pad_x_train = []\n",
    "#     for seq in sequences:\n",
    "#         lines_pad_x_train.append(torch.tensor(seq[0]))\n",
    "    \n",
    "#     lines_pad_x_train = pad_sequence(lines_pad_x_train, batch_first=True, padding_value=0)\n",
    "#     max_len = lines_pad_x_train.size()[1]\n",
    "    \n",
    "    \n",
    "#     sentences = val_data[\"Input\"]\n",
    "#     sequences = [tokenizer.encode(sente, truncation=True, add_special_tokens=False, return_tensors=\"pt\").numpy() for sente in sentences]\n",
    "#     lines_pad_x_val = []\n",
    "#     for seq in sequences:\n",
    "#         lines_pad_x_val.append(torch.tensor(seq[0]))\n",
    "#     lines_pad_x_val = pad_sequence(lines_pad_x_val, batch_first=True, padding_value=0)\n",
    "    \n",
    "#     sentences = test_data[\"Input\"]\n",
    "#     sequences = [tokenizer.encode(sente, truncation=True, add_special_tokens=False, return_tensors=\"pt\").numpy() for sente in sentences]\n",
    "#     lines_pad_x_test = []\n",
    "#     for seq in sequences:\n",
    "#         lines_pad_x_test.append(torch.tensor(seq[0]))\n",
    "#     lines_pad_x_test = pad_sequence(lines_pad_x_test, batch_first=True, padding_value=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3eac4a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMaxLen(X):\n",
    "\n",
    "    # Code for identifying max length of the data samples after tokenization using transformer tokenizer\n",
    "    \n",
    "    max_length = 0\n",
    "    # Iterate over each sample in your dataset\n",
    "    for i, input_ids in enumerate(X['input_ids']):\n",
    "        # Calculate the length of the tokenized sequence for the current sample\n",
    "        length = tf.math.reduce_sum(tf.cast(input_ids != 1, tf.int32)).numpy()\n",
    "        # Update max_length and max_row if the current length is greater\n",
    "        if length > max_length:\n",
    "            max_length = length\n",
    "            max_row = i\n",
    "\n",
    "    print(\"Max length of tokenized data:\", max_length)\n",
    "    print(\"Row with max length:\", max_row)\n",
    "\n",
    "    #X['input_ids'] = np.delete(X['input_ids'], max_row, axis=0)\n",
    "    \n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a38906f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50265\n",
      "768\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "# TENSORFLOW\n",
    "if embedding_algorithm == \"bert\":\n",
    "    model_variation = \"microsoft/codebert-base\"\n",
    "#     model_variation = \"microsoft/codebert-base\"\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_variation, do_lower_case=True) #Tokenizer\n",
    "    tokenizer = RobertaTokenizer(vocab_file=\"../tokenizer_training/cpp_tokenizer/cpp_tokenizer-vocab.json\",\n",
    "                             merges_file=\"../tokenizer_training/cpp_tokenizer/cpp_tokenizer-merges.txt\")\n",
    "    #bert-base-uncased #bert-base #albert-base-v2 # roberta-base # distilbert-base-uncased #distilbert-base \n",
    "    # Define New tokens for string and numerical i.e., strId$ and numId$\n",
    "#     new_tokens = [\"strId$\", \"numId$\"]\n",
    "#     for new_token in new_tokens:\n",
    "#         if new_token not in tokenizer.get_vocab().keys():\n",
    "#             tokenizer.add_tokens(new_token)\n",
    "            \n",
    "    bert = TFAutoModel.from_pretrained(model_variation)\n",
    "\n",
    "    #bert.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    bert_embeddings = bert.get_input_embeddings()\n",
    "    embedding_matrix = bert_embeddings.weights[0].numpy()\n",
    "    \n",
    "    num_words = embedding_matrix.shape[0]\n",
    "    print(num_words)\n",
    "    dim = embedding_matrix.shape[1]\n",
    "    print(dim)\n",
    "    \n",
    "    sentences = X_train.tolist()\n",
    "    sequences = [tokenizer(sente, truncation=True, max_length=510, add_special_tokens=False, return_tensors=\"tf\") for sente in sentences] # Tokenize the complete sentences\n",
    "\n",
    "    def padSequences(sequences, max_len):\n",
    "        lines_pad = []\n",
    "        for sequence in sequences:\n",
    "            seq = sequence['input_ids'].numpy()[0]\n",
    "            if len(seq) < max_len:\n",
    "                for i in range(len(seq), max_len):\n",
    "                    seq = np.append(seq, 0)\n",
    "            lines_pad.append(seq)\n",
    "        return lines_pad\n",
    "    \n",
    "    def get_max_len(sequences):\n",
    "        max_len = 0\n",
    "\n",
    "        for seq in sequences:\n",
    "            if len(seq['input_ids'].numpy()[0]) > max_len:\n",
    "                max_len = len(seq['input_ids'].numpy()[0])\n",
    "\n",
    "        return max_len\n",
    "    \n",
    "    max_len = get_max_len(sequences)\n",
    "    print(max_len)\n",
    "    \n",
    "    lines_pad_x_train = padSequences(sequences, max_len)\n",
    "    lines_pad_x_train = [arr.tolist() for arr in lines_pad_x_train]\n",
    "    lines_pad_x_train = np.array(lines_pad_x_train)\n",
    "        \n",
    "    val_sentences = val_data[\"text\"].tolist()\n",
    "    val_sequences = [tokenizer(sente, truncation=True, max_length=510, add_special_tokens=False, return_tensors=\"tf\") for sente in val_sentences]\n",
    "    \n",
    "    lines_pad_x_val = padSequences(val_sequences, max_len)\n",
    "    lines_pad_x_val = [arr.tolist() for arr in lines_pad_x_val]\n",
    "    lines_pad_x_val = np.array(lines_pad_x_val)\n",
    "    \n",
    "    test_sentences = test_data[\"text\"].tolist()\n",
    "    test_sequences = [tokenizer(sente, truncation=True, max_length=510, add_special_tokens=False, return_tensors=\"tf\") for sente in test_sentences]\n",
    "    \n",
    "    lines_pad_x_test = padSequences(test_sequences, max_len)\n",
    "    lines_pad_x_test = [arr.tolist() for arr in lines_pad_x_test]\n",
    "    lines_pad_x_test = np.array(lines_pad_x_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9cb2e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # PYTORCH\n",
    "# if embedding_algorithm == \"gpt\":\n",
    "#     model_variation = \"gpt2\" # \"microsoft/CodeGPT-small-py-adaptedGPT2\" # \"gpt2\" # \"microsoft/CodeGPT-small-py\" \n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_variation, do_lower_case=True) #Tokenizer\n",
    "#     # Define New tokens for string and numerical i.e., strId$ and numId$\n",
    "#     new_tokens = [\"strId$\", \"numId$\"]\n",
    "#     for new_token in new_tokens:\n",
    "#         if new_token not in tokenizer.get_vocab().keys():\n",
    "#             tokenizer.add_tokens(new_token)\n",
    "            \n",
    "#     gpt = AutoModel.from_pretrained(model_variation, num_labels=n_categories)\n",
    "\n",
    "#     gpt.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "#     embedding_matrix = gpt.wte.weight.detach().cpu().numpy()\n",
    "    \n",
    "#     num_words = len(embedding_matrix)\n",
    "#     print(num_words)\n",
    "#     dim = len(embedding_matrix[0])\n",
    "#     print(dim)\n",
    "    \n",
    "#     sentences = X_train.tolist()\n",
    "#     sequences = [tokenizer.encode(sente, truncation=True, add_special_tokens=False, return_tensors=\"pt\").numpy() for sente in sentences] # Tokenize the complete sentences\n",
    "\n",
    "#     lines_pad_x_train = []\n",
    "#     for seq in sequences:\n",
    "#         lines_pad_x_train.append(torch.tensor(seq[0]))\n",
    "    \n",
    "#     lines_pad_x_train = pad_sequence(lines_pad_x_train, batch_first=True, padding_value=0)\n",
    "#     max_len = lines_pad_x_train.size()[1]\n",
    "    \n",
    "    \n",
    "#     sentences = val_data[\"Input\"]\n",
    "#     sequences = [tokenizer.encode(sente, truncation=True, add_special_tokens=False, return_tensors=\"pt\").numpy() for sente in sentences]\n",
    "#     lines_pad_x_val = []\n",
    "#     for seq in sequences:\n",
    "#         lines_pad_x_val.append(torch.tensor(seq[0]))\n",
    "#     lines_pad_x_val = pad_sequence(lines_pad_x_val, batch_first=True, padding_value=0)\n",
    "    \n",
    "#     sentences = test_data[\"Input\"]\n",
    "#     sequences = [tokenizer.encode(sente, truncation=True, add_special_tokens=False, return_tensors=\"pt\").numpy() for sente in sentences]\n",
    "#     lines_pad_x_test = []\n",
    "#     for seq in sequences:\n",
    "#         lines_pad_x_test.append(torch.tensor(seq[0]))\n",
    "#     lines_pad_x_test = pad_sequence(lines_pad_x_test, batch_first=True, padding_value=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e03248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TENSORFLOW\n",
    "if embedding_algorithm == \"gpt\":\n",
    "    model_variation = \"gpt2\" # \"microsoft/CodeGPT-small-py-adaptedGPT2\" # \"gpt2\" # \"microsoft/CodeGPT-small-py\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_variation, do_lower_case=True) #Tokenizer\n",
    "    #bert-base-uncased #bert-base #albert-base-v2 # roberta-base # distilbert-base-uncased #distilbert-base \n",
    "    # Define New tokens for string and numerical i.e., strId$ and numId$\n",
    "#     new_tokens = [\"strId$\", \"numId$\"]\n",
    "#     for new_token in new_tokens:\n",
    "#         if new_token not in tokenizer.get_vocab().keys():\n",
    "#             tokenizer.add_tokens(new_token)\n",
    "            \n",
    "    gpt = TFGPT2LMHeadModel.from_pretrained(model_variation, num_labels=n_categories)\n",
    "\n",
    "    #gpt.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    embedding_matrix = gpt.transformer.wte.weight\n",
    "    \n",
    "    num_words = embedding_matrix.shape[0]\n",
    "    print(num_words)\n",
    "    dim = embedding_matrix.shape[1]\n",
    "    print(dim)\n",
    "    \n",
    "#     X = tokenizer(\n",
    "#         text=X_train.tolist(),\n",
    "#         add_special_tokens=False,\n",
    "#         max_length=512,\n",
    "#         truncation=True,\n",
    "#         padding=True,\n",
    "#         return_tensors='tf',\n",
    "#         return_token_type_ids=False,\n",
    "#         return_attention_mask=True,\n",
    "#         verbose=True\n",
    "#     )\n",
    "\n",
    "#     max_len = getMaxLen(X)\n",
    "    max_len = 512\n",
    "    \n",
    "    sentences = X_train.tolist()\n",
    "    sequences = [tokenizer.encode(sente, truncation=True, add_special_tokens=False, return_tensors=\"tf\").numpy() for sente in sentences] # Tokenize the complete sentences\n",
    "\n",
    "    lines_pad_x_train = []\n",
    "    for seq in sequences:\n",
    "        lines_pad_x_train.append(seq[0])\n",
    "    \n",
    "    lines_pad_x_train = pad_sequences(lines_pad_x_train, padding = 'post', maxlen = max_len)    \n",
    "    \n",
    "    sentences = val_data[\"text\"].tolist()\n",
    "    sequences = [tokenizer.encode(sente, truncation=True, add_special_tokens=False, return_tensors=\"tf\").numpy() for sente in sentences]\n",
    "    lines_pad_x_val = []\n",
    "    for seq in sequences:\n",
    "        lines_pad_x_val.append(seq[0])\n",
    "    lines_pad_x_val = pad_sequences(lines_pad_x_val, padding = 'post', maxlen = max_len)\n",
    "    \n",
    "    sentences = test_data[\"text\"].tolist()\n",
    "    sequences = [tokenizer.encode(sente, truncation=True, add_special_tokens=False, return_tensors=\"tf\").numpy() for sente in sentences]\n",
    "    lines_pad_x_test = []\n",
    "    for seq in sequences:\n",
    "        lines_pad_x_test.append(seq[0])\n",
    "    lines_pad_x_test = pad_sequences(lines_pad_x_test, padding = 'post', maxlen = max_len)\n",
    "\n",
    "    embedding_matrix = embedding_matrix.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "48e3f62c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26487, 17959, 19954)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train = np.array(Y_train)\n",
    "Y_val = np.array(val_data[\"label\"])\n",
    "Y_test = np.array(test_data[\"label\"])\n",
    "len(Y_train), len(Y_val), len(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ac833046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation functions\n",
    "def recall_metric(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = (true_positives + K.epsilon()) / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_metric(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = (true_positives + K.epsilon()) / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_metric(y_true, y_pred):\n",
    "\n",
    "    prec = precision_metric(y_true, y_pred)\n",
    "    rec = recall_metric(y_true, y_pred)\n",
    "    f1 = 2*((prec*rec)/(prec+rec+K.epsilon()))\n",
    "    return f1\n",
    "\n",
    "def f2_metric(y_true, y_pred):\n",
    "\n",
    "    prec = precision_metric(y_true, y_pred)\n",
    "    rec = recall_metric(y_true, y_pred)\n",
    "    f2 = 5*((prec*rec)/(4*prec+rec+K.epsilon()))\n",
    "    return f2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386ae338",
   "metadata": {},
   "source": [
    "Select Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cb7c6c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "patience = 10\n",
    "batch_size = 64\n",
    "lr = 0.001\n",
    "optimizer = optimizers.Adam(learning_rate=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4d61734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning Models - Classifiers\n",
    "def buildLstm(max_len, top_words, dim, seed, embedding_matrix, optimizer, n_categories):\n",
    "    model=Sequential()\n",
    "    kernel_initializer = glorot_uniform() # glorot_uniform, RandomUniform, lecun_uniform, Constant, TruncatedNormal\n",
    "    model.add(Embedding(input_dim=top_words, output_dim=dim, input_length=None, weights=[embedding_matrix], mask_zero=True, trainable=False))\n",
    "    model.add(LSTM(500, activation='tanh', dropout=0.2, return_sequences=True, stateful=False, kernel_constraint=max_norm(3), bias_constraint=max_norm(3), kernel_initializer=kernel_initializer)) # , recurrent_constraint=max_norm(3)\n",
    "    model.add(LSTM(100, activation='tanh', dropout=0.1, return_sequences=True, stateful=False, kernel_initializer=kernel_initializer))\n",
    "    model.add(LSTM(200, activation='tanh', dropout=0.1, stateful=False, kernel_initializer=kernel_initializer))\n",
    "    model.add(BatchNormalization()) # default momentum=0.99\n",
    "    #model.add(Dropout(0.2))\n",
    "    \n",
    "    #optimizer = optimizers.SGD(lr=learning_rate, decay=0.1, momentum=0.2, nesterov=True)\n",
    "    #optimizer = optimizers.RMSprop(lr=learning_rate, rho=0.9, epsilon=1e-8, decay=0.0)\n",
    "    #optimizer = optimizers.Adagrad(lr=learning_rate, epsilon=None, decay=0.004)\n",
    "    #optimizer = optimizers.Nadam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "    \n",
    "    if n_categories > 2:\n",
    "        model.add(Dense(units = n_categories, activation = 'softmax', kernel_initializer=kernel_initializer))\n",
    "        model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
    "    else:\n",
    "        model.add(Dense(units = 1, activation = 'sigmoid', kernel_initializer=kernel_initializer))\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[f1_metric])\n",
    "    return model\n",
    "\n",
    "def buildGru(max_len, top_words, dim, seed, embedding_matrix, optimizer, n_categories):\n",
    "    model=Sequential()\n",
    "    kernel_initializer = glorot_uniform() # glorot_uniform, RandomUniform, lecun_uniform, Constant, TruncatedNormal\n",
    "    model.add(Embedding(input_dim=top_words, output_dim=dim, input_length=None, weights=[embedding_matrix], mask_zero=True, trainable=False))\n",
    "    model.add(GRU(500, activation='tanh', dropout=0.2, return_sequences=True, stateful=False, kernel_constraint=max_norm(3), bias_constraint=max_norm(3), kernel_initializer=kernel_initializer)) # , recurrent_constraint=max_norm(3)\n",
    "    model.add(GRU(100, activation='tanh', dropout=0.1, return_sequences=True, stateful=False, kernel_initializer=kernel_initializer))\n",
    "    model.add(GRU(200, activation='tanh', dropout=0.1, stateful=False, kernel_initializer=kernel_initializer))\n",
    "    model.add(BatchNormalization()) # default momentum=0.99\n",
    "    #model.add(Dropout(0.2))\n",
    "    \n",
    "    #optimizer = optimizers.SGD(lr=learning_rate, decay=0.1, momentum=0.2, nesterov=True)\n",
    "    #optimizer = optimizers.RMSprop(lr=learning_rate, rho=0.9, epsilon=1e-8, decay=0.0)\n",
    "    #optimizer = optimizers.Adagrad(lr=learning_rate, epsilon=None, decay=0.004)\n",
    "    #optimizer = optimizers.Nadam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "    \n",
    "    if n_categories > 2:\n",
    "        model.add(Dense(units = n_categories, activation = 'softmax', kernel_initializer=kernel_initializer))\n",
    "        model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
    "    else:\n",
    "        model.add(Dense(units = 1, activation = 'sigmoid', kernel_initializer=kernel_initializer))\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[f1_metric]) \n",
    "    return model\n",
    "\n",
    "def buildBiLstm(max_len, top_words, dim, seed, embedding_matrix, optimizer, n_categories):\n",
    "    model=Sequential()\n",
    "    kernel_initializer = glorot_uniform() # glorot_uniform, RandomUniform, lecun_uniform, Constant, TruncatedNormal\n",
    "    model.add(Embedding(input_dim=top_words, output_dim=dim, input_length=None, weights=[embedding_matrix], mask_zero=True, trainable=False))\n",
    "    model.add(Bidirectional(LSTM(500, activation='tanh', dropout=0.2, return_sequences=True, stateful=False, kernel_constraint=max_norm(3), bias_constraint=max_norm(3), kernel_initializer=kernel_initializer))) # , recurrent_constraint=max_norm(3)\n",
    "    model.add(Bidirectional(LSTM(100, activation='tanh', dropout=0.1, return_sequences=True, stateful=False, kernel_initializer=kernel_initializer)))\n",
    "    model.add(Bidirectional(LSTM(200, activation='tanh', dropout=0.1, stateful=False, kernel_initializer=kernel_initializer)))\n",
    "    model.add(BatchNormalization()) # default momentum=0.99\n",
    "    #model.add(Dropout(0.2))\n",
    "    \n",
    "    #optimizer = optimizers.SGD(lr=learning_rate, decay=0.1, momentum=0.2, nesterov=True)\n",
    "    #optimizer = optimizers.RMSprop(lr=learning_rate, rho=0.9, epsilon=1e-8, decay=0.0)\n",
    "    #optimizer = optimizers.Adagrad(lr=learning_rate, epsilon=None, decay=0.004)\n",
    "    #optimizer = optimizers.Nadam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "    \n",
    "    if n_categories > 2:\n",
    "        model.add(Dense(units = n_categories, activation = 'softmax', kernel_initializer=kernel_initializer))\n",
    "        model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
    "    else:\n",
    "        model.add(Dense(units = 1, activation = 'sigmoid', kernel_initializer=kernel_initializer))\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[f1_metric]) \n",
    "    return model\n",
    "\n",
    "def buildBiGru(max_len, top_words, dim, seed, embedding_matrix, optimizer, n_categories):\n",
    "    model=Sequential()\n",
    "    kernel_initializer = glorot_uniform() # glorot_uniform, RandomUniform, lecun_uniform, Constant, TruncatedNormal\n",
    "    model.add(Embedding(input_dim=top_words, output_dim=dim, input_length=None, weights=[embedding_matrix], mask_zero=True, trainable=False))\n",
    "    model.add(Bidirectional(GRU(500, activation='tanh', dropout=0.2, return_sequences=True, stateful=False, kernel_constraint=max_norm(3), bias_constraint=max_norm(3), kernel_initializer=kernel_initializer))) # , recurrent_constraint=max_norm(3)\n",
    "    model.add(Bidirectional(GRU(100, activation='tanh', dropout=0.1, return_sequences=True, stateful=False, kernel_initializer=kernel_initializer)))\n",
    "    model.add(Bidirectional(GRU(200, activation='tanh', dropout=0.1, stateful=False, kernel_initializer=kernel_initializer)))\n",
    "    model.add(BatchNormalization()) # default momentum=0.99\n",
    "    #model.add(Dropout(0.2))\n",
    "    \n",
    "    #optimizer = optimizers.SGD(lr=learning_rate, decay=0.1, momentum=0.2, nesterov=True)\n",
    "    #optimizer = optimizers.RMSprop(lr=learning_rate, rho=0.9, epsilon=1e-8, decay=0.0)\n",
    "    #optimizer = optimizers.Adagrad(lr=learning_rate, epsilon=None, decay=0.004)\n",
    "    #optimizer = optimizers.Nadam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "    \n",
    "    if n_categories > 2:\n",
    "        model.add(Dense(units = n_categories, activation = 'softmax', kernel_initializer=kernel_initializer))\n",
    "        model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
    "    else:\n",
    "        model.add(Dense(units = 1, activation = 'sigmoid', kernel_initializer=kernel_initializer))\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[f1_metric])  \n",
    "    return model\n",
    "\n",
    "def buildCnn(max_len, top_words, dim, seed, embedding_matrix, optimizer, n_categories):\n",
    "    cnn_model = Sequential()\n",
    "    cnn_model.add(Embedding(top_words, dim, input_length=None, weights=[embedding_matrix], mask_zero=True, trainable=False))\n",
    "    cnn_model.add(Conv1D(filters = 128, kernel_size = 5, activation = 'relu'))\n",
    "    '''cnn_model.add(MaxPooling1D(pool_size = 5))\n",
    "    cnn_model.add(Conv1D(filters = 128, kernel_size = 5, activation = 'relu'))\n",
    "    cnn_model.add(MaxPooling1D(pool_size = 5))\n",
    "    cnn_model.add(Conv1D(filters = 128, kernel_size = 5, activation = 'relu'))'''\n",
    "    cnn_model.add(GlobalMaxPool1D())\n",
    "    #cnn_model.add(Dense(units = 128, activation = 'relu'))\n",
    "    \n",
    "    if n_categories > 2:\n",
    "        cnn_model.add(Dense(units = n_categories, activation = 'softmax'))\n",
    "        cnn_model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
    "    else:\n",
    "        cnn_model.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "        cnn_model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[f1_metric])\n",
    "    return cnn_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3d7f1620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 768)         38603520  \n",
      "                                                                 \n",
      " gru (GRU)                   (None, None, 500)         1905000   \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, None, 100)         180600    \n",
      "                                                                 \n",
      " gru_2 (GRU)                 (None, 200)               181200    \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 200)              800       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 201       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,871,321\n",
      "Trainable params: 2,267,401\n",
      "Non-trainable params: 38,603,920\n",
      "_________________________________________________________________\n",
      "model summary\\m None\n",
      "Epoch 1/100\n",
      "414/414 [==============================] - ETA: 0s - loss: 0.5143 - f1_metric: 0.6011\n",
      "Epoch 1: val_f1_metric improved from -inf to 0.44608, saving model to best_model.h5\n",
      "414/414 [==============================] - 74s 153ms/step - loss: 0.5143 - f1_metric: 0.6011 - val_loss: 0.2133 - val_f1_metric: 0.4461\n",
      "Epoch 2/100\n",
      "414/414 [==============================] - ETA: 0s - loss: 0.3293 - f1_metric: 0.7983\n",
      "Epoch 2: val_f1_metric improved from 0.44608 to 0.49974, saving model to best_model.h5\n",
      "414/414 [==============================] - 59s 142ms/step - loss: 0.3293 - f1_metric: 0.7983 - val_loss: 0.2176 - val_f1_metric: 0.4997\n",
      "Epoch 3/100\n",
      "414/414 [==============================] - ETA: 0s - loss: 0.2803 - f1_metric: 0.8308\n",
      "Epoch 3: val_f1_metric improved from 0.49974 to 0.54385, saving model to best_model.h5\n",
      "414/414 [==============================] - 59s 143ms/step - loss: 0.2803 - f1_metric: 0.8308 - val_loss: 0.1551 - val_f1_metric: 0.5439\n",
      "Epoch 4/100\n",
      "414/414 [==============================] - ETA: 0s - loss: 0.2600 - f1_metric: 0.8450\n",
      "Epoch 4: val_f1_metric did not improve from 0.54385\n",
      "414/414 [==============================] - 59s 142ms/step - loss: 0.2600 - f1_metric: 0.8450 - val_loss: 0.1716 - val_f1_metric: 0.5286\n",
      "Epoch 5/100\n",
      "414/414 [==============================] - ETA: 0s - loss: 0.2383 - f1_metric: 0.8615\n",
      "Epoch 5: val_f1_metric improved from 0.54385 to 0.54611, saving model to best_model.h5\n",
      "414/414 [==============================] - 59s 143ms/step - loss: 0.2383 - f1_metric: 0.8615 - val_loss: 0.1695 - val_f1_metric: 0.5461\n",
      "Epoch 6/100\n",
      "414/414 [==============================] - ETA: 0s - loss: 0.2197 - f1_metric: 0.8743\n",
      "Epoch 6: val_f1_metric did not improve from 0.54611\n",
      "414/414 [==============================] - 59s 142ms/step - loss: 0.2197 - f1_metric: 0.8743 - val_loss: 0.2269 - val_f1_metric: 0.5073\n",
      "Epoch 7/100\n",
      "414/414 [==============================] - ETA: 0s - loss: 0.1907 - f1_metric: 0.8960\n",
      "Epoch 7: val_f1_metric did not improve from 0.54611\n",
      "414/414 [==============================] - 59s 143ms/step - loss: 0.1907 - f1_metric: 0.8960 - val_loss: 0.2491 - val_f1_metric: 0.4847\n",
      "Epoch 8/100\n",
      "414/414 [==============================] - ETA: 0s - loss: 0.1639 - f1_metric: 0.9101\n",
      "Epoch 8: val_f1_metric did not improve from 0.54611\n",
      "414/414 [==============================] - 59s 143ms/step - loss: 0.1639 - f1_metric: 0.9101 - val_loss: 0.2740 - val_f1_metric: 0.4748\n",
      "Epoch 9/100\n",
      "414/414 [==============================] - ETA: 0s - loss: 0.1874 - f1_metric: 0.8756\n",
      "Epoch 9: val_f1_metric did not improve from 0.54611\n",
      "414/414 [==============================] - 59s 143ms/step - loss: 0.1874 - f1_metric: 0.8756 - val_loss: 2.2925 - val_f1_metric: 0.1063\n",
      "Epoch 10/100\n",
      "414/414 [==============================] - ETA: 0s - loss: 0.3654 - f1_metric: 0.7489\n",
      "Epoch 10: val_f1_metric did not improve from 0.54611\n",
      "414/414 [==============================] - 59s 143ms/step - loss: 0.3654 - f1_metric: 0.7489 - val_loss: 0.1822 - val_f1_metric: 0.5179\n",
      "Epoch 11/100\n",
      "414/414 [==============================] - ETA: 0s - loss: 0.2700 - f1_metric: 0.8379\n",
      "Epoch 11: val_f1_metric did not improve from 0.54611\n",
      "414/414 [==============================] - 59s 143ms/step - loss: 0.2700 - f1_metric: 0.8379 - val_loss: 0.2436 - val_f1_metric: 0.4989\n",
      "Epoch 12/100\n",
      "414/414 [==============================] - ETA: 0s - loss: 0.2487 - f1_metric: 0.8558\n",
      "Epoch 12: val_f1_metric did not improve from 0.54611\n",
      "414/414 [==============================] - 59s 143ms/step - loss: 0.2487 - f1_metric: 0.8558 - val_loss: 0.2267 - val_f1_metric: 0.5185\n",
      "Epoch 13/100\n",
      "414/414 [==============================] - ETA: 0s - loss: 0.2277 - f1_metric: 0.8702\n",
      "Epoch 13: val_f1_metric did not improve from 0.54611\n",
      "414/414 [==============================] - 59s 143ms/step - loss: 0.2277 - f1_metric: 0.8702 - val_loss: 0.2010 - val_f1_metric: 0.5313\n",
      "Epoch 14/100\n",
      "414/414 [==============================] - ETA: 0s - loss: 0.2151 - f1_metric: 0.8768\n",
      "Epoch 14: val_f1_metric did not improve from 0.54611\n",
      "414/414 [==============================] - 59s 143ms/step - loss: 0.2151 - f1_metric: 0.8768 - val_loss: 0.2219 - val_f1_metric: 0.5238\n",
      "Epoch 15/100\n",
      "414/414 [==============================] - ETA: 0s - loss: 0.2001 - f1_metric: 0.8871\n",
      "Epoch 15: val_f1_metric did not improve from 0.54611\n",
      "414/414 [==============================] - 59s 143ms/step - loss: 0.2001 - f1_metric: 0.8871 - val_loss: 0.2490 - val_f1_metric: 0.4971\n",
      "Epoch 15: early stopping\n",
      "Training is completed after 902771\n"
     ]
    }
   ],
   "source": [
    "print(\"Training...\")\n",
    "milli_sec1 = int(round(time.time() * 1000))\n",
    "\n",
    "userModel = \"gru\"\n",
    "\n",
    "if userModel == \"cnn\":\n",
    "    myModel = buildCnn(max_len, num_words, dim, seed, embedding_matrix, optimizer, n_categories) \n",
    "elif userModel == \"lstm\":\n",
    "    myModel = buildLstm(max_len, num_words, dim, seed, embedding_matrix, optimizer, n_categories)\n",
    "elif userModel == \"bilstm\":\n",
    "    myModel = buildBiLstm(max_len, num_words, dim, seed, embedding_matrix, optimizer, n_categories)\n",
    "elif userModel == \"gru\":\n",
    "    myModel = buildGru(max_len, num_words, dim, seed, embedding_matrix, optimizer, n_categories)\n",
    "elif userModel == \"bigru\":\n",
    "    myModel = buildBiGru(max_len, num_words, dim, seed, embedding_matrix, optimizer, n_categories)\n",
    "    \n",
    "print(\"model summary\\m\", myModel.summary())\n",
    "\n",
    "csv_logger = CSVLogger('log.csv', append=True, separator=',')\n",
    "es = EarlyStopping(monitor='val_f1_metric', mode='max', verbose=1, patience=patience)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_f1_metric', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "history = myModel.fit(lines_pad_x_train, Y_train, validation_data=(lines_pad_x_val, Y_val), epochs = n_epochs, batch_size = batch_size, shuffle=False, verbose=1, callbacks=[csv_logger,es,mc]) #, class_weight=class_weights\n",
    "\n",
    "milli_sec2 = int(round(time.time() * 1000))\n",
    "print(\"Training is completed after\", milli_sec2-milli_sec1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd68d04-85ff-4b71-9f71-0e97d866cb34",
   "metadata": {},
   "source": [
    "Load best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8d60b11d-068e-4695-9fca-16f0d33e321a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = load_model('best_model.h5')\n",
    "myModel.load_weights(\"best_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e2ac4f-5f62-413a-b54f-97c0b2312941",
   "metadata": {},
   "source": [
    "Classification report on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cdd0205c-3010-4c74-93cb-7bfcf31153e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "562/562 [==============================] - 20s 31ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.95      0.97     16978\n",
      "           1       0.46      0.75      0.57       981\n",
      "\n",
      "    accuracy                           0.94     17959\n",
      "   macro avg       0.72      0.85      0.77     17959\n",
      "weighted avg       0.96      0.94      0.94     17959\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_val, (myModel.predict(lines_pad_x_val) > 0.5).astype(\"int32\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631cd13d-ce0a-4135-91d1-333a82099219",
   "metadata": {},
   "source": [
    "Prediction and Evaluation on testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ebc1635a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624/624 [==============================] - 19s 31ms/step\n",
      "TP= 820\n",
      "TN= 17888\n",
      "FP= 976\n",
      "FN= 270\n",
      "Accuracy:93.76%\n",
      "Precision:45.66%\n",
      "Recall:75.23%\n",
      "F1 score:56.83%\n",
      "Roc_Auc score:85.03%\n",
      "F2 score:66.60%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.95      0.97     18864\n",
      "           1       0.46      0.75      0.57      1090\n",
      "\n",
      "    accuracy                           0.94     19954\n",
      "   macro avg       0.72      0.85      0.77     19954\n",
      "weighted avg       0.96      0.94      0.94     19954\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#scores = myModel.evaluate(lines_pad_x_test, Y_test, verbose=0)\n",
    "#predictions = myModel.predict_classes(X_test, verbose=0)\n",
    "predScores = myModel.predict(lines_pad_x_test)\n",
    "predictions = (predScores > 0.5).astype(\"int32\")\n",
    "\n",
    "accuracy=accuracy_score(Y_test, predictions)\n",
    "if n_categories > 2:\n",
    "    precision=precision_score(Y_test, predictions, average='macro')\n",
    "    recall=recall_score(Y_test, predictions, average='macro')\n",
    "    f1=f1_score(Y_test, predictions, average='macro')\n",
    "else:\n",
    "    precision=precision_score(Y_test, predictions)\n",
    "    recall=recall_score(Y_test, predictions)\n",
    "    f1=f1_score(Y_test, predictions)\n",
    "    roc_auc=roc_auc_score(Y_test, predictions)\n",
    "f2=5*precision*recall / (4*precision+recall)\n",
    "\n",
    "cm = confusion_matrix(Y_test, predictions)\n",
    "#print(cm)\n",
    "sn.heatmap(cm, annot=True)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(\"TP=\",tp)\n",
    "print(\"TN=\",tn)\n",
    "print(\"FP=\",fp)\n",
    "print(\"FN=\",fn)\n",
    "\n",
    "acc = ((tp+tn)/(tp+tn+fp+fn))\n",
    "\n",
    "print(\"Accuracy:%.2f%%\"%(acc*100))\n",
    "print(\"Precision:%.2f%%\"%(precision*100))\n",
    "print(\"Recall:%.2f%%\"%(recall*100))\n",
    "print(\"F1 score:%.2f%%\"%(f1*100))\n",
    "print(\"Roc_Auc score:%.2f%%\"%(roc_auc*100))\n",
    "print(\"F2 score:%.2f%%\"%(f2*100))\n",
    "print(classification_report(Y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fb23c2",
   "metadata": {},
   "source": [
    "Export classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2d395ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the path\n",
    "path = os.path.join(root_path, 'results', model_variation.split(\"/\")[-1], method, str(shuffle_seeder))\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# Define the CSV file path\n",
    "csv_file_path = os.path.join(path, f\"{shuffle_seeder}.csv\")\n",
    "\n",
    "# Write data to CSV\n",
    "data = {\n",
    "    \"accuracy\": accuracy,\n",
    "    \"precision\": precision,\n",
    "    \"recall\": recall,\n",
    "    \"f1\": f1,\n",
    "    \"f2\": f2,\n",
    "    \"roc_auc\": roc_auc\n",
    "}\n",
    "\n",
    "# Write to CSV\n",
    "with open(csv_file_path, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=data.keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerow(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1707ba6",
   "metadata": {},
   "source": [
    "Compute the average values of the classication metrics considering the results for all different seeders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d1912105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.9375563796732485, 'precision': 0.45657015590200445, 'recall': 0.7522935779816514, 'f1': 0.5682605682605683, 'f2': 0.6660168940870695, 'roc_auc': 0.8502774081596127}\n"
     ]
    }
   ],
   "source": [
    "# Define a dictionary to store cumulative sum of metrics\n",
    "cumulative_metrics = defaultdict(float)\n",
    "count = 0  # Counter to keep track of number of CSV files\n",
    "\n",
    "# Iterate over all CSV files in the results folder\n",
    "results_folder = os.path.join(root_path, \"results\", model_variation.split(\"/\")[-1], method, str(shuffle_seeder))\n",
    "for filename in os.listdir(results_folder):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        csv_file_path = os.path.join(results_folder, filename)\n",
    "        with open(csv_file_path, \"r\", newline=\"\") as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                for metric, value in row.items():\n",
    "                    cumulative_metrics[metric] += float(value)\n",
    "        count += 1\n",
    "        \n",
    "# Compute average values\n",
    "average_metrics = {metric: total / count for metric, total in cumulative_metrics.items()}\n",
    "\n",
    "# Print average values \n",
    "print(average_metrics)\n",
    "\n",
    "# Define the path for the average CSV file\n",
    "avg_csv_file_path = os.path.join(root_path, \"results\", model_variation.split(\"/\")[-1], method, \"avg.csv\")\n",
    "\n",
    "# Write average metrics to CSV\n",
    "with open(avg_csv_file_path, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=average_metrics.keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerow(average_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cc08f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
