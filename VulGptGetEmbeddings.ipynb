{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72b7c616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install transformers library.\n",
    "#!pip install -q git+https://github.com/huggingface/transformers.git\n",
    "# Install helper functions.\n",
    "#!pip install -q git+https://github.com/gmihaila/ml_things.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "420b2683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import io\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import csv\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from ml_things import plot_dict, plot_confusion_matrix, fix_text\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from numpy import vstack\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30b0402f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1bd651adbb0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize seeder and randomness\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "#tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afa32ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of transformers model - will use already pretrained model.\n",
    "# Path of transformer model - will load your own model from local disk.\n",
    "model_name_or_path = 'gpt2' # 'microsoft/CodeGPT-small-py' # 'gpt2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4c2b42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "def dropEmpty(tokens0):\n",
    "    tokens = []\n",
    "    for i in range(0, len(tokens0)):\n",
    "        temp = tokens0[i]\n",
    "        if temp != []:\n",
    "            tokens.append(temp)\n",
    "    return tokens\n",
    "\n",
    "# Read pre-processed dataset\n",
    "with open('data_reduced_bert.csv', newline='', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f)\n",
    "    data = list(reader)\n",
    "data = dropEmpty(data)\n",
    "random.shuffle(data)\n",
    "#data = data[0:100] # subsample for checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2df71a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getLengths(data):\n",
    "    lens = []\n",
    "    for i in range(len(data)):\n",
    "        lens.append(len(data[i])-2)\n",
    "    lens = pd.DataFrame(lens)\n",
    "    lensFreq = lens[0].value_counts()\n",
    "    lensFreq=pd.DataFrame(lensFreq)\n",
    "    return lens, lensFreq\n",
    "\n",
    "lens, lensFreq = getLengths(data)\n",
    "max_len = max(lens[0])\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9cb429f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labs = []\n",
    "for d in data:\n",
    "    labs.append(d[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45afda3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n## split dataset to train-val-test sets\\n### split data into train and test (85% train, 15% test)\\ntrain_val, test = train_test_split(data, test_size=val_ratio, random_state=seed, stratify = labs)\\n\\nlabs2 = []\\nfor d in train_val:\\n    labs2.append(d[1])\\n\\n### split train and validation\\ntrain, val = train_test_split(train_val, test_size=val_ratio, random_state=seed, stratify = labs2)\\n\\n\\n## create the base directory if it doesn\\'t exist\\nif not os.path.exists(\\'pythonvp\\'):\\n    os.mkdir(\\'pythonvp\\')\\n\\n## create subdirectories for clean and vuln\\nfor subset in [\\'train\\', \\'val\\', \\'test\\']:\\n    if not os.path.exists(os.path.join(\\'pythonvp\\', subset)):\\n        os.mkdir(os.path.join(\\'pythonvp\\', subset))\\n\\n    ### create subdirectories for train, val, and test\\n    for category in [\\'clean\\', \\'vuln\\']:\\n        if not os.path.exists(os.path.join(\\'pythonvp\\', subset, category)):\\n            os.mkdir(os.path.join(\\'pythonvp\\', subset, category))\\n\\n## write files to appropriate directories\\ndef makeRepo(data, subset):\\n    for item in data:\\n        filename = item[0]\\n        category = \\'clean\\' if item[1] == \\'0\\' else \\'vuln\\'\\n        body = \\' \\'.join([str(token) for token in item[2:]])\\n\\n        with open(os.path.join(\\'pythonvp\\', subset, category, filename[:-3] + \\'.py\\'), \\'w\\') as f:\\n            f.write(body)\\n\\n            \\n## call makeRepo function\\nmakeRepo(train, \\'train\\')\\nmakeRepo(val, \\'val\\')\\nmakeRepo(test, \\'test\\')\\n\\n## Merge train and val folders\\nsubset = \"train_val\"\\nif not os.path.exists(os.path.join(\\'pythonvp\\', subset)):\\n        os.mkdir(os.path.join(\\'pythonvp\\', subset))\\nfor category in [\\'clean\\', \\'vuln\\']:\\n        if not os.path.exists(os.path.join(\\'pythonvp\\', subset, category)):\\n            os.mkdir(os.path.join(\\'pythonvp\\', subset, category))\\n\\nmakeRepo(train_val, \\'train_val\\')'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creation of the dataset's repository\n",
    "val_ratio = 0.15\n",
    "'''\n",
    "## split dataset to train-val-test sets\n",
    "### split data into train and test (85% train, 15% test)\n",
    "train_val, test = train_test_split(data, test_size=val_ratio, random_state=seed, stratify = labs)\n",
    "\n",
    "labs2 = []\n",
    "for d in train_val:\n",
    "    labs2.append(d[1])\n",
    "\n",
    "### split train and validation\n",
    "train, val = train_test_split(train_val, test_size=val_ratio, random_state=seed, stratify = labs2)\n",
    "\n",
    "\n",
    "## create the base directory if it doesn't exist\n",
    "if not os.path.exists('pythonvp'):\n",
    "    os.mkdir('pythonvp')\n",
    "\n",
    "## create subdirectories for clean and vuln\n",
    "for subset in ['train', 'val', 'test']:\n",
    "    if not os.path.exists(os.path.join('pythonvp', subset)):\n",
    "        os.mkdir(os.path.join('pythonvp', subset))\n",
    "\n",
    "    ### create subdirectories for train, val, and test\n",
    "    for category in ['clean', 'vuln']:\n",
    "        if not os.path.exists(os.path.join('pythonvp', subset, category)):\n",
    "            os.mkdir(os.path.join('pythonvp', subset, category))\n",
    "\n",
    "## write files to appropriate directories\n",
    "def makeRepo(data, subset):\n",
    "    for item in data:\n",
    "        filename = item[0]\n",
    "        category = 'clean' if item[1] == '0' else 'vuln'\n",
    "        body = ' '.join([str(token) for token in item[2:]])\n",
    "\n",
    "        with open(os.path.join('pythonvp', subset, category, filename[:-3] + '.py'), 'w') as f:\n",
    "            f.write(body)\n",
    "\n",
    "            \n",
    "## call makeRepo function\n",
    "makeRepo(train, 'train')\n",
    "makeRepo(val, 'val')\n",
    "makeRepo(test, 'test')\n",
    "\n",
    "## Merge train and val folders\n",
    "subset = \"train_val\"\n",
    "if not os.path.exists(os.path.join('pythonvp', subset)):\n",
    "        os.mkdir(os.path.join('pythonvp', subset))\n",
    "for category in ['clean', 'vuln']:\n",
    "        if not os.path.exists(os.path.join('pythonvp', subset, category)):\n",
    "            os.mkdir(os.path.join('pythonvp', subset, category))\n",
    "\n",
    "makeRepo(train_val, 'train_val')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6354bc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define models and parameters\n",
    "from transformers import (set_seed,\n",
    "                          TrainingArguments,\n",
    "                          Trainer,\n",
    "                          GPT2Config,\n",
    "                          GPT2Tokenizer,\n",
    "                          AdamW, \n",
    "                          get_linear_schedule_with_warmup,\n",
    "                          GPT2ForSequenceClassification,\n",
    "                          GPT2Model)\n",
    "\n",
    "# Set seed for reproducibility.\n",
    "set_seed(seed)\n",
    "\n",
    "# Number of batches - depending on the max sequence length and GPU memory.\n",
    "# For 512 sequence length batch of 10 works without cuda memory issues.\n",
    "# For small sequence length can try batch of 32 or higher.\n",
    "batch_size = 32\n",
    "\n",
    "# Pad or truncate text sequences to a specific length\n",
    "# if `None` it will use maximum sequence of word piece tokens allowed by model.\n",
    "max_length = 120\n",
    "\n",
    "# Look for gpu to use. Will use `cpu` by default if no gpu found.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Dictionary of labels and their id - this will be used to convert.\n",
    "# String labels to number ids.\n",
    "labels_ids = {'clean': 0, 'vuln': 1}\n",
    "\n",
    "# How many labels are we using in training.\n",
    "# This is used to decide size of classification head.\n",
    "n_labels = len(labels_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc1f808e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VulnCodeDataset(Dataset):\n",
    "  r\"\"\"PyTorch Dataset class for loading data.\n",
    "\n",
    "  This is where the data parsing happens.\n",
    "\n",
    "  This class is built with reusability in mind: it can be used as is as.\n",
    "\n",
    "  Arguments:\n",
    "\n",
    "    path (:obj:`str`):\n",
    "        Path to the data partition.\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, path, use_tokenizer):\n",
    "\n",
    "    # Check if path exists.\n",
    "    if not os.path.isdir(path):\n",
    "      # Raise error if path is invalid.\n",
    "      raise ValueError('Invalid `path` variable! Needs to be a directory')\n",
    "    self.texts = []\n",
    "    self.labels = []\n",
    "    # Since the labels are defined by folders with data we loop \n",
    "    # through each label.\n",
    "    for label in ['vuln', 'clean']:\n",
    "      code_path = os.path.join(path, label)\n",
    "\n",
    "      # Get all files from path.\n",
    "      files_names = os.listdir(code_path)#[:10] # Sample for debugging.\n",
    "      # Go through each file and read its content.\n",
    "      for file_name in tqdm(files_names, desc=f'{label} files'):\n",
    "        file_path = os.path.join(code_path, file_name)\n",
    "\n",
    "        # Read content.\n",
    "        \n",
    "        #content = io.open(file_path, mode='r', encoding='utf-8').read()\n",
    "        content = io.open(file_path, mode='r', encoding=\"ISO-8859-1\").read()\n",
    "        # Fix any unicode issues.\n",
    "        content = fix_text(content)\n",
    "        # Save content.\n",
    "        self.texts.append(content)\n",
    "        # Save encode labels.\n",
    "        self.labels.append(label)\n",
    "\n",
    "    # Number of exmaples.\n",
    "    self.n_examples = len(self.labels)\n",
    "    \n",
    "\n",
    "    return\n",
    "\n",
    "  def __len__(self):\n",
    "    r\"\"\"When used `len` return the number of examples.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    return self.n_examples\n",
    "\n",
    "  def __getitem__(self, item):\n",
    "    r\"\"\"Given an index return an example from the position.\n",
    "    \n",
    "    Arguments:\n",
    "\n",
    "      item (:obj:`int`):\n",
    "          Index position to pick an example to return.\n",
    "\n",
    "    Returns:\n",
    "      :obj:`Dict[str, str]`: Dictionary of inputs that contain text and \n",
    "      asociated labels.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    return {'text':self.texts[item],\n",
    "            'label':self.labels[item]}\n",
    "\n",
    "\n",
    "\n",
    "class Gpt2ClassificationCollator(object):\n",
    "    r\"\"\"\n",
    "    Data Collator used for GPT2 in a classificaiton rask. \n",
    "    \n",
    "    It uses a given tokenizer and label encoder to convert any text and labels to numbers that \n",
    "    can go straight into a GPT2 model.\n",
    "\n",
    "    This class is built with reusability in mind: it can be used as is as long\n",
    "    as the `dataloader` outputs a batch in dictionary format that can be passed \n",
    "    straight into the model - `model(**batch)`.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "      use_tokenizer (:obj:`transformers.tokenization_?`):\n",
    "          Transformer type tokenizer used to process raw text into numbers.\n",
    "\n",
    "      labels_ids (:obj:`dict`):\n",
    "          Dictionary to encode any labels names into numbers. Keys map to \n",
    "          labels names and Values map to number associated to those labels.\n",
    "\n",
    "      max_sequence_len (:obj:`int`, `optional`)\n",
    "          Value to indicate the maximum desired sequence to truncate or pad text\n",
    "          sequences. If no value is passed it will used maximum sequence size\n",
    "          supported by the tokenizer and model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, use_tokenizer, labels_encoder, max_sequence_len=None):\n",
    "\n",
    "        # Tokenizer to be used inside the class.\n",
    "        self.use_tokenizer = use_tokenizer\n",
    "        # Check max sequence length.\n",
    "        self.max_sequence_len = use_tokenizer.model_max_length if max_sequence_len is None else max_sequence_len\n",
    "        # Label encoder used inside the class.\n",
    "        self.labels_encoder = labels_encoder\n",
    "\n",
    "        return\n",
    "\n",
    "    def __call__(self, sequences):\n",
    "        r\"\"\"\n",
    "        This function allowes the class objesct to be used as a function call.\n",
    "        Sine the PyTorch DataLoader needs a collator function, I can use this \n",
    "        class as a function.\n",
    "\n",
    "        Arguments:\n",
    "\n",
    "          item (:obj:`list`):\n",
    "              List of texts and labels.\n",
    "\n",
    "        Returns:\n",
    "          :obj:`Dict[str, object]`: Dictionary of inputs that feed into the model.\n",
    "          It holddes the statement `model(**Returned Dictionary)`.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get all texts from sequences list.\n",
    "        texts = [sequence['text'] for sequence in sequences]\n",
    "        # Get all labels from sequences list.\n",
    "        labels = [sequence['label'] for sequence in sequences]\n",
    "        # Encode all labels using label encoder.\n",
    "        labels = [self.labels_encoder[label] for label in labels]\n",
    "        # Call tokenizer on all texts to convert into tensors of numbers with \n",
    "        # appropriate padding.\n",
    "        inputs = self.use_tokenizer(text=texts, return_tensors=\"pt\", padding=True, truncation=True,  max_length=self.max_sequence_len)\n",
    "        # Update the inputs with the associated encoded labels as tensor.\n",
    "        inputs.update({'labels':torch.tensor(labels)})\n",
    "\n",
    "        return inputs\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dae5c16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUniques(data):\n",
    "    allWords = []\n",
    "    for i in range(len(data)):\n",
    "        for j in range(2, len(data[i])):\n",
    "            allWords.append(data[i][j])\n",
    "    \n",
    "    vc = pd.Series(allWords).value_counts()\n",
    "    uniques=vc.index.values.tolist()        \n",
    "    return allWords, uniques, vc\n",
    "\n",
    "def getVocab(data):\n",
    "    ## split dataset to train-val-test sets\n",
    "    ### split data into train and test (85% train, 15% test)\n",
    "    train_val_indices, test_indices = train_test_split(range(len(data)), test_size=val_ratio, random_state=seed)\n",
    "    train_val_data = [data[i] for i in train_val_indices]\n",
    "\n",
    "    allWords, vocab, freqs = getUniques(train_val_data)\n",
    "    return vocab\n",
    "\n",
    "def addTokens(data, tokenizer, model):\n",
    "    new_tokens = getVocab(data)\n",
    "\n",
    "    for new_token in new_tokens:\n",
    "        if new_token not in tokenizer.get_vocab().keys():\n",
    "            tokenizer.add_tokens(new_token)\n",
    "\n",
    "    # resize model embedding to match new tokenizer\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41358f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configuraiton...\n",
      "Loading tokenizer...\n",
      "Loading model...\n",
      "Model loaded to `cuda`\n"
     ]
    }
   ],
   "source": [
    "# Get model configuration.\n",
    "print('Loading configuraiton...')\n",
    "model_config = GPT2Config.from_pretrained(pretrained_model_name_or_path=model_name_or_path, num_labels=n_labels)\n",
    "\n",
    "# Get model's tokenizer.\n",
    "print('Loading tokenizer...')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_name_or_path=model_name_or_path, do_lower_case = True)\n",
    "# default to left padding\n",
    "tokenizer.padding_side = \"left\"\n",
    "# Define PAD Token = EOS Token = 50256\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "# Get the actual model.\n",
    "print('Loading model...')\n",
    "model = GPT2Model.from_pretrained(pretrained_model_name_or_path=model_name_or_path, config=model_config)\n",
    "\n",
    "# resize model embedding to match new tokenizer\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# fix model padding token id\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# Define New tokens for string and numerical i.e., strId$ and numId$\n",
    "new_tokens = [\"strId$\", \"numId$\"]\n",
    "for new_token in new_tokens:\n",
    "    if new_token not in tokenizer.get_vocab().keys():\n",
    "        tokenizer.add_tokens(new_token)\n",
    "\n",
    "# resize model embedding to match new tokenizer\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# add all new tokens\n",
    "#tokenizer, model = addTokens(data, tokenizer, model)\n",
    "\n",
    "# fix model new tokens ids\n",
    "\n",
    "# Load model to defined device.\n",
    "model.to(device)\n",
    "print('Model loaded to `%s`'%device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8beaa7da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 4342,   318,   281,  1672,  6827,   351, 50257,   392, 50258,    83,\n",
       "           482,   641,    13]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input = tokenizer(\"Here is an example sentence with strId$ and numId$ tokens.\", padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9580ec90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set MLP architecture\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = nn.functional.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "\n",
    "# Set MLP hyperparameters\n",
    "input_size = 768 # size of GPT-2 embeddings\n",
    "hidden_size = 100\n",
    "num_classes = 2\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# Define MLP and optimizer\n",
    "mlp = MLP(input_size, hidden_size, num_classes).to(device)\n",
    "optimizer = optim.Adam(mlp.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89434919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dealing with Train...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ecfad704b7846269a9b9ab888fe58cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vuln files:   0%|          | 0/924 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3846cc4816a64717a0a6aec1156f1182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "clean files:   0%|          | 0/2932 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created `train_dataset` with 3856 examples!\n",
      "Created `train_dataloader` with 121 batches!\n",
      "\n",
      "Dealing with Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e923ebeb2cef4d40830ea1faaa92daad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vuln files:   0%|          | 0/238 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92a2b27be9344dcca07af7af962a7541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "clean files:   0%|          | 0/767 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created `valid_dataset` with 1005 examples!\n",
      "Created `eval_dataloader` with 32 batches!\n",
      "Dealing with Testing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aad36e1b040f430594e3b0ab5c38c67f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vuln files:   0%|          | 0/275 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e86e3801f29f443dab23e3c2c5c7c67d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "clean files:   0%|          | 0/890 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created `test_dataset` with 1165 examples!\n",
      "Created `eval_dataloader` with 37 batches!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30ecf464774c4675a48421c78cfb25e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vuln files:   0%|          | 0/973 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07ead2b48cb34ff985a08746379ec0bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "clean files:   0%|          | 0/3120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created `train_dataset` with 4093 examples!\n",
      "Created `train_dataloader` with 128 batches!\n"
     ]
    }
   ],
   "source": [
    "# Create data collator to encode text and labels into numbers.\n",
    "gpt2_classificaiton_collator = Gpt2ClassificationCollator(use_tokenizer=tokenizer, \n",
    "                                                          labels_encoder=labels_ids, \n",
    "                                                          max_sequence_len=max_length)\n",
    "\n",
    "\n",
    "print('Dealing with Train...')\n",
    "# Create pytorch dataset.\n",
    "train_dataset = VulnCodeDataset(path='pythonvp\\\\train', \n",
    "                               use_tokenizer=tokenizer)\n",
    "print('Created `train_dataset` with %d examples!'%len(train_dataset))\n",
    "\n",
    "# Move pytorch dataset into dataloader.\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=gpt2_classificaiton_collator)\n",
    "print('Created `train_dataloader` with %d batches!'%len(train_dataloader))\n",
    "\n",
    "print()\n",
    "\n",
    "print('Dealing with Validation...')\n",
    "# Create pytorch dataset.\n",
    "valid_dataset =  VulnCodeDataset(path='pythonvp\\\\val', \n",
    "                               use_tokenizer=tokenizer)\n",
    "print('Created `valid_dataset` with %d examples!'%len(valid_dataset))\n",
    "\n",
    "# Move pytorch dataset into dataloader.\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=gpt2_classificaiton_collator)\n",
    "print('Created `eval_dataloader` with %d batches!'%len(valid_dataloader))\n",
    "\n",
    "print('Dealing with Testing...')\n",
    "# Create pytorch dataset.\n",
    "test_dataset =  VulnCodeDataset(path='pythonvp\\\\test', \n",
    "                               use_tokenizer=tokenizer)\n",
    "print('Created `test_dataset` with %d examples!'%len(test_dataset))\n",
    "\n",
    "# Move pytorch dataset into dataloader.\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=gpt2_classificaiton_collator)\n",
    "print('Created `eval_dataloader` with %d batches!'%len(test_dataloader))\n",
    "\n",
    "\n",
    "\n",
    "# Create pytorch dataset.\n",
    "train_val_dataset = VulnCodeDataset(path='pythonvp\\\\train_val', \n",
    "                               use_tokenizer=tokenizer)\n",
    "print('Created `train_dataset` with %d examples!'%len(train_val_dataset))\n",
    "\n",
    "# Move pytorch dataset into dataloader.\n",
    "train_val_dataloader = DataLoader(train_val_dataset, batch_size=batch_size, shuffle=True, collate_fn=gpt2_classificaiton_collator)\n",
    "print('Created `train_dataloader` with %d batches!'%len(train_val_dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6727f27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3856\n",
      "3856\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    train_embeddings = []\n",
    "    train_labels = []\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        batch_embeddings = output[0][:, -1, :]\n",
    "        batch_labels = batch['labels']\n",
    "        train_labels.append(batch_labels)\n",
    "        train_embeddings.append(batch_embeddings)\n",
    "    train_embeddings = torch.cat(train_embeddings, dim=0)\n",
    "    train_labels = torch.cat(train_labels, dim=0)\n",
    "print(len(train_embeddings))\n",
    "print(len(train_labels))\n",
    "train_embeddingsdf = pd.DataFrame(train_embeddings.cpu().numpy())\n",
    "train_embeddingsdf.insert(0, 'Label', train_labels)\n",
    "train_embeddingsdf.to_csv('train_embeddings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3601992d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1005\n",
      "1005\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    val_embeddings = []\n",
    "    val_labels = []\n",
    "    for batch in valid_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        batch_embeddings = output[0][:, -1, :]\n",
    "        batch_labels = batch['labels']\n",
    "        val_labels.append(batch_labels)\n",
    "        val_embeddings.append(batch_embeddings)\n",
    "    val_embeddings = torch.cat(val_embeddings, dim=0)\n",
    "    val_labels = torch.cat(val_labels, dim=0)\n",
    "print(len(val_embeddings))\n",
    "print(len(val_labels))\n",
    "val_embeddingsdf = pd.DataFrame(val_embeddings.cpu().numpy())\n",
    "val_embeddingsdf.insert(0, 'Label', val_labels)\n",
    "val_embeddingsdf.to_csv('val_embeddings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "210e2db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4093\n",
      "4093\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    train_val_embeddings = []\n",
    "    train_val_labels = []\n",
    "    for batch in train_val_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        batch_embeddings = output[0][:, -1, :]\n",
    "        batch_labels = batch['labels']\n",
    "        train_val_labels.append(batch_labels)\n",
    "        train_val_embeddings.append(batch_embeddings)\n",
    "    train_val_embeddings = torch.cat(train_val_embeddings, dim=0)\n",
    "    train_val_labels = torch.cat(train_val_labels, dim=0)\n",
    "print(len(train_val_embeddings))\n",
    "print(len(train_val_labels))\n",
    "train_val_embeddingsdf = pd.DataFrame(train_val_embeddings.cpu().numpy())\n",
    "train_val_embeddingsdf.insert(0, 'Label', train_val_labels)\n",
    "train_val_embeddingsdf.to_csv('train_val_embeddings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec96ba03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1165\n",
      "1165\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    test_embeddings = []\n",
    "    test_labels = []\n",
    "    for batch in test_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        batch_embeddings = output[0][:, -1, :]\n",
    "        batch_labels = batch['labels']\n",
    "        test_labels.append(batch_labels)\n",
    "        test_embeddings.append(batch_embeddings)\n",
    "    test_embeddings = torch.cat(test_embeddings, dim=0)\n",
    "    test_labels = torch.cat(test_labels, dim=0)\n",
    "print(len(test_embeddings))\n",
    "print(len(test_labels))\n",
    "test_embeddingsdf = pd.DataFrame(test_embeddings.cpu().numpy())\n",
    "test_embeddingsdf.insert(0, 'Label', test_labels)\n",
    "test_embeddingsdf.to_csv('test_embeddings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5309d9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(768, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(100, 2),\n",
    "    nn.Sigmoid() # nn.Softmax()\n",
    ")\n",
    "\n",
    "# nn.Linear(764, 100, device=\"cuda:0\")\n",
    "\n",
    "loss_fn = nn.BCELoss() # CrossEntropyLoss\n",
    "learning_rate = 0.001\n",
    "epochs = 5\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "BS = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "54b37386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the dataset\n",
    "def prepare_data(dataset):\n",
    "    train, test = dataset.get_splits() \n",
    "    # prepare data loaders\n",
    "    train_dl = DataLoader(train, batch_size=BS, shuffle=True)\n",
    "    test_dl = DataLoader(test, batch_size=BS, shuffle=False)\n",
    "    return train_dl, test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "5eaa5f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.savetxt('embeddings.txt', val_embeddings.cpu().numpy(), delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21f425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, test_dl = prepare_data(train, val)\n",
    "print(len(train_dl.dataset), len(test_dl.dataset))\n",
    "# train the model\n",
    "criterion = loss_fn\n",
    "# enumerate epochs\n",
    "for epoch in range(epochs):\n",
    "    # enumerate mini batches\n",
    "    for i, (inputs, targets) in enumerate(train_dl):\n",
    "        # clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # compute the model output\n",
    "        yhat = model(inputs)\n",
    "        # calculate loss\n",
    "        loss = criterion(yhat, targets)\n",
    "        # credit assignment\n",
    "        loss.backward()\n",
    "        # update model weights\n",
    "        optimizer.step()\n",
    "\n",
    "predictions, actuals = list(), list()\n",
    "for i, (inputs, targets) in enumerate(test_dl):\n",
    "    # evaluate the model on the test set\n",
    "    yhat = model(inputs)\n",
    "    # retrieve numpy array\n",
    "    yhat = yhat.detach().numpy()\n",
    "    actual = targets.numpy()\n",
    "    actual = actual.reshape((len(actual), 1))\n",
    "    # round to class values\n",
    "    yhat = yhat.round()\n",
    "    # store\n",
    "    predictions.append(yhat)\n",
    "    actuals.append(actual)\n",
    "predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "# calculate accuracy\n",
    "#acc = accuracy_score(actuals, predictions)\n",
    "tn, fp, fn, tp = confusion_matrix(actuals, predictions).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "\n",
    "val_acc=(tp+tn)/(tp+tn+fp+fn)\n",
    "val_prec=tp/(tp+fp)\n",
    "val_rec=tp/(tp+fn)\n",
    "val_f1=2*val_prec*val_rec / (val_prec+val_rec)\n",
    "val_f2=5*val_prec*val_rec / (4*val_prec+val_rec)\n",
    "print(\"Accuracy: \", val_acc)\n",
    "print(\"Precision: \", val_prec)\n",
    "print(\"Recall: \", val_rec)\n",
    "print(\"F1-score: \", val_f1)\n",
    "print(\"F2-score: \", val_f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140a1aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, test_dl = prepare_data(train_val, test)\n",
    "print(len(train_dl.dataset), len(test_dl.dataset))\n",
    "# train the model\n",
    "criterion = loss_fn\n",
    "# enumerate epochs\n",
    "for epoch in range(epochs):\n",
    "    # enumerate mini batches\n",
    "    for i, (inputs, targets) in enumerate(train_dl):\n",
    "        # clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # compute the model output\n",
    "        yhat = model(inputs)\n",
    "        # calculate loss\n",
    "        loss = criterion(yhat, targets)\n",
    "        # credit assignment\n",
    "        loss.backward()\n",
    "        # update model weights\n",
    "        optimizer.step()\n",
    "\n",
    "predictions, actuals = list(), list()\n",
    "for i, (inputs, targets) in enumerate(test_dl):\n",
    "    # evaluate the model on the test set\n",
    "    yhat = model(inputs)\n",
    "    # retrieve numpy array\n",
    "    yhat = yhat.detach().numpy()\n",
    "    actual = targets.numpy()\n",
    "    actual = actual.reshape((len(actual), 1))\n",
    "    # round to class values\n",
    "    yhat = yhat.round()\n",
    "    # store\n",
    "    predictions.append(yhat)\n",
    "    actuals.append(actual)\n",
    "predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "# calculate accuracy\n",
    "#acc = accuracy_score(actuals, predictions)\n",
    "tn, fp, fn, tp = confusion_matrix(actuals, predictions).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "\n",
    "val_acc=(tp+tn)/(tp+tn+fp+fn)\n",
    "val_prec=tp/(tp+fp)\n",
    "val_rec=tp/(tp+fn)\n",
    "val_f1=2*val_prec*val_rec / (val_prec+val_rec)\n",
    "val_f2=5*val_prec*val_rec / (4*val_prec+val_rec)\n",
    "print(\"Accuracy: \", val_acc)\n",
    "print(\"Precision: \", val_prec)\n",
    "print(\"Recall: \", val_rec)\n",
    "print(\"F1-score: \", val_f1)\n",
    "print(\"F2-score: \", val_f2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
