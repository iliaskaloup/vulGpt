{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "007b5205",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c52dd00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import json, os\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import AdamW, Adam\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "from transformers import set_seed\n",
    "from transformers import AdamWeightDecay\n",
    "from transformers import AutoTokenizer, RobertaTokenizer, AutoModelForSequenceClassification #, BertModel, BertTokenizer\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, \\\n",
    "roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f55447",
   "metadata": {},
   "source": [
    "Specify a constant seeder for processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca9607ac-f296-43ed-afd7-3a2c082981e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeders = [123456, 789012, 345678, 901234, 567890, 123, 456, 789, 012, 345]\n",
    "\n",
    "seed = seeders[0]\n",
    "\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494ef531",
   "metadata": {},
   "source": [
    "Pre-trained tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aca2c333",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_variation = \"microsoft/codebert-base-mlm\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_variation, do_lower_case=True) #Tokenizer\n",
    "#bert-base-uncased #bert-base # roberta-base # distilbert-base-uncased #distilbert-base # microsoft/codebert-base-mlm\n",
    "# 'albert-base-v2'\n",
    "\n",
    "# tokenizer = RobertaTokenizer(vocab_file=\"../../tokenizer_training/cpp_tokenizer/cpp_tokenizer-vocab.json\",\n",
    "#                              merges_file=\"../../tokenizer_training/cpp_tokenizer/cpp_tokenizer-merges.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88221c5a",
   "metadata": {},
   "source": [
    "Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6af2cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = os.path.join('..', '..', '..')\n",
    "dataset = pd.read_csv(os.path.join(root_path, 'data', 'train.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0048c42f",
   "metadata": {},
   "source": [
    "Shuffle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06f1a1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    index Access Gained Attack Origin Authentication Required Availability  \\\n",
      "0  167605           NaN        Remote            Not required          NaN   \n",
      "1  142713           NaN           NaN                     NaN          NaN   \n",
      "2  123912           NaN        Remote            Not required      Partial   \n",
      "3   53817           NaN         Local            Not required     Complete   \n",
      "4   29110           NaN         Local            Not required     Complete   \n",
      "\n",
      "           CVE ID                                        CVE Page   CWE ID  \\\n",
      "0  CVE-2018-16073  https://www.cvedetails.com/cve/CVE-2018-16073/  CWE-285   \n",
      "1             NaN                                             NaN      NaN   \n",
      "2   CVE-2013-0918   https://www.cvedetails.com/cve/CVE-2013-0918/  CWE-264   \n",
      "3   CVE-2016-3699   https://www.cvedetails.com/cve/CVE-2016-3699/  CWE-264   \n",
      "4   CVE-2013-4591   https://www.cvedetails.com/cve/CVE-2013-4591/  CWE-119   \n",
      "\n",
      "  Complexity Confidentiality  ... parentID  \\\n",
      "0     Medium         Partial  ...      NaN   \n",
      "1        NaN             NaN  ...      NaN   \n",
      "2     Medium         Partial  ...      NaN   \n",
      "3     Medium        Complete  ...      NaN   \n",
      "4       High        Complete  ...      NaN   \n",
      "\n",
      "                                               patch project  \\\n",
      "0  @@ -457,16 +457,27 @@ GURL SiteInstance::GetSi...  Chrome   \n",
      "1  @@ -725,6 +725,7 @@ void FrameLoader::detachDo...  Chrome   \n",
      "2  @@ -3039,9 +3039,9 @@ WebNavigationPolicy Rend...  Chrome   \n",
      "3  @@ -707,6 +707,12 @@ void __init acpi_initrd_o...   linux   \n",
      "4  @@ -3937,8 +3937,13 @@ static ssize_t __nfs4_g...   linux   \n",
      "\n",
      "                              project_after  \\\n",
      "0  0bb3f5c715eb66bb5c1fb05fd81d902ca57f33ca   \n",
      "1  be655fd4fb9ab3291a855a939496111674037a2f   \n",
      "2  0a57375ad73780e61e1770a9d88b0529b0dbd33b   \n",
      "3  a4a5ed2835e8ea042868b7401dced3f517cafa76   \n",
      "4  7d3e91a89b7adbc2831334def9e494dd9892f9af   \n",
      "\n",
      "                             project_before target  \\\n",
      "0  659c9e34e14cfcecf33f2b2e52aa133db5806fa2      0   \n",
      "1  23024078fb2e07f6852031e76f08712bcd6303f0      0   \n",
      "2  e3cb4529d79a4993535da612dafedc8c40f075bb      0   \n",
      "3  4b2b64d5a6ebc84214755ebccd599baef7c1b798      0   \n",
      "4  4c1002100898d03c5c9142ffaf58351c841ab94a      0   \n",
      "\n",
      "                                   vul_func_with_fix  \\\n",
      "0  bool SiteInstanceImpl::IsOriginLockASite(const...   \n",
      "1  static bool shouldComplete(Document* document)...   \n",
      "2  void RenderViewImpl::OnSetName(const std::stri...   \n",
      "3  ssize_t acpi_debugger_read_cmd(char *buffer, s...   \n",
      "4  static int _nfs41_test_stateid(struct nfs_serv...   \n",
      "\n",
      "                                      processed_func flaw_line flaw_line_index  \n",
      "0  bool SiteInstanceImpl::IsOriginLockASite(const...       NaN             NaN  \n",
      "1  static bool shouldComplete(Document* document)...       NaN             NaN  \n",
      "2  void RenderViewImpl::OnSetName(const std::stri...       NaN             NaN  \n",
      "3  ssize_t acpi_debugger_read_cmd(char *buffer, s...       NaN             NaN  \n",
      "4  static int _nfs41_test_stateid(struct nfs_serv...       NaN             NaN  \n",
      "\n",
      "[5 rows x 39 columns]\n",
      "150908\n"
     ]
    }
   ],
   "source": [
    "data = dataset.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "print(data.head())\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ec40f4b-c562-4685-af52-2b6ac0c8f0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89145\n"
     ]
    }
   ],
   "source": [
    "data = data[data[\"project\"] != \"Chrome\"]\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9aa48688-efda-4abc-88b8-e159fe6dbd77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed_func</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ssize_t acpi_debugger_read_cmd(char *buffer, s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>static int _nfs41_test_stateid(struct nfs_serv...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GF_Box *rvcc_New()\\n{\\n\\tISOM_DECL_BOX_ALLOC(G...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>static int snd_seq_open(struct inode *inode, s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>char **XGetFontPath(\\nregister Display *dpy,\\n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       processed_func  target\n",
       "3   ssize_t acpi_debugger_read_cmd(char *buffer, s...       0\n",
       "4   static int _nfs41_test_stateid(struct nfs_serv...       0\n",
       "8   GF_Box *rvcc_New()\\n{\\n\\tISOM_DECL_BOX_ALLOC(G...       0\n",
       "11  static int snd_seq_open(struct inode *inode, s...       0\n",
       "14  char **XGetFontPath(\\nregister Display *dpy,\\n...       1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[[\"processed_func\", \"target\"]]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e09f413",
   "metadata": {},
   "source": [
    "Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ea3dfd9-9aa9-4c8b-bfc4-02e8ac837a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset=[\"processed_func\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "479d07b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of words: 15441\n"
     ]
    }
   ],
   "source": [
    "word_counts = data[\"processed_func\"].apply(lambda x: len(x.split()))\n",
    "max_length = word_counts.max()\n",
    "print(\"Maximum number of words:\", max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd349b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "0    83578\n",
      "1     5567\n",
      "Name: count, dtype: int64\n",
      "Percentage:  6.660843762712676 %\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "vc = data[\"target\"].value_counts()\n",
    "\n",
    "print(vc)\n",
    "\n",
    "print(\"Percentage: \", (vc[1] / vc[0])*100, '%')\n",
    "\n",
    "n_categories = len(vc)\n",
    "print(n_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ab7af89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ssize_t acpi_debugger_read_cmd(char *buffer, s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>static int _nfs41_test_stateid(struct nfs_serv...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GF_Box *rvcc_New()\\n{\\n\\tISOM_DECL_BOX_ALLOC(G...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>static int snd_seq_open(struct inode *inode, s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>char **XGetFontPath(\\nregister Display *dpy,\\n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Text  Labels\n",
       "3   ssize_t acpi_debugger_read_cmd(char *buffer, s...       0\n",
       "4   static int _nfs41_test_stateid(struct nfs_serv...       0\n",
       "8   GF_Box *rvcc_New()\\n{\\n\\tISOM_DECL_BOX_ALLOC(G...       0\n",
       "11  static int snd_seq_open(struct inode *inode, s...       0\n",
       "14  char **XGetFontPath(\\nregister Display *dpy,\\n...       1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.DataFrame(({'Text': data['processed_func'], 'Labels': data['target']}))\n",
    "#data = data[0:100]\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b6f1bb",
   "metadata": {},
   "source": [
    "Split to train-val-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d246005d-9d4a-402d-9b83-6d6b38d67225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>int iwlagn_add_bssid_station(struct iwl_priv *...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>static int dnxhd_init_vlc(DNXHDContext *ctx, u...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>void CameraService::onFirstRef()\\n{\\n    LOG1(...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>int EmbedStream::getChar() {\\n  if (limited &amp;&amp;...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>json_t *json_rpc_call(CURL *curl, const char *...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Labels\n",
       "1  int iwlagn_add_bssid_station(struct iwl_priv *...       0\n",
       "2  static int dnxhd_init_vlc(DNXHDContext *ctx, u...       0\n",
       "3  void CameraService::onFirstRef()\\n{\\n    LOG1(...       0\n",
       "4  int EmbedStream::getChar() {\\n  if (limited &&...       0\n",
       "5  json_t *json_rpc_call(CURL *curl, const char *...       0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data = pd.read_csv(os.path.join(root_path, 'data', 'val.csv'))\n",
    "\n",
    "val_data = val_data[val_data[\"project\"] != \"Chrome\"]\n",
    "\n",
    "val_data = pd.DataFrame(({'Text': val_data['processed_func'], 'Labels': val_data['target']}))\n",
    "val_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d1fbe2d-5b83-4d74-bb6d-f3a5dae035bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(os.path.join(root_path, 'data', 'test.csv'))\n",
    "\n",
    "test_data = test_data[test_data[\"project\"] != \"Chrome\"]\n",
    "\n",
    "test_data = pd.DataFrame(({'Text': test_data['processed_func'], 'Labels': test_data['target']}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b393b7",
   "metadata": {},
   "source": [
    "Pre-processing step: Under-sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55de8064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution  Labels\n",
      "0    83578\n",
      "1     5567\n",
      "Name: count, dtype: int64\n",
      "Majority class  0\n",
      "Minority class  1\n",
      "Targeted number of majority class 22268\n",
      "Class distribution after augmentation Labels\n",
      "0    22268\n",
      "1     5567\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "sampling = False\n",
    "if n_categories == 2 and sampling == True:\n",
    "    # Apply under-sampling with the specified strategy\n",
    "    class_counts = pd.Series(train_data[\"Labels\"]).value_counts()\n",
    "    print(\"Class distribution \", class_counts)\n",
    "\n",
    "    majority_class = class_counts.idxmax()\n",
    "    print(\"Majority class \", majority_class)\n",
    "\n",
    "    minority_class = class_counts.idxmin()\n",
    "    print(\"Minority class \", minority_class)\n",
    "\n",
    "    target_count = 4 * class_counts[class_counts.idxmin()] # int(class_counts[class_counts.idxmax()] / 2) # 2 * class_counts[class_counts.idxmin()] # class_counts[class_counts.idxmin()] # int(class_counts.iloc[0] / 2)  \n",
    "    print(\"Targeted number of majority class\", target_count)\n",
    "\n",
    "    # under\n",
    "    sampling_strategy = {majority_class: target_count}        \n",
    "    rus = RandomUnderSampler(random_state=seed, sampling_strategy=sampling_strategy)\n",
    "\n",
    "    x_train_resampled, y_train_resampled = rus.fit_resample(np.array(train_data[\"Text\"]).reshape(-1, 1), train_data[\"Labels\"]) \n",
    "    print(\"Class distribution after augmentation\", pd.Series(y_train_resampled).value_counts())\n",
    "\n",
    "\n",
    "    # Shuffle the resampled data while preserving the correspondence between features and labels\n",
    "    x_train_resampled, y_train_resampled = shuffle(x_train_resampled, y_train_resampled, random_state=seed)\n",
    "\n",
    "    # rename\n",
    "    X_train = x_train_resampled\n",
    "    Y_train = y_train_resampled\n",
    "\n",
    "    X_train = pd.Series(X_train.reshape(-1))\n",
    "\n",
    "else:\n",
    "    X_train = train_data[\"Text\"]\n",
    "    Y_train = train_data[\"Labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc04f73",
   "metadata": {},
   "source": [
    "Pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "561c45d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_variation, num_labels=n_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ad207b",
   "metadata": {},
   "source": [
    "Resize model embedding to match new tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3369b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50265, 768, padding_idx=1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f8c8a0",
   "metadata": {},
   "source": [
    "Compute maximum length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb29633e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMaxLen(X):\n",
    "\n",
    "    # Code for identifying max length of the data samples after tokenization using transformer tokenizer\n",
    "    \n",
    "    max_length = 0\n",
    "    max_row = 0\n",
    "    \n",
    "    # Iterate over each sample in your dataset\n",
    "    for i, input_ids in enumerate(X['input_ids']):\n",
    "        # Convert input_ids to a PyTorch tensor\n",
    "        input_ids_tensor = torch.tensor(input_ids)\n",
    "        # Calculate the length of the tokenized sequence for the current sample\n",
    "        length = torch.sum(input_ids_tensor != tokenizer.pad_token_id).item()\n",
    "        # Update max_length and max_row if the current length is greater\n",
    "        if length > max_length:\n",
    "            max_length = length\n",
    "            max_row = i\n",
    "\n",
    "    print(\"Max length of tokenized data:\", max_length)\n",
    "    print(\"Row with max length:\", max_row)\n",
    "    \n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ee53d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iliaskaloup\\AppData\\Local\\Temp\\ipykernel_7632\\514096298.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids_tensor = torch.tensor(input_ids)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of tokenized data: 512\n",
      "Row with max length: 1\n"
     ]
    }
   ],
   "source": [
    "X = tokenizer(\n",
    "        text=X_train.tolist(),\n",
    "        add_special_tokens=True,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors='pt',\n",
    "        return_token_type_ids=False,\n",
    "        return_attention_mask=True,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "max_len = getMaxLen(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9649855c",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f463fd77-f389-476d-9377-a0b38cc1a050",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer(\n",
    "    text=X_train.tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=max_len,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors='pt',\n",
    "    return_token_type_ids=False,\n",
    "    return_attention_mask=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55e6d899-5c78-4fc0-b82d-4acd30001e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = tokenizer(\n",
    "    text=val_data['Text'].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=max_len,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors='pt',\n",
    "    return_token_type_ids=False,\n",
    "    return_attention_mask=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b11feae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tokenizer(\n",
    "    text=test_data['Text'].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=max_len,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors='pt',\n",
    "    return_token_type_ids=False,\n",
    "    return_attention_mask=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80aff597",
   "metadata": {},
   "source": [
    "Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba55e5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "lr = 2e-5 #5e-05\n",
    "batch_size = 8 #16\n",
    "patience = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa0b964",
   "metadata": {},
   "source": [
    "Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec18b579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([27835]), torch.Size([11124]), torch.Size([11194]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train = torch.LongTensor(Y_train.tolist())\n",
    "Y_val = torch.LongTensor(val_data[\"Labels\"].tolist())\n",
    "Y_test = torch.LongTensor(test_data[\"Labels\"].tolist())\n",
    "Y_train.size(), Y_val.size(), Y_test.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27897fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train[\"input_ids\"], X_train[\"attention_mask\"], Y_train)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "val_dataset = TensorDataset(X_val[\"input_ids\"], X_val[\"attention_mask\"], Y_val)\n",
    "val_sampler = SequentialSampler(val_dataset)\n",
    "val_dataloader = DataLoader(val_dataset, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "test_dataset = TensorDataset(X_test[\"input_ids\"], X_test[\"attention_mask\"], Y_test)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dde170da",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = lr, # default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # default is 1e-8.\n",
    "                  )\n",
    "\n",
    "max_steps = len(train_dataloader)*n_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "            num_warmup_steps=max_steps // 5,\n",
    "            num_training_steps=max_steps)\n",
    "\n",
    "loss_fun = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "580acc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_steps = len(train_dataloader) * n_epochs\n",
    "\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, # Default value in run_glue.py \n",
    "#                                             num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f8ac5a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0fc1a47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaForSequenceClassification(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): RobertaClassificationHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "No. of trainable parameters:  124647170\n"
     ]
    }
   ],
   "source": [
    "print(model.to(device))\n",
    "print(\"No. of trainable parameters: \", sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7059e551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(filename, epoch, model, optimizer, scheduler, train_loss_per_epoch, val_loss_per_epoch, train_f1_per_epoch, val_f1_per_epoch):\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'model': model,\n",
    "        'optimizer': optimizer,\n",
    "        'scheduler': scheduler,\n",
    "        'train_loss_per_epoch': train_loss_per_epoch,\n",
    "        'val_loss_per_epoch': val_loss_per_epoch,\n",
    "        'train_f1_per_epoch': train_f1_per_epoch,\n",
    "        'val_f1_per_epoch': val_f1_per_epoch\n",
    "        }\n",
    "    torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a4f51b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # we do not retrain our pre-trained BERT and train only the last linear dense layer\n",
    "# for param in model.bert_model.parameters():\n",
    "#     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fde3894",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029b82ba-e0b8-4973-8970-fc8d229d2db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize values for implementing Callbacks\n",
    "## Early Stopping\n",
    "best_val_f1 = -1\n",
    "best_epoch = -1\n",
    "no_improvement_counter = 0\n",
    "## Save best - optimal checkpointing\n",
    "checkpoint_dir = './checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "save_path = os.path.join(checkpoint_dir, 'best_weights.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f609ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch:  1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "742472ca2caa4807b435488571ef9388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/3480 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33bf96de51c0409f8a4833d6793548b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/1391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Train Loss: 0.2666 - Valid Loss: 0.0785\n",
      "Epoch 1/10 - Train F1: 0.7282 - Valid F1: 0.8670\n",
      "Model saved at epoch:  1\n",
      "Epoch:  2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fd5d93c98724df99d339dacf36db42d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/3480 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7656f3aba0645e78ce2e934b95ec321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/1391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Train Loss: 0.1534 - Valid Loss: 0.0607\n",
      "Epoch 2/10 - Train F1: 0.9072 - Valid F1: 0.8810\n",
      "Model saved at epoch:  2\n",
      "Epoch:  3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca6f5191e4654e39b995f08908599881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/3480 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0d7e2973fd34d1986e19a5740353b2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/1391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Train Loss: 0.1277 - Valid Loss: 0.0593\n",
      "Epoch 3/10 - Train F1: 0.9231 - Valid F1: 0.9064\n",
      "Model saved at epoch:  3\n",
      "Epoch:  4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "460ef375620247a7beb0414d3c6b665d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/3480 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f306ca0808534b6081570b51abdc9f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/1391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Train Loss: 0.1082 - Valid Loss: 0.0592\n",
      "Epoch 4/10 - Train F1: 0.9384 - Valid F1: 0.8904\n",
      "Epoch:  5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f44db01387142d4a0a09c99c55e7bbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/3480 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c5b19d4882d4f4288be4e859e8203b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/1391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Train Loss: 0.0869 - Valid Loss: 0.0645\n",
      "Epoch 5/10 - Train F1: 0.9506 - Valid F1: 0.9060\n",
      "Epoch:  6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87e0cfa6d52c4478902d35bf662991ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/3480 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Training...\")\n",
    "milli_sec1 = int(round(time.time() * 1000))\n",
    "\n",
    "train_loss_per_epoch = []\n",
    "val_loss_per_epoch = []\n",
    "train_f1_per_epoch = []\n",
    "val_f1_per_epoch = []\n",
    "\n",
    "for epoch_num in range(n_epochs):\n",
    "    print('Epoch: ', epoch_num + 1)\n",
    "    \n",
    "    #Training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    total_preds = []\n",
    "    total_labels = []\n",
    "    for step_num, batch_data in enumerate(tqdm(train_dataloader, desc='Training')):\n",
    "        \n",
    "        input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
    "                \n",
    "        # clear previously calculated gradients\n",
    "        model.zero_grad() # optimizer.zero_grad()\n",
    "        \n",
    "        # get model predictions for the current batch\n",
    "        output = model(input_ids = input_ids, attention_mask=att_mask) # , labels=labels\n",
    "        \n",
    "        # compute the loss between actual and predicted values\n",
    "        loss = loss_fun(output.logits, labels) #loss = output.loss #output[0]       \n",
    "        # add on to the total loss\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        clip_grad_norm_(parameters=model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Print training loss after each batch\n",
    "        #print(\"Epoch {}/{} - Batch {}/{} - Training Loss: {:.4f}\".format(epoch_num+1, n_epochs, step_num+1, len(train_dataloader), loss.item()))\n",
    "        \n",
    "        # model predictions are stored on GPU. So, push it to CPU\n",
    "        preds = np.argmax(output.logits.cpu().detach().numpy(),axis=-1)\n",
    "        # append the model predictions\n",
    "        total_preds+=list(preds)\n",
    "        total_labels+=labels.cpu().numpy().tolist()\n",
    "        \n",
    "    train_loss_per_epoch.append(train_loss / len(train_dataloader))    \n",
    "    train_accuracy=accuracy_score(total_labels, total_preds)\n",
    "    if n_categories > 2:\n",
    "        train_precision=precision_score(total_labels, total_preds, average='macro')\n",
    "        train_recall=recall_score(total_labels, total_preds, average='macro')\n",
    "        train_f1=f1_score(total_labels, total_preds, average='macro')\n",
    "    else:\n",
    "        train_precision=precision_score(total_labels, total_preds)\n",
    "        train_recall=recall_score(total_labels, total_preds)\n",
    "        train_f1=f1_score(total_labels, total_preds)\n",
    "        train_roc_auc=roc_auc_score(total_labels, total_preds)\n",
    "    train_f2 = (5*train_precision*train_recall) / (4*train_precision+train_recall)\n",
    "\n",
    "    #Validation\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    valid_pred = []\n",
    "    actual_labels = []\n",
    "    with torch.no_grad():\n",
    "        for step_num_e, batch_data in enumerate(tqdm(val_dataloader, desc='Validation')):\n",
    "            input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
    "            \n",
    "            output = model(input_ids = input_ids, attention_mask=att_mask) # , labels=labels\n",
    "            \n",
    "            preds = np.argmax(output.logits.cpu().detach().numpy(), axis=-1)\n",
    "            valid_pred+=list(preds)\n",
    "            actual_labels+=labels.cpu().numpy().tolist()\n",
    "\n",
    "            loss = loss_fun(output.logits, labels) #loss = output.loss #output[0]\n",
    "            valid_loss += loss.item()\n",
    "        \n",
    "    val_loss_per_epoch.append(valid_loss / len(val_dataloader))    \n",
    "    val_accuracy=accuracy_score(actual_labels, valid_pred)\n",
    "    if n_categories > 2:\n",
    "        val_precision=precision_score(actual_labels, valid_pred, average='macro')\n",
    "        val_recall=recall_score(actual_labels, valid_pred, average='macro')\n",
    "        val_f1=f1_score(actual_labels, valid_pred, average='macro')\n",
    "    else:\n",
    "        val_precision=precision_score(actual_labels, valid_pred)\n",
    "        val_recall=recall_score(actual_labels, valid_pred)\n",
    "        val_f1=f1_score(actual_labels, valid_pred)\n",
    "        val_roc_auc=roc_auc_score(actual_labels, valid_pred)\n",
    "    val_f2 = (5*val_precision*val_recall) / (4*val_precision+val_recall)\n",
    "    \n",
    "    print(\"Epoch {}/{} - Train Loss: {:.4f} - Valid Loss: {:.4f}\".format(epoch_num+1, n_epochs, train_loss_per_epoch[-1], val_loss_per_epoch[-1]))\n",
    "    print(\"Epoch {}/{} - Train F1: {:.4f} - Valid F1: {:.4f}\".format(epoch_num+1, n_epochs, train_f1, val_f1))\n",
    "    \n",
    "    train_f1_per_epoch.append(train_f1)\n",
    "    val_f1_per_epoch.append(val_f1)\n",
    "\n",
    "    total_epochs = epoch_num + 1\n",
    "    # Implement Callbacks: Early Stopping and save best\n",
    "    # Check if the validation F1 score has improved\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        best_epoch = epoch_num + 1\n",
    "        no_improvement_counter = 0 # Reset the counter\n",
    "        \n",
    "        # Save the best model checkpoint\n",
    "        save_checkpoint(save_path, epoch_num+1, model.state_dict(), optimizer.state_dict(), scheduler.state_dict(), train_loss_per_epoch, val_loss_per_epoch, train_f1_per_epoch, val_f1_per_epoch)\n",
    "        print(\"Model saved at epoch: \", epoch_num+1)\n",
    "    else:\n",
    "        no_improvement_counter += 1\n",
    "        \n",
    "        if no_improvement_counter >= patience:\n",
    "            print(\"No improvement for\", patience, \"consecutive epochs.\")\n",
    "            print(\"Early stopping after epoch No.\", total_epochs)\n",
    "            print(\"Best model after epoch No\", best_epoch)\n",
    "            print(\"Best achieved val_f1 = \", best_val_f1)\n",
    "            break\n",
    "\n",
    "milli_sec2 = int(round(time.time() * 1000))\n",
    "print(\"Training is completed after\", milli_sec2-milli_sec1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dcae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, total_epochs + 1)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(epochs, train_loss_per_epoch, label ='training loss')\n",
    "ax.plot(epochs, val_loss_per_epoch, label = 'validation loss' )\n",
    "ax.set_title('Training and Validation loss')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a54858",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, total_epochs + 1)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(epochs, train_f1_per_epoch,label ='training F1-score')\n",
    "ax.plot(epochs, val_f1_per_epoch, label = 'validation F1-score')\n",
    "ax.set_title('Training and Validation F1-scores')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('F1-score')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53379dac",
   "metadata": {},
   "source": [
    "Load best model from checkpoint during training with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1534cd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(save_path, map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90781fe",
   "metadata": {},
   "source": [
    "Make predictions on the testing set and compute evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c019f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_pred = []\n",
    "actual_labels = []\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for step_num, batch_data in enumerate(tqdm(test_dataloader, desc='Testing')):\n",
    "        input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
    "        \n",
    "        output = model(input_ids = input_ids, attention_mask=att_mask) #, labels= labels\n",
    "\n",
    "        loss = loss_fun(output.logits, labels) #loss = output.loss #output[0]\n",
    "        test_loss += loss.item()\n",
    "   \n",
    "        preds = np.argmax(output.logits.cpu().detach().numpy(), axis=-1)\n",
    "        test_pred+=list(preds)\n",
    "        actual_labels+=labels.cpu().numpy().tolist()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c1d9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_report = classification_report(actual_labels, test_pred)\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "total_test_loss = test_loss/len(test_dataloader) \n",
    "accuracy=accuracy_score(actual_labels, test_pred)\n",
    "if n_categories > 2:\n",
    "    precision=precision_score(actual_labels, test_pred, average='macro')\n",
    "    recall=recall_score(actual_labels, test_pred, average='macro')\n",
    "    f1=f1_score(actual_labels, test_pred, average='macro')\n",
    "else:\n",
    "    precision=precision_score(actual_labels, test_pred)\n",
    "    recall=recall_score(actual_labels, test_pred)\n",
    "    f1=f1_score(actual_labels, test_pred)\n",
    "    roc_auc=roc_auc_score(actual_labels, test_pred)\n",
    "f2 = (5*precision*recall) / (4*precision+recall)\n",
    "\n",
    "print(\"Accuracy:%.2f%%\"%(accuracy*100))\n",
    "print(\"Precision:%.2f%%\"%(precision*100))\n",
    "print(\"Recall:%.2f%%\"%(recall*100))\n",
    "print(\"F1 score:%.2f%%\"%(f1*100))\n",
    "print(\"F2 score:%.2f%%\"%(f2*100))\n",
    "if roc_auc:\n",
    "    print(\"Roc_Auc score:%.2f%%\"%(roc_auc*100))\n",
    "\n",
    "conf_matrix = confusion_matrix(actual_labels, test_pred)\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "#acc = ((tp+tn)/(tp+tn+fp+fn))\n",
    "\n",
    "print(\"TP=\",tp)\n",
    "print(\"TN=\",tn)\n",
    "print(\"FP=\",fp)\n",
    "print(\"FN=\",fn)\n",
    "#print(conf_matrix)\n",
    "sn.heatmap(conf_matrix, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ae8903",
   "metadata": {},
   "source": [
    "Export classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fb6606",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"forSequence\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c611ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the path\n",
    "path = os.path.join(root_path, 'results', model_variation.split(\"/\")[-1], method, str(seed))\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# Define the CSV file path\n",
    "csv_file_path = os.path.join(path, f\"{seed}.csv\")\n",
    "\n",
    "# Write data to CSV\n",
    "data = {\n",
    "    \"accuracy\": accuracy,\n",
    "    \"precision\": precision,\n",
    "    \"recall\": recall,\n",
    "    \"f1\": f1,\n",
    "    \"f2\": f2,\n",
    "    \"roc_auc\": roc_auc\n",
    "}\n",
    "\n",
    "# Write to CSV\n",
    "with open(csv_file_path, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=data.keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerow(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bef406a",
   "metadata": {},
   "source": [
    "Compute the average values of the classication metrics considering the results for all different seeders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6159eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary to store cumulative sum of metrics\n",
    "cumulative_metrics = defaultdict(float)\n",
    "count = 0  # Counter to keep track of number of CSV files\n",
    "\n",
    "# Iterate over all CSV files in the results folder\n",
    "results_folder = os.path.join(root_path, \"results\", model_variation.split(\"/\")[-1], method, str(seed))\n",
    "for filename in os.listdir(results_folder):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        csv_file_path = os.path.join(results_folder, filename)\n",
    "        with open(csv_file_path, \"r\", newline=\"\") as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                for metric, value in row.items():\n",
    "                    cumulative_metrics[metric] += float(value)\n",
    "        count += 1\n",
    "        \n",
    "# Compute average values\n",
    "average_metrics = {metric: total / count for metric, total in cumulative_metrics.items()}\n",
    "\n",
    "# Print average values \n",
    "print(average_metrics)\n",
    "\n",
    "# Define the path for the average CSV file\n",
    "avg_csv_file_path = os.path.join(root_path, \"results\", model_variation.split(\"/\")[-1], method, \"avg.csv\")\n",
    "\n",
    "# Write average metrics to CSV\n",
    "with open(avg_csv_file_path, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=average_metrics.keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerow(average_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6178575-9d0b-4ccf-a500-95cc8b80e954",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
