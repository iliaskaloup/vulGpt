{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e260120",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05a7b4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import json, os\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalMaxPool1D\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification #, BertModel, BertTokenizer, TFBertForSequenceClassification\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, \\\n",
    "roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7906322c",
   "metadata": {},
   "source": [
    "Specify a constant seeder for processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "663f78d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f400c5c2",
   "metadata": {},
   "source": [
    "Pre-trained tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c7aa5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_variation = \"microsoft/codebert-base-mlm\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_variation, do_lower_case=True) #Tokenizer\n",
    "#bert-base-uncased #bert-base # roberta-base # distilbert-base-uncased #distilbert-base # microsoft/codebert-base-mlm\n",
    "# 'albert-base-v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffcb9b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define New tokens for string and numerical i.e., strId$ and numId$\n",
    "new_tokens = [\"strId$\", \"numId$\"]\n",
    "for new_token in new_tokens:\n",
    "    if new_token not in tokenizer.get_vocab().keys():\n",
    "        tokenizer.add_tokens(new_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb9b166d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_metric(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = (true_positives + K.epsilon()) / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_metric(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = (true_positives + K.epsilon()) / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_metric(y_true, y_pred):\n",
    "\n",
    "    prec = precision_metric(y_true, y_pred)\n",
    "    rec = recall_metric(y_true, y_pred)\n",
    "    f1 = 2*((prec*rec)/(prec+rec+K.epsilon()))\n",
    "    return f1\n",
    "\n",
    "def f2_metric(y_true, y_pred):\n",
    "\n",
    "    prec = precision_metric(y_true, y_pred)\n",
    "    rec = recall_metric(y_true, y_pred)\n",
    "    f2 = 5*((prec*rec)/(4*prec+rec+K.epsilon()))\n",
    "    return f2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c9fbd4",
   "metadata": {},
   "source": [
    "Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec1d5a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = os.path.join('..', '..', '..')\n",
    "data = pd.read_csv(os.path.join(root_path, 'data', 'dataset.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9494e495",
   "metadata": {},
   "source": [
    "Shuffle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "654d03e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                func  vul  length\n",
      "0  static enum test_return test_binary_replace(vo...    0       9\n",
      "1  force_unmount (device *device,\\r\\n            ...    0      84\n",
      "2  bool clipboard::isformatavailablebystring(cons...    0      12\n",
      "3  int kvm_get_dirty_log(struct kvm *kvm,\\r\\n\\t\\t...    0      81\n",
      "4  static double filter_mitchell(const double x)\\...    0     124\n",
      "178460\n"
     ]
    }
   ],
   "source": [
    "data = data.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "print(data.head())\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac048926",
   "metadata": {},
   "source": [
    "Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac7f783f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of words: 191\n"
     ]
    }
   ],
   "source": [
    "word_counts = data[\"func\"].apply(lambda x: len(x.split()))\n",
    "max_length = word_counts.max()\n",
    "print(\"Maximum number of words:\", max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9388c53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    170642\n",
      "1      7818\n",
      "Name: vul, dtype: int64\n",
      "Percentage:  4.58152154803624 %\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "vc = data[\"vul\"].value_counts()\n",
    "\n",
    "print(vc)\n",
    "\n",
    "print(\"Percentage: \", (vc[1] / vc[0])*100, '%')\n",
    "\n",
    "n_categories = len(vc)\n",
    "print(n_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f80abd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>static enum test_return test_binary_replace(vo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>force_unmount (device *device,\\r\\n            ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bool clipboard::isformatavailablebystring(cons...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>int kvm_get_dirty_log(struct kvm *kvm,\\r\\n\\t\\t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>static double filter_mitchell(const double x)\\...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Labels\n",
       "0  static enum test_return test_binary_replace(vo...       0\n",
       "1  force_unmount (device *device,\\r\\n            ...       0\n",
       "2  bool clipboard::isformatavailablebystring(cons...       0\n",
       "3  int kvm_get_dirty_log(struct kvm *kvm,\\r\\n\\t\\t...       0\n",
       "4  static double filter_mitchell(const double x)\\...       0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(({'Text': data['func'], 'Labels': data['vul']}))\n",
    "#data = data[0:100]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b71280",
   "metadata": {},
   "source": [
    "Split to train-val-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a08c607",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ratio = 0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1ac30ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_seeders = [seed, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "shuffle_seeder = shuffle_seeders[0]\n",
    "\n",
    "train_val_data, test_data = train_test_split(data, test_size=val_ratio, random_state=shuffle_seeder, stratify=data['Labels'])\n",
    "train_data, val_data = train_test_split(train_val_data, test_size=val_ratio, random_state=shuffle_seeder, stratify=train_val_data['Labels'])\n",
    "# print(len(data))\n",
    "# print(len(train_val_data))\n",
    "# print(len(test_data))\n",
    "# print(len(train_data))\n",
    "# print(len(val_data))\n",
    "# print(len(val_data)+len(train_data)+len(test_data))\n",
    "# print(len(val_data)+len(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261920e1",
   "metadata": {},
   "source": [
    "Pre-processing step: Under-sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c82187a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution  0    138220\n",
      "1      6332\n",
      "Name: Labels, dtype: int64\n",
      "Majority class  0\n",
      "Minority class  1\n",
      "Targeted number of majority class 6332\n",
      "Class distribution after augmentation 0    6332\n",
      "1    6332\n",
      "Name: Labels, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "sampling = True\n",
    "if n_categories == 2 and sampling == True:\n",
    "    # Apply under-sampling with the specified strategy\n",
    "    class_counts = pd.Series(train_data[\"Labels\"]).value_counts()\n",
    "    print(\"Class distribution \", class_counts)\n",
    "\n",
    "    majority_class = class_counts.idxmax()\n",
    "    print(\"Majority class \", majority_class)\n",
    "\n",
    "    minority_class = class_counts.idxmin()\n",
    "    print(\"Minority class \", minority_class)\n",
    "\n",
    "    target_count = class_counts[class_counts.idxmin()] # int(class_counts.iloc[0] / 2) \n",
    "    print(\"Targeted number of majority class\", target_count)\n",
    "\n",
    "    # under\n",
    "    sampling_strategy = {majority_class: target_count}        \n",
    "    rus = RandomUnderSampler(random_state=seed, sampling_strategy=sampling_strategy)\n",
    "\n",
    "    x_train_resampled, y_train_resampled = rus.fit_resample(np.array(train_data[\"Text\"]).reshape(-1, 1), train_data[\"Labels\"]) \n",
    "    print(\"Class distribution after augmentation\", pd.Series(y_train_resampled).value_counts())\n",
    "\n",
    "\n",
    "    # Shuffle the resampled data while preserving the correspondence between features and labels\n",
    "    x_train_resampled, y_train_resampled = shuffle(x_train_resampled, y_train_resampled, random_state=seed)\n",
    "\n",
    "    # rename\n",
    "    X_train = x_train_resampled\n",
    "    Y_train = y_train_resampled\n",
    "\n",
    "    X_train = pd.Series(X_train.reshape(-1))\n",
    "\n",
    "else:\n",
    "    X_train = train_data[\"Text\"]\n",
    "    Y_train = train_data[\"Labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914a2fe8",
   "metadata": {},
   "source": [
    "Pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af1a5dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base-mlm and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': None, 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'chunk_size_feed_forward': 0, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['RobertaForMaskedLM'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': 0, 'pad_token_id': 1, 'eos_token_id': 2, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'microsoft/codebert-base-mlm', 'transformers_version': '4.26.1', 'model_type': 'roberta', 'output_past': True, 'vocab_size': 50265, 'hidden_size': 768, 'num_hidden_layers': 12, 'num_attention_heads': 12, 'hidden_act': 'gelu', 'intermediate_size': 3072, 'hidden_dropout_prob': 0.1, 'attention_probs_dropout_prob': 0.1, 'max_position_embeddings': 514, 'type_vocab_size': 1, 'initializer_range': 0.02, 'layer_norm_eps': 1e-05, 'position_embedding_type': 'absolute', 'use_cache': True, 'classifier_dropout': None}\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_variation, num_labels=n_categories)\n",
    "\n",
    "config = model.get_config()\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be5f9eb",
   "metadata": {},
   "source": [
    "Resize model embedding to match new tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb3719d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.models.roberta.modeling_tf_roberta.TFRobertaEmbeddings at 0x1c0b2f0b3d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b154f789",
   "metadata": {},
   "source": [
    "Decide which pre-trained layers to freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc32725",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902631b8",
   "metadata": {},
   "source": [
    "Print model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc194e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cade1df7",
   "metadata": {},
   "source": [
    "Compute maximum length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5a623f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMaxLen(X):\n",
    "\n",
    "    # Code for identifying max length of the data samples after tokenization using transformer tokenizer\n",
    "    \n",
    "    max_length = 0\n",
    "    # Iterate over each sample in your dataset\n",
    "    for i, input_ids in enumerate(X['input_ids']):\n",
    "        # Calculate the length of the tokenized sequence for the current sample\n",
    "        length = tf.math.reduce_sum(tf.cast(input_ids != 1, tf.int32)).numpy()\n",
    "        # Update max_length and max_row if the current length is greater\n",
    "        if length > max_length:\n",
    "            max_length = length\n",
    "            max_row = i\n",
    "\n",
    "    print(\"Max length of tokenized data:\", max_length)\n",
    "    print(\"Row with max length:\", max_row)\n",
    "\n",
    "    #X['input_ids'] = np.delete(X['input_ids'], max_row, axis=0)\n",
    "    \n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b2b2c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of tokenized data: 512\n",
      "Row with max length: 10\n"
     ]
    }
   ],
   "source": [
    "X = tokenizer(\n",
    "        text=X_train.tolist(),\n",
    "        add_special_tokens=True,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors='tf',\n",
    "        return_token_type_ids=False,\n",
    "        return_attention_mask=True,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "max_len = getMaxLen(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69398f9b",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68b48ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer(\n",
    "    text=X_train.tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=max_len,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids=False,\n",
    "    return_attention_mask=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "X_val = tokenizer(\n",
    "    text=val_data['Text'].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=max_len,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids=False,\n",
    "    return_attention_mask=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "X_test = tokenizer(\n",
    "    text=test_data['Text'].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=max_len,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids=False,\n",
    "    return_attention_mask=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee16fc9",
   "metadata": {},
   "source": [
    "Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f7663f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 6\n",
    "batch_size = 6\n",
    "lr = 5e-05\n",
    "patience = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49483ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(\n",
    "    learning_rate=lr, # HF recommendation\n",
    "    epsilon=1e-08,\n",
    "    decay=0.01,\n",
    "    clipnorm=1.0\n",
    ")\n",
    "\n",
    "loss = CategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f3c589",
   "metadata": {},
   "source": [
    "Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08dd3306",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    metrics=[f1_metric]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429732da",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44e7edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch 1/6\n",
      "565/565 [==============================] - ETA: 0s - loss: 0.3591 - f1_metric: 0.8088"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, dense_layer_call_fn while saving (showing 5 of 422). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoints\\best_weights\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoints\\best_weights\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565/565 [==============================] - 352s 595ms/step - loss: 0.3591 - f1_metric: 0.8088 - val_loss: 0.2522 - val_f1_metric: 0.8695\n",
      "Epoch 2/6\n",
      "565/565 [==============================] - ETA: 0s - loss: 0.2001 - f1_metric: 0.9014"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, dense_layer_call_fn while saving (showing 5 of 422). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoints\\best_weights\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoints\\best_weights\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565/565 [==============================] - 335s 593ms/step - loss: 0.2001 - f1_metric: 0.9014 - val_loss: 0.2536 - val_f1_metric: 0.8804\n",
      "Epoch 3/6\n",
      "565/565 [==============================] - ETA: 0s - loss: 0.1408 - f1_metric: 0.9370"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, dense_layer_call_fn while saving (showing 5 of 422). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoints\\best_weights\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoints\\best_weights\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565/565 [==============================] - 334s 592ms/step - loss: 0.1408 - f1_metric: 0.9370 - val_loss: 0.2874 - val_f1_metric: 0.8918\n",
      "Epoch 4/6\n",
      "565/565 [==============================] - ETA: 0s - loss: 0.1013 - f1_metric: 0.9589"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, dense_layer_call_fn while saving (showing 5 of 422). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoints\\best_weights\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoints\\best_weights\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565/565 [==============================] - 336s 594ms/step - loss: 0.1013 - f1_metric: 0.9589 - val_loss: 0.3374 - val_f1_metric: 0.9016\n",
      "Epoch 5/6\n",
      "565/565 [==============================] - 312s 552ms/step - loss: 0.0799 - f1_metric: 0.9682 - val_loss: 0.3743 - val_f1_metric: 0.8993\n",
      "Epoch 6/6\n",
      "283/565 [==============>...............] - ETA: 2:30 - loss: 0.0591 - f1_metric: 0.9769"
     ]
    }
   ],
   "source": [
    "print(\"Training...\")\n",
    "milli_sec1 = int(round(time.time() * 1000))\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_f1_metric', mode='max', patience=patience)\n",
    "model_checkpoint = ModelCheckpoint('./checkpoints/best_weights', monitor='val_f1_metric', mode='max', save_best_only=True)\n",
    "\n",
    "history = model.fit(\n",
    "    x = {'input_ids':X_train['input_ids'], 'attention_mask':X_train['attention_mask']},\n",
    "    y = to_categorical(Y_train),\n",
    "    validation_data = ({'input_ids':X_val['input_ids'], 'attention_mask':X_val['attention_mask']},\n",
    "                        to_categorical(val_data['Labels'])),\n",
    "    epochs=n_epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")\n",
    "\n",
    "milli_sec2 = int(round(time.time() * 1000))\n",
    "print(\"Training is completed after\", milli_sec2-milli_sec1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a795c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['f1_metric'])\n",
    "plt.plot(history.history['val_f1_metric'])\n",
    "plt.ylabel('model f1_metric')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='best')\n",
    "#plt.savefig('train_history.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b16381",
   "metadata": {},
   "source": [
    "Load best model from checkpoint during training with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856c66f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./checkpoints/best_weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e619626",
   "metadata": {},
   "source": [
    "Classification report on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a838fda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(val_data['Labels'], np.argmax(model.predict({'input_ids': X_val['input_ids'], 'attention_mask': X_val['attention_mask']}).logits, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d4773f",
   "metadata": {},
   "source": [
    "Make predictions on the testing set and compute evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f164ddc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict({'input_ids': X_test['input_ids'], 'attention_mask': X_test['attention_mask']}).logits\n",
    "y_predicted = np.argmax(predicted, axis=1)\n",
    "\n",
    "targets = test_data['Labels']\n",
    "print(classification_report(targets, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a8c40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=accuracy_score(targets, y_predicted)\n",
    "precision=precision_score(targets, y_predicted)\n",
    "recall=recall_score(targets, y_predicted)\n",
    "roc_auc=roc_auc_score(targets, y_predicted)\n",
    "f1=f1_score(targets, y_predicted)\n",
    "f2 = (5*precision*recall) / (4*precision+recall)\n",
    "\n",
    "conf_matrix = confusion_matrix(targets, y_predicted)\n",
    "sn.heatmap(conf_matrix, annot=True)\n",
    "\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "acc = ((tp+tn)/(tp+tn+fp+fn))\n",
    "print(\"TP=\",tp)\n",
    "print(\"TN=\",tn)\n",
    "print(\"FP=\",fp)\n",
    "print(\"FN=\",fn)\n",
    "\n",
    "print(\"Accuracy:%.2f%%\"%(accuracy*100))\n",
    "print(\"Precision:%.2f%%\"%(precision*100))\n",
    "print(\"Recall:%.2f%%\"%(recall*100))\n",
    "print(\"Roc_Auc score:%.2f%%\"%(roc_auc*100))\n",
    "print(\"F1 score:%.2f%%\"%(f1*100))\n",
    "print(\"F2 score:%.2f%%\"%(f2*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b7c35c",
   "metadata": {},
   "source": [
    "Export classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44086d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"forSequence\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9522a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the path\n",
    "path = os.path.join(root_path, 'results', model_variation.split(\"/\")[-1], method, str(shuffle_seeder))\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# Define the CSV file path\n",
    "csv_file_path = os.path.join(path, f\"{shuffle_seeder}.csv\")\n",
    "\n",
    "# Write data to CSV\n",
    "data = {\n",
    "    \"accuracy\": accuracy,\n",
    "    \"precision\": precision,\n",
    "    \"recall\": recall,\n",
    "    \"f1\": f1,\n",
    "    \"f2\": f2,\n",
    "    \"roc_auc\": roc_auc\n",
    "}\n",
    "\n",
    "# Write to CSV\n",
    "with open(csv_file_path, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=data.keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerow(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539bf67e",
   "metadata": {},
   "source": [
    "Compute the average values of the classication metrics considering the results for all different seeders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cfde15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary to store cumulative sum of metrics\n",
    "cumulative_metrics = defaultdict(float)\n",
    "count = 0  # Counter to keep track of number of CSV files\n",
    "\n",
    "# Iterate over all CSV files in the results folder\n",
    "results_folder = os.path.join(root_path, \"results\", model_variation.split(\"/\")[-1], method, str(shuffle_seeder))\n",
    "for filename in os.listdir(results_folder):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        csv_file_path = os.path.join(results_folder, filename)\n",
    "        with open(csv_file_path, \"r\", newline=\"\") as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                for metric, value in row.items():\n",
    "                    cumulative_metrics[metric] += float(value)\n",
    "        count += 1\n",
    "        \n",
    "# Compute average values\n",
    "average_metrics = {metric: total / count for metric, total in cumulative_metrics.items()}\n",
    "\n",
    "# Print average values \n",
    "print(average_metrics)\n",
    "\n",
    "# Define the path for the average CSV file\n",
    "avg_csv_file_path = os.path.join(root_path, \"results\", model_variation.split(\"/\")[-1], method, \"avg.csv\")\n",
    "\n",
    "# Write average metrics to CSV\n",
    "with open(avg_csv_file_path, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=average_metrics.keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerow(average_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
