{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3354fc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iliaskaloup\\AppData\\Local\\anaconda3\\envs\\tfenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import json, os\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "# from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "# from torch.optim import Adam\n",
    "# from transformers import get_linear_schedule_with_warmup\n",
    "# from torch.nn.utils import clip_grad_norm_\n",
    "# from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModel, TFGPT2LMHeadModel\n",
    "from transformers import set_seed\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import LSTM, SimpleRNN\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import Masking\n",
    "from tensorflow.keras.layers import Embedding, MaxPool1D\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.layers import Bidirectional, BatchNormalization\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.initializers import glorot_uniform, RandomUniform, lecun_uniform, Constant, TruncatedNormal\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D, GlobalMaxPool1D, Flatten\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalMaxPool1D\n",
    "from keras_preprocessing.text import tokenizer_from_json\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, \\\n",
    "roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from collections import defaultdict\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbfd651",
   "metadata": {},
   "source": [
    "Define method name and root path of the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74683969",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"forSequence\"\n",
    "\n",
    "root_path = os.path.join('..', '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "021a956c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_algorithm = \"bert\" # \"bert\" # \"gpt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48806676",
   "metadata": {},
   "source": [
    "Define specific seeder for all experiments and processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e3172d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "#torch.manual_seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b0f01a",
   "metadata": {},
   "source": [
    "Read data and shuffle them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29764f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                func  vul\n",
      "0  static int ipip_rcv(struct sk_buff *skb)\\n{\\n\\...    0\n",
      "1  bool LayerTreeHostImpl::IsUIResourceOpaque(UIR...    0\n",
      "2  error::Error GLES2DecoderPassthroughImpl::DoGe...    0\n",
      "3  void DocumentLoader::NotifyFinished(Resource* ...    0\n",
      "4  void conn_free(conn *c) {\\n    if (c) {\\n     ...    0\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(os.path.join(root_path, 'data', 'dataset.csv'))\n",
    "data = data.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbbb0cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of words: 15441\n"
     ]
    }
   ],
   "source": [
    "word_counts = data[\"func\"].apply(lambda x: len(x.split()))\n",
    "max_length = word_counts.max()\n",
    "print(\"Maximum number of words:\", max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63804820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vul\n",
      "0    188636\n",
      "1     10900\n",
      "Name: count, dtype: int64\n",
      "Percentage:  5.778324391950635 %\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "vc = data[\"vul\"].value_counts()\n",
    "\n",
    "print(vc)\n",
    "\n",
    "print(\"Percentage: \", (vc[1] / vc[0])*100, '%')\n",
    "\n",
    "n_categories = len(vc)\n",
    "print(n_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2405b1c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>static int ipip_rcv(struct sk_buff *skb)\\n{\\n\\...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bool LayerTreeHostImpl::IsUIResourceOpaque(UIR...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>error::Error GLES2DecoderPassthroughImpl::DoGe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>void DocumentLoader::NotifyFinished(Resource* ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>void conn_free(conn *c) {\\n    if (c) {\\n     ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  static int ipip_rcv(struct sk_buff *skb)\\n{\\n\\...      0\n",
       "1  bool LayerTreeHostImpl::IsUIResourceOpaque(UIR...      0\n",
       "2  error::Error GLES2DecoderPassthroughImpl::DoGe...      0\n",
       "3  void DocumentLoader::NotifyFinished(Resource* ...      0\n",
       "4  void conn_free(conn *c) {\\n    if (c) {\\n     ...      0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(({'text': data['func'], 'label': data['vul']}))\n",
    "#data = data[0:100]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bbec25",
   "metadata": {},
   "source": [
    "Train test split with seeder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "928f2aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ratio = 0.10\n",
    "\n",
    "#split to train-val-test\n",
    "# split dataset to train-test sets\n",
    "### split data into train and test (90% train, 10% test)\n",
    "shuffle_seeders = [seed, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "shuffle_seeder = shuffle_seeders[0]\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=val_ratio, random_state=shuffle_seeder, stratify=data['label'])\n",
    "# print(len(data))\n",
    "# print(len(train_data))\n",
    "# print(len(test_data))\n",
    "# print(len(test_data)+len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d36c93f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = train_test_split(train_data, test_size=val_ratio, random_state=shuffle_seeder, stratify=train_data['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37254222",
   "metadata": {},
   "source": [
    "Pre-processing step: Under-sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "782fb4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution  label\n",
      "0    152794\n",
      "1      8829\n",
      "Name: count, dtype: int64\n",
      "Majority class  0\n",
      "Minority class  1\n",
      "Targeted number of majority class 8829\n",
      "Class distribution after augmentation label\n",
      "0    8829\n",
      "1    8829\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "sampling = True\n",
    "if n_categories == 2 and sampling == True:\n",
    "    # Apply under-sampling with the specified strategy\n",
    "    class_counts = pd.Series(train_data[\"label\"]).value_counts()\n",
    "    print(\"Class distribution \", class_counts)\n",
    "\n",
    "    majority_class = class_counts.idxmax()\n",
    "    print(\"Majority class \", majority_class)\n",
    "\n",
    "    minority_class = class_counts.idxmin()\n",
    "    print(\"Minority class \", minority_class)\n",
    "\n",
    "    target_count = class_counts[class_counts.idxmin()] # int(class_counts.iloc[0] / 2) \n",
    "    print(\"Targeted number of majority class\", target_count)\n",
    "\n",
    "    # under\n",
    "    sampling_strategy = {majority_class: target_count}        \n",
    "    rus = RandomUnderSampler(random_state=seed, sampling_strategy=sampling_strategy)\n",
    "\n",
    "    x_train_resampled, y_train_resampled = rus.fit_resample(np.array(train_data[\"text\"]).reshape(-1, 1), train_data[\"label\"]) \n",
    "    print(\"Class distribution after augmentation\", pd.Series(y_train_resampled).value_counts())\n",
    "\n",
    "\n",
    "    # Shuffle the resampled data while preserving the correspondence between features and labels\n",
    "    x_train_resampled, y_train_resampled = shuffle(x_train_resampled, y_train_resampled, random_state=seed)\n",
    "\n",
    "    # rename\n",
    "    X_train = x_train_resampled\n",
    "    Y_train = y_train_resampled\n",
    "\n",
    "    X_train = pd.Series(X_train.reshape(-1))\n",
    "\n",
    "else:\n",
    "    X_train = train_data[\"text\"]\n",
    "    Y_train = train_data[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86325363",
   "metadata": {},
   "source": [
    "Choose transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cb1bc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# microsoft/codebert-base-mlm # microsoft/codebert-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "772f881d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PYTORCH\n",
    "# if embedding_algorithm == \"bert\":\n",
    "#     model_variation = \"microsoft/codebert-base-mlm\"\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_variation, do_lower_case=True) #Tokenizer\n",
    "#     #bert-base-uncased #bert-base #albert-base-v2 # roberta-base # distilbert-base-uncased #distilbert-base \n",
    "#     # Define New tokens for string and numerical i.e., strId$ and numId$\n",
    "#     new_tokens = [\"strId$\", \"numId$\"]\n",
    "#     for new_token in new_tokens:\n",
    "#         if new_token not in tokenizer.get_vocab().keys():\n",
    "#             tokenizer.add_tokens(new_token)\n",
    "            \n",
    "#     bert = AutoModel.from_pretrained(model_variation, num_labels=n_categories)\n",
    "\n",
    "#     bert.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "#     embedding_matrix = bert.embeddings.word_embeddings.weight.detach().cpu().numpy()\n",
    "    \n",
    "#     num_words = len(embedding_matrix)\n",
    "#     print(num_words)\n",
    "#     dim = len(embedding_matrix[0])\n",
    "#     print(dim)\n",
    "    \n",
    "#     sentences = X_train.tolist()\n",
    "#     sequences = [tokenizer.encode(sente, truncation=True, add_special_tokens=False, return_tensors=\"pt\").numpy() for sente in sentences] # Tokenize the complete sentences\n",
    "\n",
    "#     lines_pad_x_train = []\n",
    "#     for seq in sequences:\n",
    "#         lines_pad_x_train.append(torch.tensor(seq[0]))\n",
    "    \n",
    "#     lines_pad_x_train = pad_sequence(lines_pad_x_train, batch_first=True, padding_value=0)\n",
    "#     max_len = lines_pad_x_train.size()[1]\n",
    "    \n",
    "    \n",
    "#     sentences = val_data[\"Input\"]\n",
    "#     sequences = [tokenizer.encode(sente, truncation=True, add_special_tokens=False, return_tensors=\"pt\").numpy() for sente in sentences]\n",
    "#     lines_pad_x_val = []\n",
    "#     for seq in sequences:\n",
    "#         lines_pad_x_val.append(torch.tensor(seq[0]))\n",
    "#     lines_pad_x_val = pad_sequence(lines_pad_x_val, batch_first=True, padding_value=0)\n",
    "    \n",
    "#     sentences = test_data[\"Input\"]\n",
    "#     sequences = [tokenizer.encode(sente, truncation=True, add_special_tokens=False, return_tensors=\"pt\").numpy() for sente in sentences]\n",
    "#     lines_pad_x_test = []\n",
    "#     for seq in sequences:\n",
    "#         lines_pad_x_test.append(torch.tensor(seq[0]))\n",
    "#     lines_pad_x_test = pad_sequence(lines_pad_x_test, batch_first=True, padding_value=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3eac4a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMaxLen(X):\n",
    "\n",
    "    # Code for identifying max length of the data samples after tokenization using transformer tokenizer\n",
    "    \n",
    "    max_length = 0\n",
    "    # Iterate over each sample in your dataset\n",
    "    for i, input_ids in enumerate(X['input_ids']):\n",
    "        # Calculate the length of the tokenized sequence for the current sample\n",
    "        length = tf.math.reduce_sum(tf.cast(input_ids != 1, tf.int32)).numpy()\n",
    "        # Update max_length and max_row if the current length is greater\n",
    "        if length > max_length:\n",
    "            max_length = length\n",
    "            max_row = i\n",
    "\n",
    "    print(\"Max length of tokenized data:\", max_length)\n",
    "    print(\"Row with max length:\", max_row)\n",
    "\n",
    "    #X['input_ids'] = np.delete(X['input_ids'], max_row, axis=0)\n",
    "    \n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a38906f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at microsoft/codebert-base-mlm were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base-mlm.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50265\n",
      "768\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "# TENSORFLOW\n",
    "if embedding_algorithm == \"bert\":\n",
    "    model_variation = \"microsoft/codebert-base-mlm\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_variation, do_lower_case=True) #Tokenizer\n",
    "    #bert-base-uncased #bert-base #albert-base-v2 # roberta-base # distilbert-base-uncased #distilbert-base \n",
    "    # Define New tokens for string and numerical i.e., strId$ and numId$\n",
    "#     new_tokens = [\"strId$\", \"numId$\"]\n",
    "#     for new_token in new_tokens:\n",
    "#         if new_token not in tokenizer.get_vocab().keys():\n",
    "#             tokenizer.add_tokens(new_token)\n",
    "            \n",
    "    bert = TFAutoModel.from_pretrained(model_variation, num_labels=n_categories)\n",
    "\n",
    "    #bert.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    bert_embeddings = bert.get_input_embeddings()\n",
    "    embedding_matrix = bert_embeddings.weights[0].numpy()\n",
    "    \n",
    "    num_words = embedding_matrix.shape[0]\n",
    "    print(num_words)\n",
    "    dim = embedding_matrix.shape[1]\n",
    "    print(dim)\n",
    "    \n",
    "    sentences = X_train.tolist()\n",
    "    sequences = [tokenizer(sente, truncation=True, add_special_tokens=False, return_tensors=\"tf\") for sente in sentences] # Tokenize the complete sentences\n",
    "\n",
    "    def padSequences(sequences, max_len):\n",
    "        lines_pad = []\n",
    "        for sequence in sequences:\n",
    "            seq = sequence['input_ids'].numpy()[0]\n",
    "            if len(seq) < max_len:\n",
    "                for i in range(len(seq), max_len):\n",
    "                    seq = np.append(seq, 0)\n",
    "            lines_pad.append(seq)\n",
    "        return lines_pad\n",
    "    \n",
    "    def get_max_len(sequences):\n",
    "        max_len = 0\n",
    "\n",
    "        for seq in sequences:\n",
    "            if len(seq['input_ids'].numpy()[0]) > max_len:\n",
    "                max_len = len(seq['input_ids'].numpy()[0])\n",
    "\n",
    "        return max_len\n",
    "    \n",
    "    max_len = get_max_len(sequences)\n",
    "    print(max_len)\n",
    "    \n",
    "    lines_pad_x_train = padSequences(sequences, max_len)\n",
    "    lines_pad_x_train = [arr.tolist() for arr in lines_pad_x_train]\n",
    "    lines_pad_x_train = np.array(lines_pad_x_train)\n",
    "        \n",
    "    sentences = val_data[\"text\"].tolist()\n",
    "    sequences = [tokenizer(sente, truncation=True, add_special_tokens=False, return_tensors=\"tf\") for sente in sentences]\n",
    "    \n",
    "    lines_pad_x_val = padSequences(sequences, max_len)\n",
    "    lines_pad_x_val = [arr.tolist() for arr in lines_pad_x_val]\n",
    "    lines_pad_x_val = np.array(lines_pad_x_val)\n",
    "    \n",
    "    sentences = test_data[\"text\"].tolist()\n",
    "    sequences = [tokenizer(sente, truncation=True, add_special_tokens=False, return_tensors=\"tf\") for sente in sentences]\n",
    "    \n",
    "    lines_pad_x_test = padSequences(sequences, max_len)\n",
    "    lines_pad_x_test = [arr.tolist() for arr in lines_pad_x_test]\n",
    "    lines_pad_x_test = np.array(lines_pad_x_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b9cb2e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # PYTORCH\n",
    "# if embedding_algorithm == \"gpt\":\n",
    "#     model_variation = \"gpt2\" # \"microsoft/CodeGPT-small-py-adaptedGPT2\" # \"gpt2\" # \"microsoft/CodeGPT-small-py\" \n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_variation, do_lower_case=True) #Tokenizer\n",
    "#     # Define New tokens for string and numerical i.e., strId$ and numId$\n",
    "#     new_tokens = [\"strId$\", \"numId$\"]\n",
    "#     for new_token in new_tokens:\n",
    "#         if new_token not in tokenizer.get_vocab().keys():\n",
    "#             tokenizer.add_tokens(new_token)\n",
    "            \n",
    "#     gpt = AutoModel.from_pretrained(model_variation, num_labels=n_categories)\n",
    "\n",
    "#     gpt.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "#     embedding_matrix = gpt.wte.weight.detach().cpu().numpy()\n",
    "    \n",
    "#     num_words = len(embedding_matrix)\n",
    "#     print(num_words)\n",
    "#     dim = len(embedding_matrix[0])\n",
    "#     print(dim)\n",
    "    \n",
    "#     sentences = X_train.tolist()\n",
    "#     sequences = [tokenizer.encode(sente, truncation=True, add_special_tokens=False, return_tensors=\"pt\").numpy() for sente in sentences] # Tokenize the complete sentences\n",
    "\n",
    "#     lines_pad_x_train = []\n",
    "#     for seq in sequences:\n",
    "#         lines_pad_x_train.append(torch.tensor(seq[0]))\n",
    "    \n",
    "#     lines_pad_x_train = pad_sequence(lines_pad_x_train, batch_first=True, padding_value=0)\n",
    "#     max_len = lines_pad_x_train.size()[1]\n",
    "    \n",
    "    \n",
    "#     sentences = val_data[\"Input\"]\n",
    "#     sequences = [tokenizer.encode(sente, truncation=True, add_special_tokens=False, return_tensors=\"pt\").numpy() for sente in sentences]\n",
    "#     lines_pad_x_val = []\n",
    "#     for seq in sequences:\n",
    "#         lines_pad_x_val.append(torch.tensor(seq[0]))\n",
    "#     lines_pad_x_val = pad_sequence(lines_pad_x_val, batch_first=True, padding_value=0)\n",
    "    \n",
    "#     sentences = test_data[\"Input\"]\n",
    "#     sequences = [tokenizer.encode(sente, truncation=True, add_special_tokens=False, return_tensors=\"pt\").numpy() for sente in sentences]\n",
    "#     lines_pad_x_test = []\n",
    "#     for seq in sequences:\n",
    "#         lines_pad_x_test.append(torch.tensor(seq[0]))\n",
    "#     lines_pad_x_test = pad_sequence(lines_pad_x_test, batch_first=True, padding_value=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38e03248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TENSORFLOW\n",
    "if embedding_algorithm == \"gpt\":\n",
    "    model_variation = \"gpt2\" # \"microsoft/CodeGPT-small-py-adaptedGPT2\" # \"gpt2\" # \"microsoft/CodeGPT-small-py\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_variation, do_lower_case=True) #Tokenizer\n",
    "    #bert-base-uncased #bert-base #albert-base-v2 # roberta-base # distilbert-base-uncased #distilbert-base \n",
    "    # Define New tokens for string and numerical i.e., strId$ and numId$\n",
    "#     new_tokens = [\"strId$\", \"numId$\"]\n",
    "#     for new_token in new_tokens:\n",
    "#         if new_token not in tokenizer.get_vocab().keys():\n",
    "#             tokenizer.add_tokens(new_token)\n",
    "            \n",
    "    gpt = TFGPT2LMHeadModel.from_pretrained(model_variation, num_labels=n_categories)\n",
    "\n",
    "    #gpt.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    embedding_matrix = gpt.transformer.wte.weight\n",
    "    \n",
    "    num_words = embedding_matrix.shape[0]\n",
    "    print(num_words)\n",
    "    dim = embedding_matrix.shape[1]\n",
    "    print(dim)\n",
    "    \n",
    "#     X = tokenizer(\n",
    "#         text=X_train.tolist(),\n",
    "#         add_special_tokens=False,\n",
    "#         max_length=512,\n",
    "#         truncation=True,\n",
    "#         padding=True,\n",
    "#         return_tensors='tf',\n",
    "#         return_token_type_ids=False,\n",
    "#         return_attention_mask=True,\n",
    "#         verbose=True\n",
    "#     )\n",
    "\n",
    "#     max_len = getMaxLen(X)\n",
    "    max_len = 512\n",
    "    \n",
    "    sentences = X_train.tolist()\n",
    "    sequences = [tokenizer.encode(sente, truncation=True, add_special_tokens=False, return_tensors=\"tf\").numpy() for sente in sentences] # Tokenize the complete sentences\n",
    "\n",
    "    lines_pad_x_train = []\n",
    "    for seq in sequences:\n",
    "        lines_pad_x_train.append(seq[0])\n",
    "    \n",
    "    lines_pad_x_train = pad_sequences(lines_pad_x_train, padding = 'post', maxlen = max_len)    \n",
    "    \n",
    "    sentences = val_data[\"text\"]\n",
    "    sequences = [tokenizer.encode(sente, truncation=True, add_special_tokens=False, return_tensors=\"tf\").numpy() for sente in sentences]\n",
    "    lines_pad_x_val = []\n",
    "    for seq in sequences:\n",
    "        lines_pad_x_val.append(seq[0])\n",
    "    lines_pad_x_val = pad_sequences(lines_pad_x_val, padding = 'post', maxlen = max_len)\n",
    "    \n",
    "    sentences = test_data[\"text\"]\n",
    "    sequences = [tokenizer.encode(sente, truncation=True, add_special_tokens=False, return_tensors=\"tf\").numpy() for sente in sentences]\n",
    "    lines_pad_x_test = []\n",
    "    for seq in sequences:\n",
    "        lines_pad_x_test.append(seq[0])\n",
    "    lines_pad_x_test = pad_sequences(lines_pad_x_test, padding = 'post', maxlen = max_len)\n",
    "\n",
    "    embedding_matrix = embedding_matrix.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48e3f62c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17658, 17959, 19954)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train = np.array(Y_train)\n",
    "Y_val = np.array(val_data[\"label\"])\n",
    "Y_test = np.array(test_data[\"label\"])\n",
    "len(Y_train), len(Y_val), len(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac833046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation functions\n",
    "def recall_metric(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = (true_positives + K.epsilon()) / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_metric(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = (true_positives + K.epsilon()) / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_metric(y_true, y_pred):\n",
    "\n",
    "    prec = precision_metric(y_true, y_pred)\n",
    "    rec = recall_metric(y_true, y_pred)\n",
    "    f1 = 2*((prec*rec)/(prec+rec+K.epsilon()))\n",
    "    return f1\n",
    "\n",
    "def f2_metric(y_true, y_pred):\n",
    "\n",
    "    prec = precision_metric(y_true, y_pred)\n",
    "    rec = recall_metric(y_true, y_pred)\n",
    "    f2 = 5*((prec*rec)/(4*prec+rec+K.epsilon()))\n",
    "    return f2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386ae338",
   "metadata": {},
   "source": [
    "Select Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb7c6c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "patience = 10\n",
    "batch_size = 64\n",
    "lr = 0.001\n",
    "optimizer = optimizers.Adam(learning_rate=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d61734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning Models - Classifiers\n",
    "def buildLstm(max_len, top_words, dim, seed, embedding_matrix, optimizer, n_categories):\n",
    "    model=Sequential()\n",
    "    kernel_initializer = glorot_uniform() # glorot_uniform, RandomUniform, lecun_uniform, Constant, TruncatedNormal\n",
    "    model.add(Embedding(input_dim=top_words, output_dim=dim, input_length=None, weights=[embedding_matrix], mask_zero=True, trainable=False))\n",
    "    model.add(LSTM(500, activation='tanh', dropout=0.2, return_sequences=True, stateful=False, kernel_constraint=max_norm(3), bias_constraint=max_norm(3), kernel_initializer=kernel_initializer)) # , recurrent_constraint=max_norm(3)\n",
    "    model.add(LSTM(100, activation='tanh', dropout=0.1, return_sequences=True, stateful=False, kernel_initializer=kernel_initializer))\n",
    "    model.add(LSTM(200, activation='tanh', dropout=0.1, stateful=False, kernel_initializer=kernel_initializer))\n",
    "    model.add(BatchNormalization()) # default momentum=0.99\n",
    "    #model.add(Dropout(0.2))\n",
    "    \n",
    "    #optimizer = optimizers.SGD(lr=learning_rate, decay=0.1, momentum=0.2, nesterov=True)\n",
    "    #optimizer = optimizers.RMSprop(lr=learning_rate, rho=0.9, epsilon=1e-8, decay=0.0)\n",
    "    #optimizer = optimizers.Adagrad(lr=learning_rate, epsilon=None, decay=0.004)\n",
    "    #optimizer = optimizers.Nadam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "    \n",
    "    if n_categories > 2:\n",
    "        model.add(Dense(units = n_categories, activation = 'softmax', kernel_initializer=kernel_initializer))\n",
    "        model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
    "    else:\n",
    "        model.add(Dense(units = 1, activation = 'sigmoid', kernel_initializer=kernel_initializer))\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[f1_metric])\n",
    "    return model\n",
    "\n",
    "def buildGru(max_len, top_words, dim, seed, embedding_matrix, optimizer, n_categories):\n",
    "    model=Sequential()\n",
    "    kernel_initializer = glorot_uniform() # glorot_uniform, RandomUniform, lecun_uniform, Constant, TruncatedNormal\n",
    "    model.add(Embedding(input_dim=top_words, output_dim=dim, input_length=None, weights=[embedding_matrix], mask_zero=True, trainable=False))\n",
    "    model.add(GRU(500, activation='tanh', dropout=0.2, return_sequences=True, stateful=False, kernel_constraint=max_norm(3), bias_constraint=max_norm(3), kernel_initializer=kernel_initializer)) # , recurrent_constraint=max_norm(3)\n",
    "    model.add(GRU(100, activation='tanh', dropout=0.1, return_sequences=True, stateful=False, kernel_initializer=kernel_initializer))\n",
    "    model.add(GRU(200, activation='tanh', dropout=0.1, stateful=False, kernel_initializer=kernel_initializer))\n",
    "    model.add(BatchNormalization()) # default momentum=0.99\n",
    "    #model.add(Dropout(0.2))\n",
    "    \n",
    "    #optimizer = optimizers.SGD(lr=learning_rate, decay=0.1, momentum=0.2, nesterov=True)\n",
    "    #optimizer = optimizers.RMSprop(lr=learning_rate, rho=0.9, epsilon=1e-8, decay=0.0)\n",
    "    #optimizer = optimizers.Adagrad(lr=learning_rate, epsilon=None, decay=0.004)\n",
    "    #optimizer = optimizers.Nadam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "    \n",
    "    if n_categories > 2:\n",
    "        model.add(Dense(units = n_categories, activation = 'softmax', kernel_initializer=kernel_initializer))\n",
    "        model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
    "    else:\n",
    "        model.add(Dense(units = 1, activation = 'sigmoid', kernel_initializer=kernel_initializer))\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[f1_metric]) \n",
    "    return model\n",
    "\n",
    "def buildBiLstm(max_len, top_words, dim, seed, embedding_matrix, optimizer, n_categories):\n",
    "    model=Sequential()\n",
    "    kernel_initializer = glorot_uniform() # glorot_uniform, RandomUniform, lecun_uniform, Constant, TruncatedNormal\n",
    "    model.add(Embedding(input_dim=top_words, output_dim=dim, input_length=None, weights=[embedding_matrix], mask_zero=True, trainable=False))\n",
    "    model.add(Bidirectional(LSTM(500, activation='tanh', dropout=0.2, return_sequences=True, stateful=False, kernel_constraint=max_norm(3), bias_constraint=max_norm(3), kernel_initializer=kernel_initializer))) # , recurrent_constraint=max_norm(3)\n",
    "    model.add(Bidirectional(LSTM(100, activation='tanh', dropout=0.1, return_sequences=True, stateful=False, kernel_initializer=kernel_initializer)))\n",
    "    model.add(Bidirectional(LSTM(200, activation='tanh', dropout=0.1, stateful=False, kernel_initializer=kernel_initializer)))\n",
    "    model.add(BatchNormalization()) # default momentum=0.99\n",
    "    #model.add(Dropout(0.2))\n",
    "    \n",
    "    #optimizer = optimizers.SGD(lr=learning_rate, decay=0.1, momentum=0.2, nesterov=True)\n",
    "    #optimizer = optimizers.RMSprop(lr=learning_rate, rho=0.9, epsilon=1e-8, decay=0.0)\n",
    "    #optimizer = optimizers.Adagrad(lr=learning_rate, epsilon=None, decay=0.004)\n",
    "    #optimizer = optimizers.Nadam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "    \n",
    "    if n_categories > 2:\n",
    "        model.add(Dense(units = n_categories, activation = 'softmax', kernel_initializer=kernel_initializer))\n",
    "        model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
    "    else:\n",
    "        model.add(Dense(units = 1, activation = 'sigmoid', kernel_initializer=kernel_initializer))\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[f1_metric]) \n",
    "    return model\n",
    "\n",
    "def buildBiGru(max_len, top_words, dim, seed, embedding_matrix, optimizer, n_categories):\n",
    "    model=Sequential()\n",
    "    kernel_initializer = glorot_uniform() # glorot_uniform, RandomUniform, lecun_uniform, Constant, TruncatedNormal\n",
    "    model.add(Embedding(input_dim=top_words, output_dim=dim, input_length=None, weights=[embedding_matrix], mask_zero=True, trainable=False))\n",
    "    model.add(Bidirectional(GRU(500, activation='tanh', dropout=0.2, return_sequences=True, stateful=False, kernel_constraint=max_norm(3), bias_constraint=max_norm(3), kernel_initializer=kernel_initializer))) # , recurrent_constraint=max_norm(3)\n",
    "    model.add(Bidirectional(GRU(100, activation='tanh', dropout=0.1, return_sequences=True, stateful=False, kernel_initializer=kernel_initializer)))\n",
    "    model.add(Bidirectional(GRU(200, activation='tanh', dropout=0.1, stateful=False, kernel_initializer=kernel_initializer)))\n",
    "    model.add(BatchNormalization()) # default momentum=0.99\n",
    "    #model.add(Dropout(0.2))\n",
    "    \n",
    "    #optimizer = optimizers.SGD(lr=learning_rate, decay=0.1, momentum=0.2, nesterov=True)\n",
    "    #optimizer = optimizers.RMSprop(lr=learning_rate, rho=0.9, epsilon=1e-8, decay=0.0)\n",
    "    #optimizer = optimizers.Adagrad(lr=learning_rate, epsilon=None, decay=0.004)\n",
    "    #optimizer = optimizers.Nadam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "    \n",
    "    if n_categories > 2:\n",
    "        model.add(Dense(units = n_categories, activation = 'softmax', kernel_initializer=kernel_initializer))\n",
    "        model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
    "    else:\n",
    "        model.add(Dense(units = 1, activation = 'sigmoid', kernel_initializer=kernel_initializer))\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[f1_metric])  \n",
    "    return model\n",
    "\n",
    "def buildCnn(max_len, top_words, dim, seed, embedding_matrix, optimizer, n_categories):\n",
    "    cnn_model = Sequential()\n",
    "    cnn_model.add(Embedding(top_words, dim, input_length=None, weights=[embedding_matrix], mask_zero=True, trainable=False))\n",
    "    cnn_model.add(Conv1D(filters = 128, kernel_size = 5, activation = 'relu'))\n",
    "    '''cnn_model.add(MaxPooling1D(pool_size = 5))\n",
    "    cnn_model.add(Conv1D(filters = 128, kernel_size = 5, activation = 'relu'))\n",
    "    cnn_model.add(MaxPooling1D(pool_size = 5))\n",
    "    cnn_model.add(Conv1D(filters = 128, kernel_size = 5, activation = 'relu'))'''\n",
    "    cnn_model.add(GlobalMaxPool1D())\n",
    "    #cnn_model.add(Dense(units = 128, activation = 'relu'))\n",
    "    \n",
    "    if n_categories > 2:\n",
    "        cnn_model.add(Dense(units = n_categories, activation = 'softmax'))\n",
    "        model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
    "    else:\n",
    "        cnn_model.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[f1_metric])\n",
    "    return cnn_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d7f1620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iliaskaloup\\AppData\\Local\\anaconda3\\envs\\tfenv\\lib\\site-packages\\keras\\initializers\\initializers_v2.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 768)         38603520  \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, None, 500)         2538000   \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 100)         240400    \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 200)               240800    \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 200)              800       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 201       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 41,623,721\n",
      "Trainable params: 3,019,801\n",
      "Non-trainable params: 38,603,920\n",
      "_________________________________________________________________\n",
      "model summary\\m None\n",
      "Epoch 1/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.5931 - f1_metric: 0.6977\n",
      "Epoch 1: val_f1_metric improved from -inf to 0.10687, saving model to best_model.h5\n",
      "276/276 [==============================] - 96s 307ms/step - loss: 0.5931 - f1_metric: 0.6977 - val_loss: 0.7684 - val_f1_metric: 0.1069\n",
      "Epoch 2/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.5661 - f1_metric: 0.7136\n",
      "Epoch 2: val_f1_metric improved from 0.10687 to 0.25073, saving model to best_model.h5\n",
      "276/276 [==============================] - 84s 303ms/step - loss: 0.5661 - f1_metric: 0.7136 - val_loss: 0.4421 - val_f1_metric: 0.2507\n",
      "Epoch 3/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.5387 - f1_metric: 0.7353\n",
      "Epoch 3: val_f1_metric did not improve from 0.25073\n",
      "276/276 [==============================] - 83s 303ms/step - loss: 0.5387 - f1_metric: 0.7353 - val_loss: 1.1331 - val_f1_metric: 0.1473\n",
      "Epoch 4/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.4153 - f1_metric: 0.8104\n",
      "Epoch 4: val_f1_metric improved from 0.25073 to 0.32245, saving model to best_model.h5\n",
      "276/276 [==============================] - 82s 298ms/step - loss: 0.4153 - f1_metric: 0.8104 - val_loss: 0.4190 - val_f1_metric: 0.3225\n",
      "Epoch 5/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.3189 - f1_metric: 0.8664\n",
      "Epoch 5: val_f1_metric improved from 0.32245 to 0.43241, saving model to best_model.h5\n",
      "276/276 [==============================] - 84s 304ms/step - loss: 0.3189 - f1_metric: 0.8664 - val_loss: 0.3009 - val_f1_metric: 0.4324\n",
      "Epoch 6/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.2964 - f1_metric: 0.8789\n",
      "Epoch 6: val_f1_metric improved from 0.43241 to 0.46123, saving model to best_model.h5\n",
      "276/276 [==============================] - 84s 305ms/step - loss: 0.2964 - f1_metric: 0.8789 - val_loss: 0.2758 - val_f1_metric: 0.4612\n",
      "Epoch 7/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.2760 - f1_metric: 0.8943\n",
      "Epoch 7: val_f1_metric did not improve from 0.46123\n",
      "276/276 [==============================] - 84s 304ms/step - loss: 0.2760 - f1_metric: 0.8943 - val_loss: 0.2463 - val_f1_metric: 0.4605\n",
      "Epoch 8/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.2588 - f1_metric: 0.8994\n",
      "Epoch 8: val_f1_metric did not improve from 0.46123\n",
      "276/276 [==============================] - 86s 310ms/step - loss: 0.2588 - f1_metric: 0.8994 - val_loss: 0.2674 - val_f1_metric: 0.4442\n",
      "Epoch 9/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.2392 - f1_metric: 0.9108\n",
      "Epoch 9: val_f1_metric did not improve from 0.46123\n",
      "276/276 [==============================] - 85s 307ms/step - loss: 0.2392 - f1_metric: 0.9108 - val_loss: 0.3113 - val_f1_metric: 0.4196\n",
      "Epoch 10/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.2164 - f1_metric: 0.9223\n",
      "Epoch 10: val_f1_metric did not improve from 0.46123\n",
      "276/276 [==============================] - 84s 305ms/step - loss: 0.2164 - f1_metric: 0.9223 - val_loss: 0.3900 - val_f1_metric: 0.3637\n",
      "Epoch 11/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.1985 - f1_metric: 0.9300\n",
      "Epoch 11: val_f1_metric did not improve from 0.46123\n",
      "276/276 [==============================] - 84s 303ms/step - loss: 0.1985 - f1_metric: 0.9300 - val_loss: 0.3399 - val_f1_metric: 0.4044\n",
      "Epoch 12/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.1806 - f1_metric: 0.9384\n",
      "Epoch 12: val_f1_metric did not improve from 0.46123\n",
      "276/276 [==============================] - 84s 306ms/step - loss: 0.1806 - f1_metric: 0.9384 - val_loss: 0.3590 - val_f1_metric: 0.4151\n",
      "Epoch 13/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.1644 - f1_metric: 0.9441\n",
      "Epoch 13: val_f1_metric did not improve from 0.46123\n",
      "276/276 [==============================] - 83s 303ms/step - loss: 0.1644 - f1_metric: 0.9441 - val_loss: 0.3749 - val_f1_metric: 0.3893\n",
      "Epoch 14/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.1425 - f1_metric: 0.9525\n",
      "Epoch 14: val_f1_metric did not improve from 0.46123\n",
      "276/276 [==============================] - 85s 307ms/step - loss: 0.1425 - f1_metric: 0.9525 - val_loss: 0.4257 - val_f1_metric: 0.3857\n",
      "Epoch 15/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.1278 - f1_metric: 0.9601\n",
      "Epoch 15: val_f1_metric did not improve from 0.46123\n",
      "276/276 [==============================] - 84s 306ms/step - loss: 0.1278 - f1_metric: 0.9601 - val_loss: 0.4612 - val_f1_metric: 0.3777\n",
      "Epoch 16/100\n",
      "276/276 [==============================] - ETA: 0s - loss: 0.1233 - f1_metric: 0.9600\n",
      "Epoch 16: val_f1_metric did not improve from 0.46123\n",
      "276/276 [==============================] - 84s 306ms/step - loss: 0.1233 - f1_metric: 0.9600 - val_loss: 0.3231 - val_f1_metric: 0.4558\n",
      "Epoch 16: early stopping\n",
      "Training is completed after 1358109\n"
     ]
    }
   ],
   "source": [
    "print(\"Training...\")\n",
    "milli_sec1 = int(round(time.time() * 1000))\n",
    "\n",
    "userModel = \"lstm\"\n",
    "\n",
    "if userModel == \"cnn\":\n",
    "    myModel = buildCnn(max_len, num_words, dim, seed, embedding_matrix, optimizer, n_categories) \n",
    "elif userModel == \"lstm\":\n",
    "    myModel = buildLstm(max_len, num_words, dim, seed, embedding_matrix, optimizer, n_categories)\n",
    "elif userModel == \"bilstm\":\n",
    "    myModel = buildBiLstm(max_len, num_words, dim, seed, embedding_matrix, optimizer, n_categories)\n",
    "elif userModel == \"gru\":\n",
    "    myModel = buildGru(max_len, num_words, dim, seed, embedding_matrix, optimizer, n_categories)\n",
    "elif userModel == \"bigru\":\n",
    "    myModel = buildBiGru(max_len, num_words, dim, seed, embedding_matrix, optimizer, n_categories)\n",
    "    \n",
    "print(\"model summary\\m\", myModel.summary())\n",
    "\n",
    "csv_logger = CSVLogger('log.csv', append=True, separator=',')\n",
    "es = EarlyStopping(monitor='val_f1_metric', mode='max', verbose=1, patience=patience)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_f1_metric', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "history = myModel.fit(lines_pad_x_train, Y_train, validation_data=(lines_pad_x_val, Y_val), epochs = n_epochs, batch_size = batch_size, shuffle=True, verbose=1, callbacks=[csv_logger,es,mc]) #, class_weight=class_weights\n",
    "\n",
    "milli_sec2 = int(round(time.time() * 1000))\n",
    "print(\"Training is completed after\", milli_sec2-milli_sec1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ebc1635a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP= 0\n",
      "TN= 18864\n",
      "FP= 0\n",
      "FN= 1090\n",
      "Accuracy:94.54%\n",
      "Precision:0.00%\n",
      "Recall:0.00%\n",
      "F1 score:0.00%\n",
      "Roc_Auc score:50.00%\n",
      "F2 score:nan%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97     18864\n",
      "           1       0.00      0.00      0.00      1090\n",
      "\n",
      "    accuracy                           0.95     19954\n",
      "   macro avg       0.47      0.50      0.49     19954\n",
      "weighted avg       0.89      0.95      0.92     19954\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iliaskaloup\\AppData\\Local\\anaconda3\\envs\\tfenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\iliaskaloup\\AppData\\Local\\Temp\\ipykernel_9908\\672218592.py:21: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  f2=5*precision*recall / (4*precision+recall)\n",
      "C:\\Users\\iliaskaloup\\AppData\\Local\\anaconda3\\envs\\tfenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\iliaskaloup\\AppData\\Local\\anaconda3\\envs\\tfenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\iliaskaloup\\AppData\\Local\\anaconda3\\envs\\tfenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#load best model\n",
    "#model = load_model('best_model.h5')\n",
    "myModel.load_weights(\"best_model.h5\")\n",
    "\n",
    "#scores = myModel.evaluate(lines_pad_x_test, Y_test, verbose=0)\n",
    "#predictions = myModel.predict_classes(X_test, verbose=0)\n",
    "predScores = myModel.predict(lines_pad_x_test)\n",
    "predictions = (predScores > 0.5).astype(\"int32\")\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "accuracy=accuracy_score(Y_test, predicted_labels)\n",
    "if n_categories > 2:\n",
    "    precision=precision_score(Y_test, predicted_labels, average='macro')\n",
    "    recall=recall_score(Y_test, predicted_labels, average='macro')\n",
    "    f1=f1_score(Y_test, predicted_labels, average='macro')\n",
    "else:\n",
    "    precision=precision_score(Y_test, predicted_labels)\n",
    "    recall=recall_score(Y_test, predicted_labels)\n",
    "    f1=f1_score(Y_test, predicted_labels)\n",
    "    roc_auc=roc_auc_score(Y_test, predicted_labels)\n",
    "f2=5*precision*recall / (4*precision+recall)\n",
    "\n",
    "cm = confusion_matrix(Y_test, predicted_labels)\n",
    "#print(cm)\n",
    "sn.heatmap(cm, annot=True)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(\"TP=\",tp)\n",
    "print(\"TN=\",tn)\n",
    "print(\"FP=\",fp)\n",
    "print(\"FN=\",fn)\n",
    "\n",
    "acc = ((tp+tn)/(tp+tn+fp+fn))\n",
    "\n",
    "print(\"Accuracy:%.2f%%\"%(acc*100))\n",
    "print(\"Precision:%.2f%%\"%(precision*100))\n",
    "print(\"Recall:%.2f%%\"%(recall*100))\n",
    "print(\"F1 score:%.2f%%\"%(f1*100))\n",
    "print(\"Roc_Auc score:%.2f%%\"%(roc_auc*100))\n",
    "print(\"F2 score:%.2f%%\"%(f2*100))\n",
    "print(classification_report(Y_test, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fb23c2",
   "metadata": {},
   "source": [
    "Export classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d395ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the path\n",
    "path = os.path.join(root_path, 'results', model_variation.split(\"/\")[-1], method, str(shuffle_seeder))\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# Define the CSV file path\n",
    "csv_file_path = os.path.join(path, f\"{shuffle_seeder}.csv\")\n",
    "\n",
    "# Write data to CSV\n",
    "data = {\n",
    "    \"accuracy\": accuracy,\n",
    "    \"precision\": precision,\n",
    "    \"recall\": recall,\n",
    "    \"f1\": f1,\n",
    "    \"f2\": f2,\n",
    "    \"roc_auc\": roc_auc\n",
    "}\n",
    "\n",
    "# Write to CSV\n",
    "with open(csv_file_path, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=data.keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerow(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1707ba6",
   "metadata": {},
   "source": [
    "Compute the average values of the classication metrics considering the results for all different seeders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1912105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.9453743610303699, 'precision': 0.47268718051518493, 'recall': 0.5, 'f1': 0.4859601215930754, 'f2': 0.49428781050204385, 'roc_auc': 0.5}\n"
     ]
    }
   ],
   "source": [
    "# Define a dictionary to store cumulative sum of metrics\n",
    "cumulative_metrics = defaultdict(float)\n",
    "count = 0  # Counter to keep track of number of CSV files\n",
    "\n",
    "# Iterate over all CSV files in the results folder\n",
    "results_folder = os.path.join(root_path, \"results\", model_variation.split(\"/\")[-1], method, str(shuffle_seeder))\n",
    "for filename in os.listdir(results_folder):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        csv_file_path = os.path.join(results_folder, filename)\n",
    "        with open(csv_file_path, \"r\", newline=\"\") as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                for metric, value in row.items():\n",
    "                    cumulative_metrics[metric] += float(value)\n",
    "        count += 1\n",
    "        \n",
    "# Compute average values\n",
    "average_metrics = {metric: total / count for metric, total in cumulative_metrics.items()}\n",
    "\n",
    "# Print average values \n",
    "print(average_metrics)\n",
    "\n",
    "# Define the path for the average CSV file\n",
    "avg_csv_file_path = os.path.join(root_path, \"results\", model_variation.split(\"/\")[-1], method, \"avg.csv\")\n",
    "\n",
    "# Write average metrics to CSV\n",
    "with open(avg_csv_file_path, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=average_metrics.keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerow(average_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cc08f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
