{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3354fc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import json, os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import csv\n",
    "from transformers import AutoTokenizer, T5ForSequenceClassification\n",
    "from transformers import AdamWeightDecay\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, \\\n",
    "roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from collections import defaultdict\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbfd651",
   "metadata": {},
   "source": [
    "Define method name and root path of the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74683969",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"forSequence\"\n",
    "\n",
    "root_path = os.path.join('..', '..', '..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48806676",
   "metadata": {},
   "source": [
    "Define specific seeder for all experiments and processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e3172d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2164406da30>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fb50e2",
   "metadata": {},
   "source": [
    "Define functions for data read and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef7011ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropEmpty(tokens0):\n",
    "    tokens = []\n",
    "    for i in range(0, len(tokens0)):\n",
    "        temp = tokens0[i]\n",
    "        if temp != []:\n",
    "            tokens.append(temp)\n",
    "    return tokens\n",
    "\n",
    "def listToString(s): \n",
    "    \n",
    "    # initialize an empty string\n",
    "    str1 = \"\" \n",
    "    \n",
    "    # traverse in the string \n",
    "    count = 0\n",
    "    for ele in s: \n",
    "        if count==0:\n",
    "            str1 = str1 + ele\n",
    "        else:\n",
    "            str1 = str1 + ' ' + ele\n",
    "        count = count + 1\n",
    "        #str1 += ele  \n",
    "    \n",
    "    # return string  \n",
    "    return str1\n",
    "\n",
    "def prepareData(data):\n",
    "        \n",
    "    # lowercase\n",
    "    lines = []\n",
    "    labels = []\n",
    "    headlines = []\n",
    "    for i in range(0, len(data)):\n",
    "        labels.append(int(data[i][1]))\n",
    "        headlines.append(data[i][0])\n",
    "        line = data[i][2:]\n",
    "        lows = [w.lower() for w in line]\n",
    "        lines.append(lows)\n",
    "    \n",
    "    texts = []\n",
    "    for i in range(0, len(lines)):\n",
    "        texts.append(listToString(lines[i]))\n",
    "    \n",
    "    return texts, labels, headlines \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b0f01a",
   "metadata": {},
   "source": [
    "Read data and shuffle them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29764f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_reduced_bert # bigvul_data_reduced\n",
    "with open(os.path.join(root_path, 'data', 'data_reduced_bert.csv'), newline='', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f)\n",
    "    data = list(reader)\n",
    "data = dropEmpty(data)\n",
    "random.shuffle(data) # shuffle the data\n",
    "\n",
    "# Data preparation\n",
    "texts, labels, headlines = prepareData(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d37f542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dci_callback_plugin.py',\n",
       " '0',\n",
       " 'class',\n",
       " 'CallbackModule',\n",
       " 'calls',\n",
       " 'strId$',\n",
       " 'def',\n",
       " '__init__',\n",
       " 'self',\n",
       " 'super',\n",
       " 'CallbackModule',\n",
       " 'self',\n",
       " '__init__',\n",
       " 'self',\n",
       " 'self',\n",
       " 'None',\n",
       " 'self',\n",
       " 'None',\n",
       " 'self',\n",
       " 'numId$',\n",
       " 'self',\n",
       " 'strId$',\n",
       " 'def',\n",
       " 'v2_playbook_on_task_start',\n",
       " 'self',\n",
       " 'task',\n",
       " 'is_conditional',\n",
       " 'super',\n",
       " 'CallbackModule',\n",
       " 'self',\n",
       " 'v2_playbook_on_task_start',\n",
       " 'task',\n",
       " 'is_conditional',\n",
       " 'def',\n",
       " 'v2_runner_on_ok',\n",
       " 'self',\n",
       " 'result',\n",
       " 'kwargs',\n",
       " 'strId$',\n",
       " 'Event',\n",
       " 'executed',\n",
       " 'after',\n",
       " 'each',\n",
       " 'command',\n",
       " 'when',\n",
       " 'it',\n",
       " 'succeed',\n",
       " 'Get',\n",
       " 'the',\n",
       " 'output',\n",
       " 'of',\n",
       " 'the',\n",
       " 'command',\n",
       " 'and',\n",
       " 'create',\n",
       " 'a',\n",
       " 'file',\n",
       " 'associated',\n",
       " 'to',\n",
       " 'the',\n",
       " 'current',\n",
       " 'strId$',\n",
       " 'super',\n",
       " 'CallbackModule',\n",
       " 'self',\n",
       " 'v2_runner_on_ok',\n",
       " 'result',\n",
       " 'kwargs',\n",
       " 'if',\n",
       " 'strId$',\n",
       " 'in',\n",
       " 'result',\n",
       " '_result',\n",
       " 'output',\n",
       " 'strId$',\n",
       " 'join',\n",
       " 'result',\n",
       " '_result',\n",
       " 'strId$',\n",
       " 'strId$',\n",
       " 'elif',\n",
       " 'strId$',\n",
       " 'in',\n",
       " 'result',\n",
       " '_result',\n",
       " 'output',\n",
       " 'strId$',\n",
       " 'join',\n",
       " 'result',\n",
       " '_result',\n",
       " 'strId$',\n",
       " 'strId$',\n",
       " 'else',\n",
       " 'output',\n",
       " 'str',\n",
       " 'result',\n",
       " '_result',\n",
       " 'if',\n",
       " 'result',\n",
       " '_task',\n",
       " 'get_name',\n",
       " 'strId$',\n",
       " 'and',\n",
       " 'self',\n",
       " 'strId$',\n",
       " 'create',\n",
       " 'self',\n",
       " 'name',\n",
       " 'result',\n",
       " '_task',\n",
       " 'get_name',\n",
       " 'content',\n",
       " 'output',\n",
       " 'encode',\n",
       " 'strId$',\n",
       " 'mime',\n",
       " 'self',\n",
       " 'job_id',\n",
       " 'self',\n",
       " 'elif',\n",
       " 'result',\n",
       " '_task',\n",
       " 'get_name',\n",
       " 'strId$',\n",
       " 'and',\n",
       " 'output',\n",
       " 'strId$',\n",
       " 'create',\n",
       " 'self',\n",
       " 'name',\n",
       " 'result',\n",
       " '_task',\n",
       " 'get_name',\n",
       " 'content',\n",
       " 'output',\n",
       " 'encode',\n",
       " 'strId$',\n",
       " 'mime',\n",
       " 'self',\n",
       " 'self',\n",
       " 'self',\n",
       " 'numId$',\n",
       " 'def',\n",
       " 'v2_runner_on_failed',\n",
       " 'self',\n",
       " 'result',\n",
       " 'ignore_errors',\n",
       " 'False',\n",
       " 'strId$',\n",
       " 'Event',\n",
       " 'executed',\n",
       " 'when',\n",
       " 'a',\n",
       " 'command',\n",
       " 'failed',\n",
       " 'Create',\n",
       " 'the',\n",
       " 'final',\n",
       " 'on',\n",
       " 'failure',\n",
       " 'strId$',\n",
       " 'super',\n",
       " 'CallbackModule',\n",
       " 'self',\n",
       " 'v2_runner_on_failed',\n",
       " 'result',\n",
       " 'ignore_errors',\n",
       " 'if',\n",
       " 'result',\n",
       " '_result',\n",
       " 'strId$',\n",
       " 'output',\n",
       " 'strId$',\n",
       " 'result',\n",
       " '_result',\n",
       " 'strId$',\n",
       " 'result',\n",
       " '_result',\n",
       " 'strId$',\n",
       " 'else',\n",
       " 'output',\n",
       " 'result',\n",
       " '_result',\n",
       " 'strId$',\n",
       " 'new_state',\n",
       " 'create',\n",
       " 'self',\n",
       " 'status',\n",
       " 'strId$',\n",
       " 'comment',\n",
       " 'self',\n",
       " 'job_id',\n",
       " 'self',\n",
       " 'json',\n",
       " 'self',\n",
       " 'new_state',\n",
       " 'strId$',\n",
       " 'strId$',\n",
       " 'if',\n",
       " 'result',\n",
       " '_task',\n",
       " 'get_name',\n",
       " 'strId$',\n",
       " 'and',\n",
       " 'output',\n",
       " 'strId$',\n",
       " 'create',\n",
       " 'self',\n",
       " 'name',\n",
       " 'result',\n",
       " '_task',\n",
       " 'get_name',\n",
       " 'content',\n",
       " 'output',\n",
       " 'encode',\n",
       " 'strId$',\n",
       " 'mime',\n",
       " 'self',\n",
       " 'self',\n",
       " 'self',\n",
       " 'numId$',\n",
       " 'def',\n",
       " 'self',\n",
       " 'play',\n",
       " 'strId$',\n",
       " 'Event',\n",
       " 'executed',\n",
       " 'before',\n",
       " 'each',\n",
       " 'play',\n",
       " 'Create',\n",
       " 'a',\n",
       " 'new',\n",
       " 'and',\n",
       " 'save',\n",
       " 'the',\n",
       " 'current',\n",
       " 'id',\n",
       " 'strId$',\n",
       " 'super',\n",
       " 'CallbackModule',\n",
       " 'self',\n",
       " 'play',\n",
       " 'self',\n",
       " 'numId$',\n",
       " 'self',\n",
       " 'play',\n",
       " 'get_vars',\n",
       " 'strId$',\n",
       " 'status',\n",
       " 'play',\n",
       " 'get_vars',\n",
       " 'strId$',\n",
       " 'self',\n",
       " 'play',\n",
       " 'get_vars',\n",
       " 'strId$',\n",
       " 'self',\n",
       " 'play',\n",
       " 'extra_vars',\n",
       " 'strId$',\n",
       " 'if',\n",
       " 'strId$',\n",
       " 'in',\n",
       " 'play',\n",
       " 'get_vars',\n",
       " 'self',\n",
       " 'play',\n",
       " 'get_vars',\n",
       " 'strId$',\n",
       " 'else',\n",
       " 'self',\n",
       " 'strId$',\n",
       " 'new_state',\n",
       " 'create',\n",
       " 'self',\n",
       " 'status',\n",
       " 'status',\n",
       " 'comment',\n",
       " 'self',\n",
       " 'job_id',\n",
       " 'self',\n",
       " 'json',\n",
       " 'self',\n",
       " 'new_state',\n",
       " 'strId$',\n",
       " 'strId$']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88ef46d",
   "metadata": {},
   "source": [
    "Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c44d5ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements in dataset: 4184\n",
      "2 categories found:\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# explore data\n",
    "n_elements=len(headlines)\n",
    "print('Elements in dataset:', n_elements)\n",
    "categories=sorted(list(set(labels))) #set will return the unique different entries\n",
    "n_categories=len(categories)\n",
    "print(\"{} categories found:\".format(n_categories))\n",
    "for category in categories:\n",
    "    print(category)\n",
    "    \n",
    "fig=plt.figure(figsize=(20,8))\n",
    "lbl, counts = np.unique(labels,return_counts=True)\n",
    "ticks = range(len(counts))\n",
    "plt.bar(ticks,counts, align='center')\n",
    "plt.xticks(ticks,lbl)\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2405b1c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>Is_Vulnerable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>class callbackmodule calls strid$ def __init__...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this_file os path abspath __file__ decode sys ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>class migration migrations migration dependenc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dataset pd strid$ x dataset iloc numid$ values...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>__metaclass__ type ansible_metadata strid$ str...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Input  Is_Vulnerable\n",
       "0  class callbackmodule calls strid$ def __init__...              0\n",
       "1  this_file os path abspath __file__ decode sys ...              0\n",
       "2  class migration migrations migration dependenc...              0\n",
       "3  dataset pd strid$ x dataset iloc numid$ values...              1\n",
       "4  __metaclass__ type ansible_metadata strid$ str...              0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(({'Input': texts, 'Is_Vulnerable': labels}))\n",
    "#data = data[0:100]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bbec25",
   "metadata": {},
   "source": [
    "Train test split with seeder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "928f2aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ratio = 0.10\n",
    "\n",
    "#split to train-val-test\n",
    "# split dataset to train-test sets\n",
    "### split data into train and test (90% train, 10% test)\n",
    "shuffle_seeders = [seed, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "shuffle_seeder = shuffle_seeders[0]\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=val_ratio, random_state=shuffle_seeder, stratify=data['Is_Vulnerable'])\n",
    "# print(len(data))\n",
    "# print(len(train_data))\n",
    "# print(len(test_data))\n",
    "# print(len(test_data)+len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d36c93f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = train_test_split(train_data, test_size=val_ratio, random_state=shuffle_seeder, stratify=train_data['Is_Vulnerable'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37254222",
   "metadata": {},
   "source": [
    "Pre-processing step: Under-sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "782fb4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution  0    65\n",
      "1    16\n",
      "Name: Is_Vulnerable, dtype: int64\n",
      "Majority class  0\n",
      "Minority class  1\n",
      "Targeted number of majority class 16\n",
      "Class distribution after augmentation 0    16\n",
      "1    16\n",
      "Name: Is_Vulnerable, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "sampling = True\n",
    "if n_categories == 2 and sampling == True:\n",
    "    # Apply under-sampling with the specified strategy\n",
    "    class_counts = pd.Series(train_data[\"Is_Vulnerable\"]).value_counts()\n",
    "    print(\"Class distribution \", class_counts)\n",
    "\n",
    "    majority_class = class_counts.idxmax()\n",
    "    print(\"Majority class \", majority_class)\n",
    "\n",
    "    minority_class = class_counts.idxmin()\n",
    "    print(\"Minority class \", minority_class)\n",
    "\n",
    "    target_count = class_counts[class_counts.idxmin()] # int(class_counts.iloc[0] / 2) \n",
    "    print(\"Targeted number of majority class\", target_count)\n",
    "\n",
    "    # under\n",
    "    sampling_strategy = {majority_class: target_count}        \n",
    "    rus = RandomUnderSampler(random_state=seed, sampling_strategy=sampling_strategy)\n",
    "\n",
    "    x_train_resampled, y_train_resampled = rus.fit_resample(np.array(train_data[\"Input\"]).reshape(-1, 1), train_data[\"Is_Vulnerable\"]) \n",
    "    print(\"Class distribution after augmentation\", pd.Series(y_train_resampled).value_counts())\n",
    "\n",
    "\n",
    "    # Shuffle the resampled data while preserving the correspondence between features and labels\n",
    "    x_train_resampled, y_train_resampled = shuffle(x_train_resampled, y_train_resampled, random_state=seed)\n",
    "\n",
    "    # rename\n",
    "    X_train = x_train_resampled\n",
    "    Y_train = y_train_resampled\n",
    "\n",
    "    X_train = pd.Series(X_train.reshape(-1))\n",
    "\n",
    "else:\n",
    "    X_train = train_data[\"Input\"]\n",
    "    Y_train = train_data[\"Is_Vulnerable\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86325363",
   "metadata": {},
   "source": [
    "Choose transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "772f881d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c0860eb70e94abe90bc3b3c203c059b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\torchenv\\lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\iliaskaloup\\.cache\\huggingface\\hub\\models--Salesforce--codet5-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f633bad8a6da4d2d9f04547e1f6d7156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/703k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aabd1b695084f0c9558151fdf5e70eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/294k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a358af69d1f6441bb4971739834ea5fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "433a4f3fde04435cb026f3a4e9655ab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/12.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_variation = \"Salesforce/codet5-base\" # \"google-t5/t5-base\" # Salesforce/codet5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_variation, do_lower_case=True) #Tokenizer\n",
    "#bert-base-uncased #bert-base #albert-base-v2 # roberta-base # distilbert-base-uncased #distilbert-base # microsoft/codebert-base-mlm # microsoft/codebert-base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d8f06e",
   "metadata": {},
   "source": [
    "Insert new tokens to the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e1b708d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define New tokens for string and numerical i.e., strId$ and numId$\n",
    "new_tokens = [\"strId$\", \"numId$\"]\n",
    "for new_token in new_tokens:\n",
    "    if new_token not in tokenizer.get_vocab().keys():\n",
    "        tokenizer.add_tokens(new_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efd3c1e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06acf1d32a8546db87e85ff4c88edf3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.57k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b707131cf8b4f6f8688d9bc58348001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at Salesforce/codet5-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(32102, 768)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = T5ForSequenceClassification.from_pretrained(model_variation, num_labels=n_categories)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239da0d2",
   "metadata": {},
   "source": [
    "Find the max length of the tokined input sequneces of the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ee7fab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMaxLen(X):\n",
    "\n",
    "    # Code for identifying max length of the data samples after tokenization using transformer tokenizer\n",
    "    \n",
    "    max_length = 0\n",
    "    max_row = 0\n",
    "    \n",
    "    # Iterate over each sample in your dataset\n",
    "    for i, input_ids in enumerate(X['input_ids']):\n",
    "        # Convert input_ids to a PyTorch tensor\n",
    "        input_ids_tensor = torch.tensor(input_ids)\n",
    "        # Calculate the length of the tokenized sequence for the current sample\n",
    "        length = torch.sum(input_ids_tensor != tokenizer.pad_token_id).item()\n",
    "        # Update max_length and max_row if the current length is greater\n",
    "        if length > max_length:\n",
    "            max_length = length\n",
    "            max_row = i\n",
    "\n",
    "    print(\"Max length of tokenized data:\", max_length)\n",
    "    print(\"Row with max length:\", max_row)\n",
    "    \n",
    "    return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2927c24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of tokenized data: 512\n",
      "Row with max length: 5\n",
      "Max tokenized length 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iliaskaloup\\AppData\\Local\\Temp\\ipykernel_17096\\514096298.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids_tensor = torch.tensor(input_ids)\n"
     ]
    }
   ],
   "source": [
    "X = tokenizer(\n",
    "        text=X_train.tolist(),\n",
    "        add_special_tokens=True,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors='pt',\n",
    "        return_token_type_ids=False,\n",
    "        return_attention_mask=True,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "max_len = getMaxLen(X)\n",
    "print(\"Max tokenized length\", max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0120f241",
   "metadata": {},
   "source": [
    "Tokenize train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7120237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_X(x_train, x_val, x_test, max_len):\n",
    "    \n",
    "    X_train = tokenizer(\n",
    "        text=x_train.tolist(),\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors='pt',\n",
    "        return_token_type_ids=False,\n",
    "        return_attention_mask=True,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    X_val = tokenizer(\n",
    "        text=x_val.tolist(),\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors='pt',\n",
    "        return_token_type_ids=False,\n",
    "        return_attention_mask=True,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    X_test = tokenizer(\n",
    "        text=x_test.tolist(),\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors='pt',\n",
    "        return_token_type_ids=False,\n",
    "        return_attention_mask=True,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    return X_train, X_val, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c10863d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test = tokenize_X(X_train, val_data['Input'], test_data['Input'], max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d5742c",
   "metadata": {},
   "source": [
    "Aternative way using batch encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64d058ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def encode(docs, max_len):\n",
    "#     '''\n",
    "#     This function takes list of texts and returns input_ids and attention_mask of texts\n",
    "#     '''\n",
    "#     encoded_dict = tokenizer.batch_encode_plus(docs.tolist(), add_special_tokens=True, max_length=max_len, padding=True,\n",
    "#                             return_attention_mask=True, truncation=True, return_token_type_ids=False, return_tensors='pt', verbose=True)\n",
    "    \n",
    "#     return encoded_dict\n",
    "\n",
    "# X_train = encode(X_train, max_len)\n",
    "# #X_val = encode(val_data['Input'], max_len)\n",
    "# X_test = encode(test_data['Input'], max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a588408f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32]), torch.Size([9]), torch.Size([10]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train = torch.LongTensor(Y_train.tolist())\n",
    "Y_val = torch.LongTensor(val_data[\"Is_Vulnerable\"].tolist())\n",
    "Y_test = torch.LongTensor(test_data[\"Is_Vulnerable\"].tolist())\n",
    "Y_train.size(), Y_val.size(), Y_test.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c41e669",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 6 #16\n",
    "\n",
    "train_dataset = TensorDataset(X_train[\"input_ids\"], X_train[\"attention_mask\"], Y_train)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "val_dataset = TensorDataset(X_val[\"input_ids\"], X_val[\"attention_mask\"], Y_val)\n",
    "val_sampler = SequentialSampler(val_dataset)\n",
    "val_dataloader = DataLoader(val_dataset, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "test_dataset = TensorDataset(X_test[\"input_ids\"], X_test[\"attention_mask\"], Y_test)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfe9ffb",
   "metadata": {},
   "source": [
    "Select Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ddbd55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "patience = 5\n",
    "lr = 2e-4 #5e-05 #  5e-5, 3e-5, 2e-5\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "            num_warmup_steps=0,\n",
    "            num_training_steps=len(train_dataloader)*n_epochs )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692dc92c",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "545e9a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c1cf50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5ForSequenceClassification(\n",
      "  (transformer): T5Model(\n",
      "    (shared): Embedding(32102, 768)\n",
      "    (encoder): T5Stack(\n",
      "      (embed_tokens): Embedding(32102, 768)\n",
      "      (block): ModuleList(\n",
      "        (0): T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (relative_attention_bias): Embedding(32, 12)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseActDense(\n",
      "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): ReLU()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseActDense(\n",
      "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): ReLU()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseActDense(\n",
      "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): ReLU()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseActDense(\n",
      "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): ReLU()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseActDense(\n",
      "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): ReLU()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseActDense(\n",
      "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): ReLU()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (6): T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseActDense(\n",
      "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): ReLU()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (7): T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseActDense(\n",
      "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): ReLU()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (8): T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseActDense(\n",
      "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): ReLU()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (9): T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseActDense(\n",
      "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): ReLU()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (10): T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseActDense(\n",
      "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): ReLU()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (11): T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseActDense(\n",
      "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): ReLU()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (final_layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (decoder): T5Stack(\n",
      "      (embed_tokens): Embedding(32102, 768)\n",
      "      (block): ModuleList(\n",
      "        (0): T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (relative_attention_bias): Embedding(32, 12)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerCrossAttention(\n",
      "              (EncDecAttention): T5Attention(\n",
      "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (2): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseActDense(\n",
      "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): ReLU()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerCrossAttention(\n",
      "              (EncDecAttention): T5Attention(\n",
      "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (2): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseActDense(\n",
      "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): ReLU()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerCrossAttention(\n",
      "              (EncDecAttention): T5Attention(\n",
      "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (2): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseActDense(\n",
      "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): ReLU()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerCrossAttention(\n",
      "              (EncDecAttention): T5Attention(\n",
      "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (2): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseActDense(\n",
      "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): ReLU()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerCrossAttention(\n",
      "              (EncDecAttention): T5Attention(\n",
      "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (2): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseActDense(\n",
      "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): ReLU()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerCrossAttention(\n",
      "              (EncDecAttention): T5Attention(\n",
      "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (2): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseActDense(\n",
      "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): ReLU()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (6): T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerCrossAttention(\n",
      "              (EncDecAttention): T5Attention(\n",
      "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (2): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseActDense(\n",
      "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): ReLU()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (7): T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerCrossAttention(\n",
      "              (EncDecAttention): T5Attention(\n",
      "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (2): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseActDense(\n",
      "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): ReLU()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (8): T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerCrossAttention(\n",
      "              (EncDecAttention): T5Attention(\n",
      "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (2): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseActDense(\n",
      "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): ReLU()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (9): T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerCrossAttention(\n",
      "              (EncDecAttention): T5Attention(\n",
      "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (2): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseActDense(\n",
      "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): ReLU()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (10): T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerCrossAttention(\n",
      "              (EncDecAttention): T5Attention(\n",
      "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (2): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseActDense(\n",
      "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): ReLU()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (11): T5Block(\n",
      "          (layer): ModuleList(\n",
      "            (0): T5LayerSelfAttention(\n",
      "              (SelfAttention): T5Attention(\n",
      "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (1): T5LayerCrossAttention(\n",
      "              (EncDecAttention): T5Attention(\n",
      "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (2): T5LayerFF(\n",
      "              (DenseReluDense): T5DenseActDense(\n",
      "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (act): ReLU()\n",
      "              )\n",
      "              (layer_norm): T5LayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (final_layer_norm): T5LayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (classification_head): T5ClassificationHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "No. of trainable parameters:  223475714\n"
     ]
    }
   ],
   "source": [
    "print(model.to(device))\n",
    "print(\"No. of trainable parameters: \", sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b82f6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(filename, epoch, model, optimizer, scheduler, train_loss_per_epoch, val_loss_per_epoch, train_f1_per_epoch, val_f1_per_epoch):\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'model': model,\n",
    "        'optimizer': optimizer,\n",
    "        'scheduler': scheduler,\n",
    "        'train_loss_per_epoch': train_loss_per_epoch,\n",
    "        'val_loss_per_epoch': val_loss_per_epoch,\n",
    "        'train_f1_per_epoch': train_f1_per_epoch,\n",
    "        'val_f1_per_epoch': val_f1_per_epoch\n",
    "        }\n",
    "    torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8fbd165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # we do not retrain our pre-trained BERT and train only the last linear dense layer\n",
    "# for param in model.bert_model.parameters():\n",
    "#     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d7f1620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch:  1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1581b1f6f7cb4099a1cd62da792addec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebb7c39ad4514994b9e4157747f5fe11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Train Loss: 1.7743 - Valid Loss: 3.1010\n",
      "Epoch 1/100 - Train F1: 0.3871 - Valid F1: 0.3636\n",
      "Model saved at epoch:  1\n",
      "Epoch:  2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c78ac0c76a094a668b74b37ca8f505d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# backward pass to calculate the gradients\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\u001b[39;00m\n\u001b[0;32m     46\u001b[0m clip_grad_norm_(parameters\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\torchenv\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\torchenv\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Training...\")\n",
    "milli_sec1 = int(round(time.time() * 1000))\n",
    "\n",
    "# Initialize values for implementing Callbacks\n",
    "## Early Stopping\n",
    "best_val_f1 = -1\n",
    "best_epoch = -1\n",
    "no_improvement_counter = 0\n",
    "## Save best - optimal checkpointing\n",
    "checkpoint_dir = './checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "save_path = os.path.join(checkpoint_dir, 'best_weights.pt')\n",
    "\n",
    "train_loss_per_epoch = []\n",
    "val_loss_per_epoch = []\n",
    "train_f1_per_epoch = []\n",
    "val_f1_per_epoch = []\n",
    "\n",
    "for epoch_num in range(n_epochs):\n",
    "    print('Epoch: ', epoch_num + 1)\n",
    "    \n",
    "    #Training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    total_preds = []\n",
    "    total_labels = []\n",
    "    for step_num, batch_data in enumerate(tqdm(train_dataloader, desc='Training')):\n",
    "        \n",
    "        input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
    "        \n",
    "        # clear previously calculated gradients\n",
    "        model.zero_grad() # optimizer.zero_grad()\n",
    "        \n",
    "        # get model predictions for the current batch\n",
    "        output = model(input_ids = input_ids, attention_mask=att_mask) # , labels=labels\n",
    "        \n",
    "        # compute the loss between actual and predicted values\n",
    "        loss = nn.CrossEntropyLoss()(output.logits, labels) #loss = output.loss #output[0]       \n",
    "        # add on to the total loss\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        clip_grad_norm_(parameters=model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Print training loss after each batch\n",
    "        #print(\"Epoch {}/{} - Batch {}/{} - Training Loss: {:.4f}\".format(epoch_num+1, n_epochs, step_num+1, len(train_dataloader), loss.item()))\n",
    "        \n",
    "        # model predictions are stored on GPU. So, push it to CPU\n",
    "        preds = np.argmax(output.logits.cpu().detach().numpy(),axis=-1)\n",
    "        # append the model predictions\n",
    "        total_preds+=list(preds)\n",
    "        total_labels+=labels.tolist()\n",
    "        \n",
    "    train_loss_per_epoch.append(train_loss / len(train_dataloader))    \n",
    "    train_accuracy=accuracy_score(total_labels, total_preds)\n",
    "    if n_categories > 2:\n",
    "        train_precision=precision_score(total_labels, total_preds, average='macro')\n",
    "        train_recall=recall_score(total_labels, total_preds, average='macro')\n",
    "        train_f1=f1_score(total_labels, total_preds, average='macro')\n",
    "    else:\n",
    "        train_precision=precision_score(total_labels, total_preds)\n",
    "        train_recall=recall_score(total_labels, total_preds)\n",
    "        train_f1=f1_score(total_labels, total_preds)\n",
    "        train_roc_auc=roc_auc_score(total_labels, total_preds)\n",
    "    train_f2 = (5*train_precision*train_recall) / (4*train_precision+train_recall)\n",
    "\n",
    "    #Validation\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    valid_pred = []\n",
    "    actual_labels = []\n",
    "    with torch.no_grad():\n",
    "        for step_num_e, batch_data in enumerate(tqdm(val_dataloader, desc='Validation')):\n",
    "            input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
    "            \n",
    "            output = model(input_ids = input_ids, attention_mask=att_mask) # , labels=labels\n",
    "            \n",
    "            preds = np.argmax(output.logits.cpu().detach().numpy(),axis=-1)\n",
    "            valid_pred+=list(preds)\n",
    "            actual_labels+=labels.tolist()\n",
    "\n",
    "            loss = nn.CrossEntropyLoss()(output.logits, labels) #loss = output.loss #output[0]\n",
    "            valid_loss += loss.item()\n",
    "        \n",
    "    val_loss_per_epoch.append(valid_loss / len(val_dataloader))    \n",
    "    val_accuracy=accuracy_score(actual_labels, valid_pred)\n",
    "    if n_categories > 2:\n",
    "        val_precision=precision_score(actual_labels, valid_pred, average='macro')\n",
    "        val_recall=recall_score(actual_labels, valid_pred, average='macro')\n",
    "        val_f1=f1_score(actual_labels, valid_pred, average='macro')\n",
    "    else:\n",
    "        val_precision=precision_score(actual_labels, valid_pred)\n",
    "        val_recall=recall_score(actual_labels, valid_pred)\n",
    "        val_f1=f1_score(actual_labels, valid_pred)\n",
    "        val_roc_auc=roc_auc_score(actual_labels, valid_pred)\n",
    "    val_f2 = (5*val_precision*val_recall) / (4*val_precision+val_recall)\n",
    "    \n",
    "    print(\"Epoch {}/{} - Train Loss: {:.4f} - Valid Loss: {:.4f}\".format(epoch_num+1, n_epochs, train_loss_per_epoch[-1], val_loss_per_epoch[-1]))\n",
    "    print(\"Epoch {}/{} - Train F1: {:.4f} - Valid F1: {:.4f}\".format(epoch_num+1, n_epochs, train_f1, val_f1))\n",
    "    \n",
    "    train_f1_per_epoch.append(train_f1)\n",
    "    val_f1_per_epoch.append(val_f1)\n",
    "    \n",
    "    # Implement Callbacks: Early Stopping and save best\n",
    "    # Check if the validation F1 score has improved\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        best_epoch = epoch_num + 1\n",
    "        no_improvement_counter = 0 # Reset the counter\n",
    "        \n",
    "        # Save the best model checkpoint\n",
    "        save_checkpoint(save_path, epoch_num+1, model.state_dict(), optimizer.state_dict(), scheduler.state_dict(), train_loss_per_epoch, val_loss_per_epoch, train_f1_per_epoch, val_f1_per_epoch)\n",
    "        print(\"Model saved at epoch: \", epoch_num+1)\n",
    "    else:\n",
    "        no_improvement_counter += 1\n",
    "        \n",
    "        if no_improvement_counter >= patience:\n",
    "            print(\"No improvement for\", patience, \"consecutive epochs.\")\n",
    "            total_epochs = epoch_num + 1\n",
    "            print(\"Early stopping after epoch No.\", total_epochs)\n",
    "            print(\"Best model after epoch No\", best_epoch)\n",
    "            print(\"Best achieved val_f1 = \", best_val_f1)\n",
    "            break\n",
    "\n",
    "milli_sec2 = int(round(time.time() * 1000))\n",
    "print(\"Training is completed after\", milli_sec2-milli_sec1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82afc9ea",
   "metadata": {},
   "source": [
    "Plot history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c80f2b04",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'total_epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[43mtotal_epochs\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      2\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots()\n\u001b[0;32m      3\u001b[0m ax\u001b[38;5;241m.\u001b[39mplot(epochs, train_loss_per_epoch,label \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'total_epochs' is not defined"
     ]
    }
   ],
   "source": [
    "epochs = range(1, total_epochs + 1)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(epochs, train_loss_per_epoch,label ='training loss')\n",
    "ax.plot(epochs, val_loss_per_epoch, label = 'validation loss' )\n",
    "ax.set_title('Training and Validation loss')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a790f7d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'total_epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[43mtotal_epochs\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      2\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots()\n\u001b[0;32m      3\u001b[0m ax\u001b[38;5;241m.\u001b[39mplot(epochs, train_f1_per_epoch,label \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining F1-score\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'total_epochs' is not defined"
     ]
    }
   ],
   "source": [
    "epochs = range(1, total_epochs + 1)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(epochs, train_f1_per_epoch,label ='training F1-score')\n",
    "ax.plot(epochs, val_f1_per_epoch, label = 'validation F1-score')\n",
    "ax.set_title('Training and Validation F1-scores')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('F1-score')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35aa7b6",
   "metadata": {},
   "source": [
    "Load best model from checkpoint during training with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c1b6f91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForSequenceClassification(\n",
       "  (transformer): T5Model(\n",
       "    (shared): Embedding(32102, 768)\n",
       "    (encoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32102, 768)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 12)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (11): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (decoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32102, 768)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 12)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (11): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (classification_head): T5ClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(save_path, map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8e6947",
   "metadata": {},
   "source": [
    "Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "558ed86d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31d5d74c2a3941d9b3e60dcf0a396f46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "test_pred = []\n",
    "actual_labels = []\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for step_num, batch_data in enumerate(tqdm(test_dataloader, desc='Testing')):\n",
    "        input_ids, att_mask, labels = [data.to(device) for data in batch_data]\n",
    "        \n",
    "        output = model(input_ids = input_ids, attention_mask=att_mask) #, labels= labels\n",
    "\n",
    "        loss = nn.CrossEntropyLoss()(output.logits, labels) #loss = output.loss #output[0]\n",
    "        test_loss += loss.item()\n",
    "   \n",
    "        preds = np.argmax(output.logits.cpu().detach().numpy(),axis=-1)\n",
    "        test_pred+=list(preds)\n",
    "        actual_labels+=labels.tolist()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c45d709",
   "metadata": {},
   "source": [
    "Evaluation on test set (unseen data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9e2a0dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         8\n",
      "           1       0.20      1.00      0.33         2\n",
      "\n",
      "    accuracy                           0.20        10\n",
      "   macro avg       0.10      0.50      0.17        10\n",
      "weighted avg       0.04      0.20      0.07        10\n",
      "\n",
      "Accuracy:20.00%\n",
      "Precision:20.00%\n",
      "Recall:100.00%\n",
      "F1 score:33.33%\n",
      "F2 score:55.56%\n",
      "Roc_Auc score:50.00%\n",
      "TP= 2\n",
      "TN= 0\n",
      "FP= 8\n",
      "FN= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\torchenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\anaconda3\\envs\\torchenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\anaconda3\\envs\\torchenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_report = classification_report(actual_labels, test_pred)\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "total_test_loss = test_loss/len(test_dataloader) \n",
    "accuracy=accuracy_score(actual_labels, test_pred)\n",
    "if n_categories > 2:\n",
    "    precision=precision_score(actual_labels, test_pred, average='macro')\n",
    "    recall=recall_score(actual_labels, test_pred, average='macro')\n",
    "    f1=f1_score(actual_labels, test_pred, average='macro')\n",
    "else:\n",
    "    precision=precision_score(actual_labels, test_pred)\n",
    "    recall=recall_score(actual_labels, test_pred)\n",
    "    f1=f1_score(actual_labels, test_pred)\n",
    "    roc_auc=roc_auc_score(actual_labels, test_pred)\n",
    "f2 = (5*precision*recall) / (4*precision+recall)\n",
    "\n",
    "print(\"Accuracy:%.2f%%\"%(accuracy*100))\n",
    "print(\"Precision:%.2f%%\"%(precision*100))\n",
    "print(\"Recall:%.2f%%\"%(recall*100))\n",
    "print(\"F1 score:%.2f%%\"%(f1*100))\n",
    "print(\"F2 score:%.2f%%\"%(f2*100))\n",
    "if roc_auc:\n",
    "    print(\"Roc_Auc score:%.2f%%\"%(roc_auc*100))\n",
    "\n",
    "conf_matrix = confusion_matrix(actual_labels, test_pred)\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "#acc = ((tp+tn)/(tp+tn+fp+fn))\n",
    "\n",
    "print(\"TP=\",tp)\n",
    "print(\"TN=\",tn)\n",
    "print(\"FP=\",fp)\n",
    "print(\"FN=\",fn)\n",
    "#print(conf_matrix)\n",
    "sn.heatmap(conf_matrix, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fb23c2",
   "metadata": {},
   "source": [
    "Export classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "2d395ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the path\n",
    "path = os.path.join(root_path, 'results', model_variation.split(\"/\")[-1], method, str(shuffle_seeder))\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# Define the CSV file path\n",
    "csv_file_path = os.path.join(path, f\"{shuffle_seeder}.csv\")\n",
    "\n",
    "# Write data to CSV\n",
    "data = {\n",
    "    \"accuracy\": accuracy,\n",
    "    \"precision\": precision,\n",
    "    \"recall\": recall,\n",
    "    \"f1\": f1,\n",
    "    \"f2\": f2,\n",
    "    \"roc_auc\": roc_auc\n",
    "}\n",
    "\n",
    "# Write to CSV\n",
    "with open(csv_file_path, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=data.keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerow(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1707ba6",
   "metadata": {},
   "source": [
    "Compute the average values of the classication metrics considering the results for all different seeders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "d1912105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.2, 'precision': 0.2, 'recall': 1.0, 'f1': 0.33333333333333337, 'f2': 0.5555555555555556, 'roc_auc': 0.5}\n"
     ]
    }
   ],
   "source": [
    "# Define a dictionary to store cumulative sum of metrics\n",
    "cumulative_metrics = defaultdict(float)\n",
    "count = 0  # Counter to keep track of number of CSV files\n",
    "\n",
    "# Iterate over all CSV files in the results folder\n",
    "results_folder = os.path.join(root_path, \"results\", model_variation.split(\"/\")[-1], method, str(shuffle_seeder))\n",
    "for filename in os.listdir(results_folder):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        csv_file_path = os.path.join(results_folder, filename)\n",
    "        with open(csv_file_path, \"r\", newline=\"\") as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                for metric, value in row.items():\n",
    "                    cumulative_metrics[metric] += float(value)\n",
    "        count += 1\n",
    "        \n",
    "# Compute average values\n",
    "average_metrics = {metric: total / count for metric, total in cumulative_metrics.items()}\n",
    "\n",
    "# Print average values \n",
    "print(average_metrics)\n",
    "\n",
    "# Define the path for the average CSV file\n",
    "avg_csv_file_path = os.path.join(root_path, \"results\", model_variation.split(\"/\")[-1], method, \"avg.csv\")\n",
    "\n",
    "# Write average metrics to CSV\n",
    "with open(avg_csv_file_path, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=average_metrics.keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerow(average_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cc08f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
